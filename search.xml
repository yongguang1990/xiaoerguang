<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[排序算法之基数排序]]></title>
    <url>%2F2019%2F07%2F27%2F%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E4%B9%8B%E5%9F%BA%E6%95%B0%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[1.10 基数排序基数排序是一种非比较型整数排序算法，其原理是将整数按位数切割成不同的数字，然后按每个位数分别比较。由于整数也可以表达字符串（比如名字或日期）和特定格式的浮点数，所以基数排序也不是只能使用于整数。 1. 基数排序 vs 计数排序 vs 桶排序基数排序有两种方法： 这三种排序算法都利用了桶的概念，但对桶的使用方法上有明显差异： 基数排序：根据键值的每位数字来分配桶； 计数排序：每个桶只存储单一键值； 桶排序：每个桶存储一定范围的数值； 2. LSD 基数排序动图演示 代码实现JavaScript实例12345678910111213141516171819202122232425//LSD Radix Sortvar counter = [];function radixSort(arr, maxDigit) &#123; var mod = 10; var dev = 1; for (var i = 0; i &lt; maxDigit; i++, dev *= 10, mod *= 10) &#123; for(var j = 0; j &lt; arr.length; j++) &#123; var bucket = parseInt((arr[j] % mod) / dev); if(counter[bucket]==null) &#123; counter[bucket] = []; &#125; counter[bucket].push(arr[j]); &#125; var pos = 0; for(var j = 0; j &lt; counter.length; j++) &#123; var value = null; if(counter[j]!=null) &#123; while ((value = counter[j].shift()) != null) &#123; arr[pos++] = value; &#125; &#125; &#125; &#125; return arr;&#125; Java实例1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980/** * 基数排序 * 考虑负数的情况还可以参考： https://code.i-harness.com/zh-CN/q/e98fa9 */public class RadixSort implements IArraySort &#123; @Override public int[] sort(int[] sourceArray) throws Exception &#123; // 对 arr 进行拷贝，不改变参数内容 int[] arr = Arrays.copyOf(sourceArray, sourceArray.length); int maxDigit = getMaxDigit(arr); return radixSort(arr, maxDigit); &#125; /** * 获取最高位数 */ private int getMaxDigit(int[] arr) &#123; int maxValue = getMaxValue(arr); return getNumLenght(maxValue); &#125; private int getMaxValue(int[] arr) &#123; int maxValue = arr[0]; for (int value : arr) &#123; if (maxValue &lt; value) &#123; maxValue = value; &#125; &#125; return maxValue; &#125; protected int getNumLenght(long num) &#123; if (num == 0) &#123; return 1; &#125; int lenght = 0; for (long temp = num; temp != 0; temp /= 10) &#123; lenght++; &#125; return lenght; &#125; private int[] radixSort(int[] arr, int maxDigit) &#123; int mod = 10; int dev = 1; for (int i = 0; i &lt; maxDigit; i++, dev *= 10, mod *= 10) &#123; // 考虑负数的情况，这里扩展一倍队列数，其中 [0-9]对应负数，[10-19]对应正数 (bucket + 10) int[][] counter = new int[mod * 2][0]; for (int j = 0; j &lt; arr.length; j++) &#123; int bucket = ((arr[j] % mod) / dev) + mod; counter[bucket] = arrayAppend(counter[bucket], arr[j]); &#125; int pos = 0; for (int[] bucket : counter) &#123; for (int value : bucket) &#123; arr[pos++] = value; &#125; &#125; &#125; return arr; &#125; /** * 自动扩容，并保存数据 * * @param arr * @param value */ private int[] arrayAppend(int[] arr, int value) &#123; arr = Arrays.copyOf(arr, arr.length + 1); arr[arr.length - 1] = value; return arr; &#125;&#125; PHP实例1234567891011121314151617181920212223242526272829303132function radixSort($arr, $maxDigit = null)&#123; if ($maxDigit === null) &#123; $maxDigit = max($arr); &#125; $counter = []; for ($i = 0; $i &lt; $maxDigit; $i++) &#123; for ($j = 0; $j &lt; count($arr); $j++) &#123; preg_match_all('/\d/', (string) $arr[$j], $matches); $numArr = $matches[0]; $lenTmp = count($numArr); $bucket = array_key_exists($lenTmp - $i - 1, $numArr) ? intval($numArr[$lenTmp - $i - 1]) : 0; if (!array_key_exists($bucket, $counter)) &#123; $counter[$bucket] = []; &#125; $counter[$bucket][] = $arr[$j]; &#125; $pos = 0; for ($j = 0; $j &lt; count($counter); $j++) &#123; $value = null; if ($counter[$j] !== null) &#123; while (($value = array_shift($counter[$j])) !== null) &#123; $arr[$pos++] = $value; &#125; &#125; &#125; &#125; return $arr;&#125; C++实例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061int maxbit(int data[], int n) //辅助函数，求数据的最大位数&#123; int maxData = data[0]; ///&lt; 最大数 /// 先求出最大数，再求其位数，这样有原先依次每个数判断其位数，稍微优化点。 for (int i = 1; i &lt; n; ++i) &#123; if (maxData &lt; data[i]) maxData = data[i]; &#125; int d = 1; int p = 10; while (maxData &gt;= p) &#123; //p *= 10; // Maybe overflow maxData /= 10; ++d; &#125; return d;/* int d = 1; //保存最大的位数 int p = 10; for(int i = 0; i &lt; n; ++i) &#123; while(data[i] &gt;= p) &#123; p *= 10; ++d; &#125; &#125; return d;*/&#125;void radixsort(int data[], int n) //基数排序&#123; int d = maxbit(data, n); int *tmp = new int[n]; int *count = new int[10]; //计数器 int i, j, k; int radix = 1; for(i = 1; i &lt;= d; i++) //进行d次排序 &#123; for(j = 0; j &lt; 10; j++) count[j] = 0; //每次分配前清空计数器 for(j = 0; j &lt; n; j++) &#123; k = (data[j] / radix) % 10; //统计每个桶中的记录数 count[k]++; &#125; for(j = 1; j &lt; 10; j++) count[j] = count[j - 1] + count[j]; //将tmp中的位置依次分配给每个桶 for(j = n - 1; j &gt;= 0; j--) //将所有桶中记录依次收集到tmp中 &#123; k = (data[j] / radix) % 10; tmp[count[k] - 1] = data[j]; count[k]--; &#125; for(j = 0; j &lt; n; j++) //将临时数组的内容复制到data中 data[j] = tmp[j]; radix = radix * 10; &#125; delete []tmp; delete []count;&#125; C实例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273#include&lt;stdio.h&gt;#define MAX 20//#define SHOWPASS#define BASE 10void print(int *a, int n) &#123; int i; for (i = 0; i &lt; n; i++) &#123; printf("%d\t", a[i]); &#125;&#125;void radixsort(int *a, int n) &#123; int i, b[MAX], m = a[0], exp = 1; for (i = 1; i &lt; n; i++) &#123; if (a[i] &gt; m) &#123; m = a[i]; &#125; &#125; while (m / exp &gt; 0) &#123; int bucket[BASE] = &#123; 0 &#125;; for (i = 0; i &lt; n; i++) &#123; bucket[(a[i] / exp) % BASE]++; &#125; for (i = 1; i &lt; BASE; i++) &#123; bucket[i] += bucket[i - 1]; &#125; for (i = n - 1; i &gt;= 0; i--) &#123; b[--bucket[(a[i] / exp) % BASE]] = a[i]; &#125; for (i = 0; i &lt; n; i++) &#123; a[i] = b[i]; &#125; exp *= BASE;#ifdef SHOWPASS printf("\nPASS : "); print(a, n);#endif &#125;&#125;int main() &#123; int arr[MAX]; int i, n; printf("Enter total elements (n &lt;= %d) : ", MAX); scanf("%d", &amp;n); n = n &lt; MAX ? n : MAX; printf("Enter %d Elements : ", n); for (i = 0; i &lt; n; i++) &#123; scanf("%d", &amp;arr[i]); &#125; printf("\nARRAY : "); print(&amp;arr[0], n); radixsort(&amp;arr[0], n); printf("\nSORTED : "); print(&amp;arr[0], n); printf("\n"); return 0;&#125; Lua实例1234567891011121314151617181920212223242526272829303132333435363738394041424344-- 获取表中位数local maxBit = function (tt) local weight = 10; -- 十進制 local bit = 1; for k, v in pairs(tt) do while v &gt;= weight do weight = weight * 10; bit = bit + 1; end end return bit;end-- 基数排序local radixSort = function (tt) local maxbit = maxBit(tt); local bucket = &#123;&#125;; local temp = &#123;&#125;; local radix = 1; for i = 1, maxbit do for j = 1, 10 do bucket[j] = 0; --- 清空桶 end for k, v in pairs(tt) do local remainder = math.floor((v / radix)) % 10 + 1; bucket[remainder] = bucket[remainder] + 1; -- 每個桶數量自動增加1 end for j = 2, 10 do bucket[j] = bucket[j - 1] + bucket[j]; -- 每个桶的数量 = 以前桶数量和 + 自个数量 end -- 按照桶的位置，排序--这个是桶式排序，必须使用倒序，因为排序方法是从小到大，顺序下来，会出现大的在小的上面清空。 for k = #tt, 1, -1 do local remainder = math.floor((tt[k] / radix)) % 10 + 1; temp[bucket[remainder]] = tt[k]; bucket[remainder] = bucket[remainder] - 1; end for k, v in pairs(temp) do tt[k] = v; end radix = radix * 10; endend;]]></content>
      <tags>
        <tag>排序算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序算法之桶排序]]></title>
    <url>%2F2019%2F07%2F26%2F%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E4%B9%8B%E6%A1%B6%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[桶排序桶排序是计数排序的升级版。它利用了函数的映射关系，高效与否的关键就在于这个映射函数的确定。为了使桶排序更加高效，我们需要做到这两点： 在额外空间充足的情况下，尽量增大桶的数量 使用的映射函数能够将输入的 N 个数据均匀的分配到 K 个桶中 同时，对于桶中元素的排序，选择何种比较排序算法对于性能的影响至关重要。 1. 什么时候最快当输入的数据可以均匀的分配到每一个桶中。 2. 什么时候最慢当输入的数据被分配到了同一个桶中。 3. 示意图元素分布在桶中： 然后，元素在每个桶中排序： 代码实现JavaScript实例12345678910111213141516171819202122232425262728293031323334353637383940function bucketSort(arr, bucketSize) &#123; if (arr.length === 0) &#123; return arr; &#125; var i; var minValue = arr[0]; var maxValue = arr[0]; for (i = 1; i &lt; arr.length; i++) &#123; if (arr[i] &lt; minValue) &#123; minValue = arr[i]; // 输入数据的最小值 &#125; else if (arr[i] &gt; maxValue) &#123; maxValue = arr[i]; // 输入数据的最大值 &#125; &#125; //桶的初始化 var DEFAULT_BUCKET_SIZE = 5; // 设置桶的默认数量为5 bucketSize = bucketSize || DEFAULT_BUCKET_SIZE; var bucketCount = Math.floor((maxValue - minValue) / bucketSize) + 1; var buckets = new Array(bucketCount); for (i = 0; i &lt; buckets.length; i++) &#123; buckets[i] = []; &#125; //利用映射函数将数据分配到各个桶中 for (i = 0; i &lt; arr.length; i++) &#123; buckets[Math.floor((arr[i] - minValue) / bucketSize)].push(arr[i]); &#125; arr.length = 0; for (i = 0; i &lt; buckets.length; i++) &#123; insertionSort(buckets[i]); // 对每个桶进行排序，这里使用了插入排序 for (var j = 0; j &lt; buckets[i].length; j++) &#123; arr.push(buckets[i][j]); &#125; &#125; return arr;&#125; Java实例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364public class BucketSort implements IArraySort &#123; private static final InsertSort insertSort = new InsertSort(); @Override public int[] sort(int[] sourceArray) throws Exception &#123; // 对 arr 进行拷贝，不改变参数内容 int[] arr = Arrays.copyOf(sourceArray, sourceArray.length); return bucketSort(arr, 5); &#125; private int[] bucketSort(int[] arr, int bucketSize) throws Exception &#123; if (arr.length == 0) &#123; return arr; &#125; int minValue = arr[0]; int maxValue = arr[0]; for (int value : arr) &#123; if (value &lt; minValue) &#123; minValue = value; &#125; else if (value &gt; maxValue) &#123; maxValue = value; &#125; &#125; int bucketCount = (int) Math.floor((maxValue - minValue) / bucketSize) + 1; int[][] buckets = new int[bucketCount][0]; // 利用映射函数将数据分配到各个桶中 for (int i = 0; i &lt; arr.length; i++) &#123; int index = (int) Math.floor((arr[i] - minValue) / bucketSize); buckets[index] = arrAppend(buckets[index], arr[i]); &#125; int arrIndex = 0; for (int[] bucket : buckets) &#123; if (bucket.length &lt;= 0) &#123; continue; &#125; // 对每个桶进行排序，这里使用了插入排序 bucket = insertSort.sort(bucket); for (int value : bucket) &#123; arr[arrIndex++] = value; &#125; &#125; return arr; &#125; /** * 自动扩容，并保存数据 * * @param arr * @param value */ private int[] arrAppend(int[] arr, int value) &#123; arr = Arrays.copyOf(arr, arr.length + 1); arr[arr.length - 1] = value; return arr; &#125;&#125; PHP实例12345678910111213141516171819202122232425262728293031323334353637function bucketSort($arr, $bucketSize = 5)&#123; if (count($arr) === 0) &#123; return $arr; &#125; $minValue = $arr[0]; $maxValue = $arr[0]; for ($i = 1; $i &lt; count($arr); $i++) &#123; if ($arr[$i] &lt; $minValue) &#123; $minValue = $arr[$i]; &#125; else if ($arr[$i] &gt; $maxValue) &#123; $maxValue = $arr[$i]; &#125; &#125; $bucketCount = floor(($maxValue - $minValue) / $bucketSize) + 1; $buckets = array(); for ($i = 0; $i &lt; count($buckets); $i++) &#123; $buckets[$i] = []; &#125; for ($i = 0; $i &lt; count($arr); $i++) &#123; $buckets[floor(($arr[$i] - $minValue) / $bucketSize)][] = $arr[$i]; &#125; $arr = array(); for ($i = 0; $i &lt; count($buckets); $i++) &#123; $bucketTmp = $buckets[$i]; sort($bucketTmp); for ($j = 0; $j &lt; count($bucketTmp); $j++) &#123; $arr[] = $bucketTmp[$j]; &#125; &#125; return $arr;&#125; C++实例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364#include&lt;iterator&gt;#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;const int BUCKET_NUM = 10;struct ListNode&#123; explicit ListNode(int i=0):mData(i),mNext(NULL)&#123;&#125; ListNode* mNext; int mData;&#125;;ListNode* insert(ListNode* head,int val)&#123; ListNode dummyNode; ListNode *newNode = new ListNode(val); ListNode *pre,*curr; dummyNode.mNext = head; pre = &amp;dummyNode; curr = head; while(NULL!=curr &amp;&amp; curr-&gt;mData&lt;=val)&#123; pre = curr; curr = curr-&gt;mNext; &#125; newNode-&gt;mNext = curr; pre-&gt;mNext = newNode; return dummyNode.mNext;&#125;ListNode* Merge(ListNode *head1,ListNode *head2)&#123; ListNode dummyNode; ListNode *dummy = &amp;dummyNode; while(NULL!=head1 &amp;&amp; NULL!=head2)&#123; if(head1-&gt;mData &lt;= head2-&gt;mData)&#123; dummy-&gt;mNext = head1; head1 = head1-&gt;mNext; &#125;else&#123; dummy-&gt;mNext = head2; head2 = head2-&gt;mNext; &#125; dummy = dummy-&gt;mNext; &#125; if(NULL!=head1) dummy-&gt;mNext = head1; if(NULL!=head2) dummy-&gt;mNext = head2; return dummyNode.mNext;&#125;void BucketSort(int n,int arr[])&#123; vector&lt;ListNode*&gt; buckets(BUCKET_NUM,(ListNode*)(0)); for(int i=0;i&lt;n;++i)&#123; int index = arr[i]/BUCKET_NUM; ListNode *head = buckets.at(index); buckets.at(index) = insert(head,arr[i]); &#125; ListNode *head = buckets.at(0); for(int i=1;i&lt;BUCKET_NUM;++i)&#123; head = Merge(head,buckets.at(i)); &#125; for(int i=0;i&lt;n;++i)&#123; arr[i] = head-&gt;mData; head = head-&gt;mNext; &#125;&#125;]]></content>
      <tags>
        <tag>排序算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序算法之计数排序]]></title>
    <url>%2F2019%2F07%2F25%2F%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E4%B9%8B%E8%AE%A1%E6%95%B0%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[计数排序计数排序的核心在于将输入的数据值转化为键存储在额外开辟的数组空间中。作为一种线性时间复杂度的排序，计数排序要求输入的数据必须是有确定范围的整数。 \1. 计数排序的特征 当输入的元素是 n 个 0 到 k 之间的整数时，它的运行时间是 Θ(n + k)。计数排序不是比较排序，排序的速度快于任何比较排序算法。 由于用来计数的数组C的长度取决于待排序数组中数据的范围（等于待排序数组的最大值与最小值的差加上1），这使得计数排序对于数据范围很大的数组，需要大量时间和内存。例如：计数排序是用来排序0到100之间的数字的最好的算法，但是它不适合按字母顺序排序人名。但是，计数排序可以用在基数排序中的算法来排序数据范围很大的数组。 通俗地理解，例如有 10 个年龄不同的人，统计出有 8 个人的年龄比 A 小，那 A 的年龄就排在第 9 位,用这个方法可以得到其他每个人的位置,也就排好了序。当然，年龄有重复时需要特殊处理（保证稳定性），这就是为什么最后要反向填充目标数组，以及将每个数字的统计减去 1 的原因。 算法的步骤如下： （1）找出待排序的数组中最大和最小的元素 （2）统计数组中每个值为i的元素出现的次数，存入数组C的第i项 （3）对所有的计数累加（从C中的第一个元素开始，每一项和前一项相加） （4）反向填充目标数组：将每个元素i放在新数组的第C(i)项，每放一个元素就将C(i)减去1 2. 动图演示 代码实现JavaScript实例12345678910111213141516171819202122function countingSort(arr, maxValue) &#123; var bucket = new Array(maxValue+1), sortedIndex = 0; arrLen = arr.length, bucketLen = maxValue + 1; for (var i = 0; i &lt; arrLen; i++) &#123; if (!bucket[arr[i]]) &#123; bucket[arr[i]] = 0; &#125; bucket[arr[i]]++; &#125; for (var j = 0; j &lt; bucketLen; j++) &#123; while(bucket[j] &gt; 0) &#123; arr[sortedIndex++] = j; bucket[j]--; &#125; &#125; return arr;&#125; Python实例123456789101112131415def countingSort(arr, maxValue): bucketLen = maxValue+1 bucket = [0]*bucketLen sortedIndex =0 arrLen = len(arr) for i in range(arrLen): if not bucket[arr[i]]: bucket[arr[i]]=0 bucket[arr[i]]+=1 for j in range(bucketLen): while bucket[j]&gt;0: arr[sortedIndex] = j sortedIndex+=1 bucket[j]-=1 return arr Go实例123456789101112131415161718192021func countingSort(arr []int, maxValue int) []int &#123; bucketLen := maxValue + 1 bucket := make([]int, bucketLen) // 初始为0的数组 sortedIndex := 0 length := len(arr) for i := 0; i &lt; length; i++ &#123; bucket[arr[i]] += 1 &#125; for j := 0; j &lt; bucketLen; j++ &#123; for bucket[j] &gt; 0 &#123; arr[sortedIndex] = j sortedIndex += 1 bucket[j] -= 1 &#125; &#125; return arr&#125; Java实例1234567891011121314151617181920212223242526272829303132333435363738394041public class CountingSort implements IArraySort &#123; @Override public int[] sort(int[] sourceArray) throws Exception &#123; // 对 arr 进行拷贝，不改变参数内容 int[] arr = Arrays.copyOf(sourceArray, sourceArray.length); int maxValue = getMaxValue(arr); return countingSort(arr, maxValue); &#125; private int[] countingSort(int[] arr, int maxValue) &#123; int bucketLen = maxValue + 1; int[] bucket = new int[bucketLen]; for (int value : arr) &#123; bucket[value]++; &#125; int sortedIndex = 0; for (int j = 0; j &lt; bucketLen; j++) &#123; while (bucket[j] &gt; 0) &#123; arr[sortedIndex++] = j; bucket[j]--; &#125; &#125; return arr; &#125; private int getMaxValue(int[] arr) &#123; int maxValue = arr[0]; for (int value : arr) &#123; if (maxValue &lt; value) &#123; maxValue = value; &#125; &#125; return maxValue; &#125;&#125; PHP实例123456789101112131415161718192021222324function countingSort($arr, $maxValue = null)&#123; if ($maxValue === null) &#123; $maxValue = max($arr); &#125; for ($m = 0; $m &lt; $maxValue + 1; $m++) &#123; $bucket[] = null; &#125; $arrLen = count($arr); for ($i = 0; $i &lt; $arrLen; $i++) &#123; if (!array_key_exists($arr[$i], $bucket)) &#123; $bucket[$arr[$i]] = 0; &#125; $bucket[$arr[$i]]++; &#125; $sortedIndex = 0; foreach ($bucket as $key =&gt; $len) &#123; if ($len !== null) $arr[$sortedIndex++] = $key; &#125; return $arr;&#125; C实例12345678910111213141516171819202122232425262728293031323334353637383940414243#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;time.h&gt;void print_arr(int *arr, int n) &#123; int i; printf("%d", arr[0]); for (i = 1; i &lt; n; i++) printf(" %d", arr[i]); printf("\n");&#125;void counting_sort(int *ini_arr, int *sorted_arr, int n) &#123; int *count_arr = (int *) malloc(sizeof(int) * 100); int i, j, k; for (k = 0; k &lt; 100; k++) count_arr[k] = 0; for (i = 0; i &lt; n; i++) count_arr[ini_arr[i]]++; for (k = 1; k &lt; 100; k++) count_arr[k] += count_arr[k - 1]; for (j = n; j &gt; 0; j--) sorted_arr[--count_arr[ini_arr[j - 1]]] = ini_arr[j - 1]; free(count_arr);&#125;int main(int argc, char **argv) &#123; int n = 10; int i; int *arr = (int *) malloc(sizeof(int) * n); int *sorted_arr = (int *) malloc(sizeof(int) * n); srand(time(0)); for (i = 0; i &lt; n; i++) arr[i] = rand() % 100; printf("ini_array: "); print_arr(arr, n); counting_sort(arr, sorted_arr, n); printf("sorted_array: "); print_arr(sorted_arr, n); free(arr); free(sorted_arr); return 0;&#125;]]></content>
      <tags>
        <tag>排序算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序算法之堆排序]]></title>
    <url>%2F2019%2F07%2F24%2F%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E4%B9%8B%E5%A0%86%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[堆排序堆排序（Heapsort）是指利用堆这种数据结构所设计的一种排序算法。堆积是一个近似完全二叉树的结构，并同时满足堆积的性质：即子结点的键值或索引总是小于（或者大于）它的父节点。堆排序可以说是一种利用堆的概念来排序的选择排序。分为两种方法： 大顶堆：每个节点的值都大于或等于其子节点的值，在堆排序算法中用于升序排列； 小顶堆：每个节点的值都小于或等于其子节点的值，在堆排序算法中用于降序排列； 堆排序的平均时间复杂度为 Ο(nlogn)。 1. 算法步骤 创建一个堆 H[0……n-1]； 把堆首（最大值）和堆尾互换； 把堆的尺寸缩小 1，并调用 shift_down(0)，目的是把新的数组顶端数据调整到相应位置； 重复步骤 2，直到堆的尺寸为 1。 2. 动图演示 代码实现JavaScript实例1234567891011121314151617181920212223242526272829303132333435363738394041424344var len; // 因为声明的多个函数都需要数据长度，所以把len设置成为全局变量function buildMaxHeap(arr) &#123; // 建立大顶堆 len = arr.length; for (var i = Math.floor(len/2); i &gt;= 0; i--) &#123; heapify(arr, i); &#125;&#125;function heapify(arr, i) &#123; // 堆调整 var left = 2 * i + 1, right = 2 * i + 2, largest = i; if (left &lt; len &amp;&amp; arr[left] &gt; arr[largest]) &#123; largest = left; &#125; if (right &lt; len &amp;&amp; arr[right] &gt; arr[largest]) &#123; largest = right; &#125; if (largest != i) &#123; swap(arr, i, largest); heapify(arr, largest); &#125;&#125;function swap(arr, i, j) &#123; var temp = arr[i]; arr[i] = arr[j]; arr[j] = temp;&#125;function heapSort(arr) &#123; buildMaxHeap(arr); for (var i = arr.length-1; i &gt; 0; i--) &#123; swap(arr, 0, i); len--; heapify(arr, 0); &#125; return arr;&#125; Python实例123456789101112131415161718192021222324252627282930def buildMaxHeap(arr): import math for i in range(math.floor(len(arr)/2),-1,-1): heapify(arr,i)def heapify(arr, i): left = 2*i+1 right = 2*i+2 largest = i if left &lt; arrLen and arr[left] &gt; arr[largest]: largest = left if right &lt; arrLen and arr[right] &gt; arr[largest]: largest = right if largest != i: swap(arr, i, largest) heapify(arr, largest)def swap(arr, i, j): arr[i], arr[j] = arr[j], arr[i]def heapSort(arr): global arrLen arrLen = len(arr) buildMaxHeap(arr) for i in range(len(arr)-1,0,-1): swap(arr,0,i) arrLen -=1 heapify(arr, 0) return arr Go实例123456789101112131415161718192021222324252627282930313233343536func heapSort(arr []int) []int &#123; arrLen := len(arr) buildMaxHeap(arr, arrLen) for i := arrLen - 1; i &gt;= 0; i-- &#123; swap(arr, 0, i) arrLen -= 1 heapify(arr, 0, arrLen) &#125; return arr&#125;func buildMaxHeap(arr []int, arrLen int) &#123; for i := arrLen / 2; i &gt;= 0; i-- &#123; heapify(arr, i, arrLen) &#125;&#125;func heapify(arr []int, i, arrLen int) &#123; left := 2*i + 1 right := 2*i + 2 largest := i if left &lt; arrLen &amp;&amp; arr[left] &gt; arr[largest] &#123; largest = left &#125; if right &lt; arrLen &amp;&amp; arr[right] &gt; arr[largest] &#123; largest = right &#125; if largest != i &#123; swap(arr, i, largest) heapify(arr, largest, arrLen) &#125;&#125;func swap(arr []int, i, j int) &#123; arr[i], arr[j] = arr[j], arr[i]&#125; Java实例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public class HeapSort implements IArraySort &#123; @Override public int[] sort(int[] sourceArray) throws Exception &#123; // 对 arr 进行拷贝，不改变参数内容 int[] arr = Arrays.copyOf(sourceArray, sourceArray.length); int len = arr.length; buildMaxHeap(arr, len); for (int i = len - 1; i &gt; 0; i--) &#123; swap(arr, 0, i); len--; heapify(arr, 0, len); &#125; return arr; &#125; private void buildMaxHeap(int[] arr, int len) &#123; for (int i = (int) Math.floor(len / 2); i &gt;= 0; i--) &#123; heapify(arr, i, len); &#125; &#125; private void heapify(int[] arr, int i, int len) &#123; int left = 2 * i + 1; int right = 2 * i + 2; int largest = i; if (left &lt; len &amp;&amp; arr[left] &gt; arr[largest]) &#123; largest = left; &#125; if (right &lt; len &amp;&amp; arr[right] &gt; arr[largest]) &#123; largest = right; &#125; if (largest != i) &#123; swap(arr, i, largest); heapify(arr, largest, len); &#125; &#125; private void swap(int[] arr, int i, int j) &#123; int temp = arr[i]; arr[i] = arr[j]; arr[j] = temp; &#125;&#125; PHP实例1234567891011121314151617181920212223242526272829303132333435363738394041424344454647function buildMaxHeap(&amp;$arr)&#123; global $len; for ($i = floor($len/2); $i &gt;= 0; $i--) &#123; heapify($arr, $i); &#125;&#125;function heapify(&amp;$arr, $i)&#123; global $len; $left = 2 * $i + 1; $right = 2 * $i + 2; $largest = $i; if ($left &lt; $len &amp;&amp; $arr[$left] &gt; $arr[$largest]) &#123; $largest = $left; &#125; if ($right &lt; $len &amp;&amp; $arr[$right] &gt; $arr[$largest]) &#123; $largest = $right; &#125; if ($largest != $i) &#123; swap($arr, $i, $largest); heapify($arr, $largest); &#125;&#125;function swap(&amp;$arr, $i, $j)&#123; $temp = $arr[$i]; $arr[$i] = $arr[$j]; $arr[$j] = $temp;&#125;function heapSort($arr) &#123; global $len; $len = count($arr); buildMaxHeap($arr); for ($i = count($arr) - 1; $i &gt; 0; $i--) &#123; swap($arr, 0, $i); $len--; heapify($arr, 0); &#125; return $arr;&#125; C实例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;void swap(int *a, int *b) &#123; int temp = *b; *b = *a; *a = temp;&#125;void max_heapify(int arr[], int start, int end) &#123; // 建立父節點指標和子節點指標 int dad = start; int son = dad * 2 + 1; while (son &lt;= end) &#123; // 若子節點指標在範圍內才做比較 if (son + 1 &lt;= end &amp;&amp; arr[son] &lt; arr[son + 1]) // 先比較兩個子節點大小，選擇最大的 son++; if (arr[dad] &gt; arr[son]) //如果父節點大於子節點代表調整完畢，直接跳出函數 return; else &#123; // 否則交換父子內容再繼續子節點和孫節點比較 swap(&amp;arr[dad], &amp;arr[son]); dad = son; son = dad * 2 + 1; &#125; &#125;&#125;void heap_sort(int arr[], int len) &#123; int i; // 初始化，i從最後一個父節點開始調整 for (i = len / 2 - 1; i &gt;= 0; i--) max_heapify(arr, i, len - 1); // 先將第一個元素和已排好元素前一位做交換，再重新調整，直到排序完畢 for (i = len - 1; i &gt; 0; i--) &#123; swap(&amp;arr[0], &amp;arr[i]); max_heapify(arr, 0, i - 1); &#125;&#125;int main() &#123; int arr[] = &#123; 3, 5, 3, 0, 8, 6, 1, 5, 8, 6, 2, 4, 9, 4, 7, 0, 1, 8, 9, 7, 3, 1, 2, 5, 9, 7, 4, 0, 2, 6 &#125;; int len = (int) sizeof(arr) / sizeof(*arr); heap_sort(arr, len); int i; for (i = 0; i &lt; len; i++) printf("%d ", arr[i]); printf("\n"); return 0;&#125;]]></content>
      <tags>
        <tag>排序算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序算法之快速排序]]></title>
    <url>%2F2019%2F07%2F23%2F%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E4%B9%8B%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[快速排序快速排序是由东尼·霍尔所发展的一种排序算法。在平均状况下，排序 n 个项目要 Ο(nlogn) 次比较。在最坏状况下则需要 Ο(n2) 次比较，但这种状况并不常见。事实上，快速排序通常明显比其他 Ο(nlogn) 算法更快，因为它的内部循环（inner loop）可以在大部分的架构上很有效率地被实现出来。 快速排序使用分治法（Divide and conquer）策略来把一个串行（list）分为两个子串行（sub-lists）。 快速排序又是一种分而治之思想在排序算法上的典型应用。本质上来看，快速排序应该算是在冒泡排序基础上的递归分治法。 快速排序的名字起的是简单粗暴，因为一听到这个名字你就知道它存在的意义，就是快，而且效率高！它是处理大数据最快的排序算法之一了。虽然 Worst Case 的时间复杂度达到了 O(n²)，但是人家就是优秀，在大多数情况下都比平均时间复杂度为 O(n logn) 的排序算法表现要更好，可是这是为什么呢，我也不知道。好在我的强迫症又犯了，查了 N 多资料终于在《算法艺术与信息学竞赛》上找到了满意的答案： 快速排序的最坏运行情况是 O(n²)，比如说顺序数列的快排。但它的平摊期望时间是 O(nlogn)，且 O(nlogn) 记号中隐含的常数因子很小，比复杂度稳定等于 O(nlogn) 的归并排序要小很多。所以，对绝大多数顺序性较弱的随机数列而言，快速排序总是优于归并排序。 1. 算法步骤 从数列中挑出一个元素，称为 “基准”（pivot）; 重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区退出之后，该基准就处于数列的中间位置。这个称为分区（partition）操作； 递归地（recursive）把小于基准值元素的子数列和大于基准值元素的子数列排序； 2. 动图演示 代码实现JavaScript实例1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556function quickSort(arr, left, right) &#123; var len = arr.length, partitionIndex, left = typeof left != 'number' ? 0 : left, right = typeof right != 'number' ? len - 1 : right; if (left &lt; right) &#123; partitionIndex = partition(arr, left, right); quickSort(arr, left, partitionIndex-1); quickSort(arr, partitionIndex+1, right); &#125; return arr;&#125;function partition(arr, left ,right) &#123; // 分区操作 var pivot = left, // 设定基准值（pivot） index = pivot + 1; for (var i = index; i &lt;= right; i++) &#123; if (arr[i] &lt; arr[pivot]) &#123; swap(arr, i, index); index++; &#125; &#125; swap(arr, pivot, index - 1); return index-1;&#125;function swap(arr, i, j) &#123; var temp = arr[i]; arr[i] = arr[j]; arr[j] = temp;&#125;function partition2(arr, low, high) &#123; let pivot = arr[low]; while (low &lt; high) &#123; while (low &lt; high &amp;&amp; arr[high] &gt; pivot) &#123; --high; &#125; arr[low] = arr[high]; while (low &lt; high &amp;&amp; arr[low] &lt;= pivot) &#123; ++low; &#125; arr[high] = arr[low]; &#125; arr[low] = pivot; return low;&#125;function quickSort2(arr, low, high) &#123; if (low &lt; high) &#123; let pivot = partition2(arr, low, high); quickSort2(arr, low, pivot - 1); quickSort2(arr, pivot + 1, high); &#125; return arr;&#125; Python实例1234567891011121314151617181920212223def quickSort(arr, left=None, right=None): left = 0 if not isinstance(left,(int, float)) else left right = len(arr)-1 if not isinstance(right,(int, float)) else right if left &lt; right: partitionIndex = partition(arr, left, right) quickSort(arr, left, partitionIndex-1) quickSort(arr, partitionIndex+1, right) return arrdef partition(arr, left, right): pivot = left index = pivot+1 i = index while i &lt;= right: if arr[i] &lt; arr[pivot]: swap(arr, i, index) index+=1 i+=1 swap(arr,pivot,index-1) return index-1def swap(arr, i, j): arr[i], arr[j] = arr[j], arr[i] Go实例123456789101112131415161718192021222324252627282930func quickSort(arr []int) []int &#123; return _quickSort(arr, 0, len(arr)-1)&#125;func _quickSort(arr []int, left, right int) []int &#123; if left &lt; right &#123; partitionIndex := partition(arr, left, right) _quickSort(arr, left, partitionIndex-1) _quickSort(arr, partitionIndex+1, right) &#125; return arr&#125;func partition(arr []int, left, right int) int &#123; pivot := left index := pivot + 1 for i := index; i &lt;= right; i++ &#123; if arr[i] &lt; arr[pivot] &#123; swap(arr, i, index) index += 1 &#125; &#125; swap(arr, pivot, index-1) return index - 1&#125;func swap(arr []int, i, j int) &#123; arr[i], arr[j] = arr[j], arr[i]&#125; Java实例12345678910111213141516171819202122232425262728293031323334353637383940public class QuickSort implements IArraySort &#123; @Override public int[] sort(int[] sourceArray) throws Exception &#123; // 对 arr 进行拷贝，不改变参数内容 int[] arr = Arrays.copyOf(sourceArray, sourceArray.length); return quickSort(arr, 0, arr.length - 1); &#125; private int[] quickSort(int[] arr, int left, int right) &#123; if (left &lt; right) &#123; int partitionIndex = partition(arr, left, right); quickSort(arr, left, partitionIndex - 1); quickSort(arr, partitionIndex + 1, right); &#125; return arr; &#125; private int partition(int[] arr, int left, int right) &#123; // 设定基准值（pivot） int pivot = left; int index = pivot + 1; for (int i = index; i &lt;= right; i++) &#123; if (arr[i] &lt; arr[pivot]) &#123; swap(arr, i, index); index++; &#125; &#125; swap(arr, pivot, index - 1); return index - 1; &#125; private void swap(int[] arr, int i, int j) &#123; int temp = arr[i]; arr[i] = arr[j]; arr[j] = temp; &#125;&#125; PHP实例1234567891011121314151617181920function quickSort($arr)&#123; if (count($arr) &lt;= 1) return $arr; $middle = $arr[0]; $leftArray = array(); $rightArray = array(); for ($i = 1; $i &lt; count($arr); $i++) &#123; if ($arr[$i] &gt; $middle) $rightArray[] = $arr[$i]; else $leftArray[] = $arr[$i]; &#125; $leftArray = quickSort($leftArray); $leftArray[] = $middle; $rightArray = quickSort($rightArray); return array_merge($leftArray, $rightArray);&#125;]]></content>
      <tags>
        <tag>排序算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序算法之归并排序]]></title>
    <url>%2F2019%2F07%2F22%2F%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E4%B9%8B%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[归并排序归并排序（Merge sort）是建立在归并操作上的一种有效的排序算法。该算法是采用分治法（Divide and Conquer）的一个非常典型的应用。 作为一种典型的分而治之思想的算法应用，归并排序的实现由两种方法： 自上而下的递归（所有递归的方法都可以用迭代重写，所以就有了第 2 种方法）； 自下而上的迭代； 在《数据结构与算法 JavaScript 描述》中，作者给出了自下而上的迭代方法。但是对于递归法，作者却认为： However, it is not possible to do so in JavaScript, as the recursion goes too deep for the language to handle. 然而，在 JavaScript 中这种方式不太可行，因为这个算法的递归深度对它来讲太深了。 说实话，我不太理解这句话。意思是 JavaScript 编译器内存太小，递归太深容易造成内存溢出吗？还望有大神能够指教。 和选择排序一样，归并排序的性能不受输入数据的影响，但表现比选择排序好的多，因为始终都是 O(nlogn) 的时间复杂度。代价是需要额外的内存空间。 2. 算法步骤 申请空间，使其大小为两个已经排序序列之和，该空间用来存放合并后的序列； 设定两个指针，最初位置分别为两个已经排序序列的起始位置； 比较两个指针所指向的元素，选择相对小的元素放入到合并空间，并移动指针到下一位置； 重复步骤 3 直到某一指针达到序列尾； 将另一序列剩下的所有元素直接复制到合并序列尾。 3. 动图演示 代码实现JavaScript实例12345678910111213141516171819202122232425262728293031function mergeSort(arr) &#123; // 采用自上而下的递归方法 var len = arr.length; if(len &lt; 2) &#123; return arr; &#125; var middle = Math.floor(len / 2), left = arr.slice(0, middle), right = arr.slice(middle); return merge(mergeSort(left), mergeSort(right));&#125;function merge(left, right)&#123; var result = []; while (left.length &amp;&amp; right.length) &#123; if (left[0] &lt;= right[0]) &#123; result.push(left.shift()); &#125; else &#123; result.push(right.shift()); &#125; &#125; while (left.length) result.push(left.shift()); while (right.length) result.push(right.shift()); return result;&#125; Python实例1234567891011121314151617181920def mergeSort(arr): import math if(len(arr)&lt;2): return arr middle = math.floor(len(arr)/2) left, right = arr[0:middle], arr[middle:] return merge(mergeSort(left), mergeSort(right))def merge(left,right): result = [] while left and right: if left[0] &lt;= right[0]: result.append(left.pop(0)); else: result.append(right.pop(0)); while left: result.append(left.pop(0)); while right: result.append(right.pop(0)); return result Go实例1234567891011121314151617181920212223242526272829303132333435func mergeSort(arr []int) []int &#123; length := len(arr) if length &lt; 2 &#123; return arr &#125; middle := length / 2 left := arr[0:middle] right := arr[middle:] return merge(mergeSort(left), mergeSort(right))&#125;func merge(left []int, right []int) []int &#123; var result []int for len(left) != 0 &amp;&amp; len(right) != 0 &#123; if left[0] &lt;= right[0] &#123; result = append(result, left[0]) left = left[1:] &#125; else &#123; result = append(result, right[0]) right = right[1:] &#125; &#125; for len(left) != 0 &#123; result = append(result, left[0]) left = left[1:] &#125; for len(right) != 0 &#123; result = append(result, right[0]) right = right[1:] &#125; return result&#125; Java实例123456789101112131415161718192021222324252627282930313233343536373839404142434445public class MergeSort implements IArraySort &#123; @Override public int[] sort(int[] sourceArray) throws Exception &#123; // 对 arr 进行拷贝，不改变参数内容 int[] arr = Arrays.copyOf(sourceArray, sourceArray.length); if (arr.length &lt; 2) &#123; return arr; &#125; int middle = (int) Math.floor(arr.length / 2); int[] left = Arrays.copyOfRange(arr, 0, middle); int[] right = Arrays.copyOfRange(arr, middle, arr.length); return merge(sort(left), sort(right)); &#125; protected int[] merge(int[] left, int[] right) &#123; int[] result = new int[left.length + right.length]; int i = 0; while (left.length &gt; 0 &amp;&amp; right.length &gt; 0) &#123; if (left[0] &lt;= right[0]) &#123; result[i++] = left[0]; left = Arrays.copyOfRange(left, 1, left.length); &#125; else &#123; result[i++] = right[0]; right = Arrays.copyOfRange(right, 1, right.length); &#125; &#125; while (left.length &gt; 0) &#123; result[i++] = left[0]; left = Arrays.copyOfRange(left, 1, left.length); &#125; while (right.length &gt; 0) &#123; result[i++] = right[0]; right = Arrays.copyOfRange(right, 1, right.length); &#125; return result; &#125;&#125; PHP实例1234567891011121314151617181920212223242526272829303132function mergeSort($arr)&#123; $len = count($arr); if ($len &lt; 2) &#123; return $arr; &#125; $middle = floor($len / 2); $left = array_slice($arr, 0, $middle); $right = array_slice($arr, $middle); return merge(mergeSort($left), mergeSort($right));&#125;function merge($left, $right)&#123; $result = []; while (count($left) &gt; 0 &amp;&amp; count($right) &gt; 0) &#123; if ($left[0] &lt;= $right[0]) &#123; $result[] = array_shift($left); &#125; else &#123; $result[] = array_shift($right); &#125; &#125; while (count($left)) $result[] = array_shift($left); while (count($right)) $result[] = array_shift($right); return $result;&#125; C实例1234567891011121314151617181920212223242526272829303132int min(int x, int y) &#123; return x &lt; y ? x : y;&#125;void merge_sort(int arr[], int len) &#123; int *a = arr; int *b = (int *) malloc(len * sizeof(int)); int seg, start; for (seg = 1; seg &lt; len; seg += seg) &#123; for (start = 0; start &lt; len; start += seg * 2) &#123; int low = start, mid = min(start + seg, len), high = min(start + seg * 2, len); int k = low; int start1 = low, end1 = mid; int start2 = mid, end2 = high; while (start1 &lt; end1 &amp;&amp; start2 &lt; end2) b[k++] = a[start1] &lt; a[start2] ? a[start1++] : a[start2++]; while (start1 &lt; end1) b[k++] = a[start1++]; while (start2 &lt; end2) b[k++] = a[start2++]; &#125; int *temp = a; a = b; b = temp; &#125; if (a != arr) &#123; int i; for (i = 0; i &lt; len; i++) b[i] = a[i]; b = a; &#125; free(b);&#125;]]></content>
      <tags>
        <tag>排序算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序算法之希尔排序]]></title>
    <url>%2F2019%2F07%2F21%2F%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E4%B9%8B%E5%B8%8C%E5%B0%94%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[希尔排序希尔排序，也称递减增量排序算法，是插入排序的一种更高效的改进版本。但希尔排序是非稳定排序算法。 希尔排序是基于插入排序的以下两点性质而提出改进方法的： 插入排序在对几乎已经排好序的数据操作时，效率高，即可以达到线性排序的效率； 但插入排序一般来说是低效的，因为插入排序每次只能将数据移动一位； 希尔排序的基本思想是：先将整个待排序的记录序列分割成为若干子序列分别进行直接插入排序，待整个序列中的记录”基本有序”时，再对全体记录进行依次直接插入排序。 1. 算法步骤选择一个增量序列 t1，t2，……，tk，其中 ti &gt; tj, tk = 1； 按增量序列个数 k，对序列进行 k 趟排序； 每趟排序，根据对应的增量 ti，将待排序列分割成若干长度为 m 的子序列，分别对各子表进行直接插入排序。仅增量因子为 1 时，整个序列作为一个表来处理，表长度即为整个序列的长度。 2. 动图演示 代码实现JavaScript实例123456789101112131415161718function shellSort(arr) &#123; var len = arr.length, temp, gap = 1; while(gap &lt; len/3) &#123; //动态定义间隔序列 gap =gap*3+1; &#125; for (gap; gap &gt; 0; gap = Math.floor(gap/3)) &#123; for (var i = gap; i &lt; len; i++) &#123; temp = arr[i]; for (var j = i-gap; j &gt;= 0 &amp;&amp; arr[j] &gt; temp; j-=gap) &#123; arr[j+gap] = arr[j]; &#125; arr[j+gap] = temp; &#125; &#125; return arr;&#125; Python实例12345678910111213141516def shellSort(arr): import math gap=1 while(gap &lt; len(arr)/3): gap = gap*3+1 while gap &gt; 0: for i in range(gap,len(arr)): temp = arr[i] j = i-gap while j &gt;=0 and arr[j] &gt; temp: arr[j+gap]=arr[j] j-=gap arr[j+gap] = temp gap = math.floor(gap/3) return arr&#125; Go实例1234567891011121314151617181920func shellSort(arr []int) []int &#123; length := len(arr) gap := 1 for gap &lt; gap/3 &#123; gap = gap*3 + 1 &#125; for gap &gt; 0 &#123; for i := gap; i &lt; length; i++ &#123; temp := arr[i] j := i - gap for j &gt;= 0 &amp;&amp; arr[j] &gt; temp &#123; arr[j+gap] = arr[j] j -= gap &#125; arr[j+gap] = temp &#125; gap = gap / 3 &#125; return arr&#125; Java实例12345678910111213141516171819202122232425262728public class ShellSort implements IArraySort &#123; @Override public int[] sort(int[] sourceArray) throws Exception &#123; // 对 arr 进行拷贝，不改变参数内容 int[] arr = Arrays.copyOf(sourceArray, sourceArray.length); int gap = 1; while (gap &lt; arr.length) &#123; gap = gap * 3 + 1; &#125; while (gap &gt; 0) &#123; for (int i = gap; i &lt; arr.length; i++) &#123; int tmp = arr[i]; int j = i - gap; while (j &gt;= 0 &amp;&amp; arr[j] &gt; tmp) &#123; arr[j + gap] = arr[j]; j -= gap; &#125; arr[j + gap] = tmp; &#125; gap = (int) Math.floor(gap / 3); &#125; return arr; &#125;&#125; PHP实例12345678910111213141516171819function shellSort($arr)&#123; $len = count($arr); $temp = 0; $gap = 1; while($gap &lt; $len / 3) &#123; $gap = $gap * 3 + 1; &#125; for ($gap; $gap &gt; 0; $gap = floor($gap / 3)) &#123; for ($i = $gap; $i &lt; $len; $i++) &#123; $temp = $arr[$i]; for ($j = $i - $gap; $j &gt;= 0 &amp;&amp; $arr[$j] &gt; $temp; $j -= $gap) &#123; $arr[$j+$gap] = $arr[$j]; &#125; $arr[$j+$gap] = $temp; &#125; &#125; return $arr;&#125; C实例1234567891011void shell_sort(int arr[], int len) &#123; int gap, i, j; int temp; for (gap = len &gt;&gt; 1; gap &gt; 0; gap &gt;&gt;= 1) for (i = gap; i &lt; len; i++) &#123; temp = arr[i]; for (j = i - gap; j &gt;= 0 &amp;&amp; arr[j] &gt; temp; j -= gap) arr[j + gap] = arr[j]; arr[j + gap] = temp; &#125;&#125;]]></content>
      <tags>
        <tag>排序算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序算法之插入排序]]></title>
    <url>%2F2019%2F07%2F20%2F%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E4%B9%8B%E6%8F%92%E5%85%A5%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[插入排序插入排序的代码实现虽然没有冒泡排序和选择排序那么简单粗暴，但它的原理应该是最容易理解的了，因为只要打过扑克牌的人都应该能够秒懂。插入排序是一种最简单直观的排序算法，它的工作原理是通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入。 插入排序和冒泡排序一样，也有一种优化算法，叫做拆半插入。 1. 算法步骤将第一待排序序列第一个元素看做一个有序序列，把第二个元素到最后一个元素当成是未排序序列。 从头到尾依次扫描未排序序列，将扫描到的每个元素插入有序序列的适当位置。（如果待插入的元素与有序序列中的某个元素相等，则将待插入元素插入到相等元素的后面。） 2. 动图演示 代码实现JavaScript实例1234567891011121314function insertionSort(arr) &#123; var len = arr.length; var preIndex, current; for (var i = 1; i &lt; len; i++) &#123; preIndex = i - 1; current = arr[i]; while(preIndex &gt;= 0 &amp;&amp; arr[preIndex] &gt; current) &#123; arr[preIndex+1] = arr[preIndex]; preIndex--; &#125; arr[preIndex+1] = current; &#125; return arr;&#125; Python实例123456789def insertionSort(arr): for i in range(len(arr)): preIndex = i-1 current = arr[i] while preIndex &gt;= 0 and arr[preIndex] &gt; current: arr[preIndex+1] = arr[preIndex] preIndex-=1 arr[preIndex+1] = current return arr Go实例123456789101112func insertionSort(arr []int) []int &#123; for i := range arr &#123; preIndex := i - 1 current := arr[i] for preIndex &gt;= 0 &amp;&amp; arr[preIndex] &gt; current &#123; arr[preIndex+1] = arr[preIndex] preIndex -= 1 &#125; arr[preIndex+1] = current &#125; return arr&#125; Java实例1234567891011121314151617181920212223242526272829public class InsertSort implements IArraySort &#123; @Override public int[] sort(int[] sourceArray) throws Exception &#123; // 对 arr 进行拷贝，不改变参数内容 int[] arr = Arrays.copyOf(sourceArray, sourceArray.length); // 从下标为1的元素开始选择合适的位置插入，因为下标为0的只有一个元素，默认是有序的 for (int i = 1; i &lt; arr.length; i++) &#123; // 记录要插入的数据 int tmp = arr[i]; // 从已经排序的序列最右边的开始比较，找到比其小的数 int j = i; while (j &gt; 0 &amp;&amp; tmp &lt; arr[j - 1]) &#123; arr[j] = arr[j - 1]; j--; &#125; // 存在比其小的数，插入 if (j != i) &#123; arr[j] = tmp; &#125; &#125; return arr; &#125;&#125; PHP实例1234567891011121314function insertionSort($arr)&#123; $len = count($arr); for ($i = 1; $i &lt; $len; $i++) &#123; $preIndex = $i - 1; $current = $arr[$i]; while($preIndex &gt;= 0 &amp;&amp; $arr[$preIndex] &gt; $current) &#123; $arr[$preIndex+1] = $arr[$preIndex]; $preIndex--; &#125; $arr[$preIndex+1] = $current; &#125; return $arr;&#125; Swift实例12345678for i in 1..&lt;arr.endIndex &#123; let temp = arr[i] for j in (0..&lt;i).reversed() &#123; if arr[j] &gt; temp &#123; arr.swapAt(j, j+1) &#125; &#125;&#125;]]></content>
      <tags>
        <tag>排序算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序算法之选择排序]]></title>
    <url>%2F2019%2F07%2F19%2F%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E4%B9%8B%E9%80%89%E6%8B%A9%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[选择排序选择排序是一种简单直观的排序算法，无论什么数据进去都是 O(n²) 的时间复杂度。所以用到它的时候，数据规模越小越好。唯一的好处可能就是不占用额外的内存空间了吧。 1. 算法步骤首先在未排序序列中找到最小（大）元素，存放到排序序列的起始位置。 再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾。 重复第二步，直到所有元素均排序完毕。 2. 动图演示 代码实现JavaScript 代码实现实例12345678910111213141516function selectionSort(arr) &#123; var len = arr.length; var minIndex, temp; for (var i = 0; i &lt; len - 1; i++) &#123; minIndex = i; for (var j = i + 1; j &lt; len; j++) &#123; if (arr[j] &lt; arr[minIndex]) &#123; // 寻找最小的数 minIndex = j; // 将最小数的索引保存 &#125; &#125; temp = arr[i]; arr[i] = arr[minIndex]; arr[minIndex] = temp; &#125; return arr;&#125; Python 代码实现实例1234567891011def selectionSort(arr): for i in range(len(arr) - 1): # 记录最小数的索引 minIndex = i for j in range(i + 1, len(arr)): if arr[j] &lt; arr[minIndex]: minIndex = j # i 不是最小数时，将 i 和最小数进行交换 if i != minIndex: arr[i], arr[minIndex] = arr[minIndex], arr[i] return arr Go 代码实现实例12345678910111213func selectionSort(arr []int) []int &#123; length := len(arr) for i := 0; i &lt; length-1; i++ &#123; min := i for j := i + 1; j &lt; length; j++ &#123; if arr[min] &gt; arr[j] &#123; min = j &#125; &#125; arr[i], arr[min] = arr[min], arr[i] &#125; return arr&#125; Java 代码实现实例1234567891011121314151617181920212223242526272829public class SelectionSort implements IArraySort &#123; @Override public int[] sort(int[] sourceArray) throws Exception &#123; int[] arr = Arrays.copyOf(sourceArray, sourceArray.length); // 总共要经过 N-1 轮比较 for (int i = 0; i &lt; arr.length - 1; i++) &#123; int min = i; // 每轮需要比较的次数 N-i for (int j = i + 1; j &lt; arr.length; j++) &#123; if (arr[j] &lt; arr[min]) &#123; // 记录目前能找到的最小值元素的下标 min = j; &#125; &#125; // 将找到的最小值和i位置所在的值进行交换 if (i != min) &#123; int tmp = arr[i]; arr[i] = arr[min]; arr[min] = tmp; &#125; &#125; return arr; &#125;&#125; PHP 代码实现实例12345678910111213141516function selectionSort($arr)&#123; $len = count($arr); for ($i = 0; $i &lt; $len - 1; $i++) &#123; $minIndex = $i; for ($j = $i + 1; $j &lt; $len; $j++) &#123; if ($arr[$j] &lt; $arr[$minIndex]) &#123; $minIndex = $j; &#125; &#125; $temp = $arr[$i]; $arr[$i] = $arr[$minIndex]; $arr[$minIndex] = $temp; &#125; return $arr;&#125; C 语言实例12345678910111213141516171819void swap(int *a,int *b) //交換兩個變數&#123; int temp = *a; *a = *b; *b = temp;&#125;void selection_sort(int arr[], int len) &#123; int i,j; for (i = 0 ; i &lt; len - 1 ; i++) &#123; int min = i; for (j = i + 1; j &lt; len; j++) //走訪未排序的元素 if (arr[j] &lt; arr[min]) //找到目前最小值 min = j; //紀錄最小值 swap(&amp;arr[min], &amp;arr[i]); //做交換 &#125;&#125; Swift实例123456789101112131415import Foundation/// 选择排序////// - Parameter list: 需要排序的数组func selectionSort(_ list: inout [Int]) -&gt; Void &#123; for j in 0..&lt;list.count - 1 &#123; var minIndex = j for i in j..&lt;list.count &#123; if list[minIndex] &gt; list[i] &#123; minIndex = i &#125; &#125; list.swapAt(j, minIndex) &#125;&#125;]]></content>
      <tags>
        <tag>排序算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序算法之冒泡排序]]></title>
    <url>%2F2019%2F07%2F18%2F%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E4%B9%8B%E5%86%92%E6%B3%A1%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[冒泡排序冒泡排序（Bubble Sort）也是一种简单直观的排序算法。它重复地走访过要排序的数列，一次比较两个元素，如果他们的顺序错误就把他们交换过来。走访数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。这个算法的名字由来是因为越小的元素会经由交换慢慢”浮”到数列的顶端。 作为最简单的排序算法之一，冒泡排序给我的感觉就像 Abandon 在单词书里出现的感觉一样，每次都在第一页第一位，所以最熟悉。冒泡排序还有一种优化算法，就是立一个 flag，当在一趟序列遍历中元素没有发生交换，则证明该序列已经有序。但这种改进对于提升性能来 说并没有什么太大作用。 1. 算法步骤比较相邻的元素。如果第一个比第二个大，就交换他们两个。 对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对。这步做完后，最后的元素会是最大的数。 针对所有的元素重复以上的步骤，除了最后一个。 持续每次对越来越少的元素重复上面的步骤，直到没有任何一对数字需要比较。 2. 动图演示 3. 什么时候最快当输入的数据已经是正序时（都已经是正序了，我还要你冒泡排序有何用啊）。 4. 什么时候最慢当输入的数据是反序时（写一个 for 循环反序输出数据不就行了，干嘛要用你冒泡排序呢，我是闲的吗）。 5. JavaScript 代码实现实例12345678910111213function bubbleSort(arr) &#123; var len = arr.length; for (var i = 0; i &lt; len - 1; i++) &#123; for (var j = 0; j &lt; len - 1 - i; j++) &#123; if (arr[j] &gt; arr[j+1]) &#123; // 相邻元素两两对比 var temp = arr[j+1]; // 元素交换 arr[j+1] = arr[j]; arr[j] = temp; &#125; &#125; &#125; return arr;&#125; 6. Python 代码实现实例123456def bubbleSort(arr): for i in range(1, len(arr)): for j in range(0, len(arr)-i): if arr[j] &gt; arr[j+1]: arr[j], arr[j + 1] = arr[j + 1], arr[j] return arr 7. Go 代码实现实例1234567891011func bubbleSort(arr []int) []int &#123; length := len(arr) for i := 0; i &lt; length; i++ &#123; for j := 0; j &lt; length-1-i; j++ &#123; if arr[j] &gt; arr[j+1] &#123; arr[j], arr[j+1] = arr[j+1], arr[j] &#125; &#125; &#125; return arr&#125; 8. Java 代码实现实例12345678910111213141516171819202122232425262728public class BubbleSort implements IArraySort &#123; @Override public int[] sort(int[] sourceArray) throws Exception &#123; // 对 arr 进行拷贝，不改变参数内容 int[] arr = Arrays.copyOf(sourceArray, sourceArray.length); for (int i = 1; i &lt; arr.length; i++) &#123; // 设定一个标记，若为true，则表示此次循环没有进行交换，也就是待排序列已经有序，排序已经完成。 boolean flag = true; for (int j = 0; j &lt; arr.length - i; j++) &#123; if (arr[j] &gt; arr[j + 1]) &#123; int tmp = arr[j]; arr[j] = arr[j + 1]; arr[j + 1] = tmp; flag = false; &#125; &#125; if (flag) &#123; break; &#125; &#125; return arr; &#125;&#125; 9. PHP 代码实现实例1234567891011121314function bubbleSort($arr)&#123; $len = count($arr); for ($i = 0; $i &lt; $len - 1; $i++) &#123; for ($j = 0; $j &lt; $len - 1 - $i; $j++) &#123; if ($arr[$j] &gt; $arr[$j+1]) &#123; $tmp = $arr[$j]; $arr[$j] = $arr[$j+1]; $arr[$j+1] = $tmp; &#125; &#125; &#125; return $arr;&#125; 10. C 语言实例1234567891011121314151617181920#include &lt;stdio.h&gt;void bubble_sort(int arr[], int len) &#123; int i, j, temp; for (i = 0; i &lt; len - 1; i++) for (j = 0; j &lt; len - 1 - i; j++) if (arr[j] &gt; arr[j + 1]) &#123; temp = arr[j]; arr[j] = arr[j + 1]; arr[j + 1] = temp; &#125;&#125;int main() &#123; int arr[] = &#123; 22, 34, 3, 32, 82, 55, 89, 50, 37, 5, 64, 35, 9, 70 &#125;; int len = (int) sizeof(arr) / sizeof(*arr); bubble_sort(arr, len); int i; for (i = 0; i &lt; len; i++) printf("%d ", arr[i]); return 0;&#125; 11. Ruby实例123456789101112class Array def bubble_sort! for i in 0...(size - 1) for j in 0...(size - i - 1) self[j], self[j + 1] = self[j + 1], self[j] if self[j] &gt; self[j + 1] end end self endendputs [22, 34, 3, 32, 82, 55, 89, 50, 37, 5, 64, 35, 9, 70].bubble_sort! 14. Swift实例123456789101112131415161718192021222324import Foundationfunc bubbleSort (arr: inout [Int]) &#123; for i in 0..&lt;arr.count - 1 &#123; for j in 0..&lt;arr.count - 1 - i &#123; if arr[j] &gt; arr[j+1] &#123; arr.swapAt(j, j+1) &#125; &#125; &#125;&#125;// 测试调用func testSort () &#123; // 生成随机数数组进行排序操作 var list:[Int] = [] for _ in 0...99 &#123; list.append(Int(arc4random_uniform(100))) &#125; print("\(list)") bubbleSort(arr:&amp;list) print("\(list)")&#125;]]></content>
      <tags>
        <tag>排序算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL server 导出数据库表结构及注释]]></title>
    <url>%2F2019%2F05%2F27%2FSQL-server-%E5%AF%BC%E5%87%BA%E6%95%B0%E6%8D%AE%E5%BA%93%E8%A1%A8%E7%BB%93%E6%9E%84%E5%8F%8A%E6%B3%A8%E9%87%8A%2F</url>
    <content type="text"><![CDATA[1、sqlserver导出数据库表结构及语句如下，注意：只适用sqlserver数据库，只需要复制如下语句，修改倒数第三行固定表名即可查询。 1234567891011121314151617181920212223242526272829303132333435363738394041SELECT 表名 = Case When A.colorder=1 Then D.name Else &apos;&apos; End, 表说明 = Case When A.colorder=1 Then isnull(F.value,&apos;&apos;) Else &apos;&apos; End, 字段序号 = A.colorder, 字段名 = A.name, 字段说明 = isnull(G.[value],&apos;&apos;), 标识 = Case When COLUMNPROPERTY( A.id,A.name,&apos;IsIdentity&apos;)=1 Then &apos;√&apos;Else &apos;&apos; End, 主键 = Case When exists(SELECT 1 FROM sysobjects Where xtype=&apos;PK&apos; and parent_obj=A.id and name in ( SELECT name FROM sysindexes WHERE indid in( SELECT indid FROM sysindexkeys WHERE id = A.id AND colid=A.colid))) then &apos;√&apos; else &apos;&apos; end, 类型 = B.name, 占用字节数 = A.Length, 长度 = COLUMNPROPERTY(A.id,A.name,&apos;PRECISION&apos;), 小数位数 = isnull(COLUMNPROPERTY(A.id,A.name,&apos;Scale&apos;),0), 允许空 = Case When A.isnullable=1 Then &apos;√&apos;Else &apos;&apos; End, 默认值 = isnull(E.Text,&apos;&apos;) FROM syscolumns A Left Join systypes B On A.xusertype=B.xusertype Inner Join sysobjects D On A.id=D.id and D.xtype=&apos;U&apos; and D.name&lt;&gt;&apos;dtproperties&apos; Left Join syscomments E on A.cdefault=E.id Left Join sys.extended_properties G on A.id=G.major_id and A.colid=G.minor_id Left Join sys.extended_properties F On D.id=F.major_id and F.minor_id=0 where d.name=&apos;T_VerificationSuccessRate&apos; --如果只查询指定表,只修改此处即可 Order By A.id,A.colorder 2、导出结果如下：]]></content>
      <tags>
        <tag>sqlserver</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java解析HTML]]></title>
    <url>%2F2019%2F03%2F15%2Fjava%E8%A7%A3%E6%9E%90HTML%2F</url>
    <content type="text"><![CDATA[pom.xml引用如下： 12345678910111213141516171819202122232425262728293031323334353637383940&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;ZZ_GetNetworkProxy&lt;/groupId&gt; &lt;artifactId&gt;ZZ_GetNetworkProxy&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;ZZ_GetNetworkProxy&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;/properties&gt; &lt;dependencies&gt;&lt;!-- java解析HTML核心jar包 --&gt;&lt;!-- https://mvnrepository.com/artifact/org.jsoup/jsoup --&gt;&lt;dependency&gt; &lt;groupId&gt;org.jsoup&lt;/groupId&gt; &lt;artifactId&gt;jsoup&lt;/artifactId&gt; &lt;version&gt;1.11.3&lt;/version&gt;&lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/com.alibaba/fastjson --&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.47&lt;/version&gt;&lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;3.8.1&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 代码如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263package com.ccservice.util;import org.jsoup.Jsoup;import org.jsoup.nodes.Document;import org.jsoup.nodes.Element;import org.jsoup.nodes.Node;import org.jsoup.select.Elements;import com.alibaba.fastjson.JSONArray;import com.alibaba.fastjson.JSONObject;public class ProxyUtil &#123; public static void main(String[] args) &#123; //获取代理的个数。 ProxyUtil.getProxyArray(100); &#125; public static JSONArray getProxyArray(int proxyNum) &#123; JSONArray proxy = new JSONArray();//代理数组 int m=1; while (proxy.size()&lt;proxyNum) &#123;//如果不满足需求代理个数就一直查询 //获取代理的网址 String url = "https://www.xicidaili.com/nn/"+m; proxy=getProxy(proxy,url); m++; System.out.println("执行了"+m+"次===proxy大小为："+proxy.size()); &#125; return proxy; &#125; private static JSONArray getProxy(JSONArray proxy, String url) &#123; try &#123; Document doc = Jsoup.connect(url).get(); Element table = doc.getElementById("ip_list");// 获取表格对象。 Element tbody = table.child(0);// 获取表格内tbody int tableSize = tbody.childNodeSize(); Node tr = null; Document proxyIp = null; Document proxyProt = null; Document proxyTime = null; for (int i = 2; i &lt; tableSize; i = i + 2) &#123; JSONObject proxyObj = new JSONObject(); tr = tbody.childNode(i);// 取出行数据 proxyTime = Jsoup.parse(tr.childNode(13).toString());//获取耗时 Elements time = proxyTime.getElementsByAttribute("title"); String waitTime=time.attr("title").split("\\.")[0];//获取秒数 if(Integer.valueOf(waitTime)&lt;1)&#123;//如果小于1，说明耗时很短。为优质代理 proxyIp = Jsoup.parse(tr.childNode(3).toString());//获取IP地址 proxyProt = Jsoup.parse(tr.childNode(5).toString());//获取端口号 proxyObj.put("ip", proxyIp.text()); proxyObj.put("port", proxyProt.text()); System.out.println("ip："+proxyIp.text()+"===端口："+proxyProt.text()); proxy.add(proxyObj);//放入json数组 &#125; &#125; return proxy; &#125; catch (Exception e) &#123; System.err.println("获取异常了"); &#125; return proxy; &#125;&#125;]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[maven命令大全]]></title>
    <url>%2F2019%2F03%2F02%2Fmaven%E5%91%BD%E4%BB%A4%E5%A4%A7%E5%85%A8%2F</url>
    <content type="text"><![CDATA[1]]></content>
      <categories>
        <category>-maven</category>
      </categories>
      <tags>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java设计模式-工厂模式]]></title>
    <url>%2F2019%2F02%2F16%2Fjava%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E6%8A%BD%E8%B1%A1%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[例子下面的代码展示了如何使用抽象工厂模式。 我们将要创建形状和打印机。对于形状，我们会有圆形，矩形和正方形。对于打印机，我们将有纸张打印机，网络打印机和屏幕打印机。 对于shape，我们将创建Shape界面，如下所示： 123interface Shape &#123; void draw();&#125; 然后我们创建实现Shape接口的具体类。 123456789101112131415161718192021class Rectangle implements Shape &#123; @Override public void draw() &#123; System.out.println("Inside Rectangle::draw() method."); &#125;&#125;class Square implements Shape &#123; @Override public void draw() &#123; System.out.println("Inside Square::draw() method."); &#125;&#125;class Circle implements Shape &#123; @Override public void draw() &#123; System.out.println("Inside Circle::draw() method."); &#125;&#125; 我们为打印机创建一个界面。 123interface Printer&#123; void print();&#125; 然后我们创建实现Printer接口的具体类。 123456789101112131415161718192021class PaperPrinter implements Printer&#123; @Override public void print() &#123; System.out.println("paper"); &#125;&#125;class WebPrinter implements Printer&#123; @Override public void print() &#123; System.out.println("web"); &#125;&#125;class ScreenPrinter implements Printer&#123; @Override public void print() &#123; System.out.println("screen"); &#125;&#125; 最后，我们创建一个抽象类来获取打印机和形状对象的工厂。 1234abstract class AbstractFactory &#123; abstract Printer getPrinter(String type); abstract Shape getShape(String shape) ;&#125; 最后，我们创建Factory类，根据给定的信息扩展AbstractFactory以生成具体类的对象。 1234567891011121314151617181920212223242526272829303132333435363738394041424344class ShapeFactory extends AbstractFactory &#123; @Override public Shape getShape(String shapeType)&#123; if(shapeType == null)&#123; return null; &#125; if(shapeType.equalsIgnoreCase("CIRCLE"))&#123; return new Circle(); &#125; else if(shapeType.equalsIgnoreCase("RECTANGLE"))&#123; return new Rectangle(); &#125; else if(shapeType.equalsIgnoreCase("SQUARE"))&#123; return new Square(); &#125; return null; &#125; @Override Printer getPrinter(String type) &#123; return null; &#125;&#125;class PrinterFactory extends AbstractFactory &#123; @Override public Shape getShape(String shapeType)&#123; return null; &#125; @Override Printer getPrinter(String type) &#123; if(type == null)&#123; return null; &#125; if(type.equalsIgnoreCase("paper"))&#123; return new PaperPrinter(); &#125; else if(type.equalsIgnoreCase("web"))&#123; return new WebPrinter(); &#125; else if(type.equalsIgnoreCase("Screen"))&#123; return new ScreenPrinter(); &#125; return null; &#125;&#125; 创建一个Factory生成器/生产器类，通过传递Shape或Printer等信息来获取工厂。 12345678910class FactoryProducer &#123; public static AbstractFactory getFactory(String choice)&#123; if(choice.equalsIgnoreCase("SHAPE"))&#123; return new ShapeFactory(); &#125; else if(choice.equalsIgnoreCase("Printer"))&#123; return new PrinterFactory(); &#125; return null; &#125;&#125; 下面的代码展示了如何使用抽象工厂模式。 1234567891011121314151617181920212223242526272829303132333435public class Main &#123; public static void main(String[] args) &#123; //get shape factory AbstractFactory shapeFactory = FactoryProducer.getFactory("SHAPE"); //get an object of Shape Circle Shape shape1 = shapeFactory.getShape("CIRCLE"); //call draw method of Shape Circle shape1.draw(); //get an object of Shape Rectangle Shape shape2 = shapeFactory.getShape("RECTANGLE"); //call draw method of Shape Rectangle shape2.draw(); //get an object of Shape Square Shape shape3 = shapeFactory.getShape("SQUARE"); //call draw method of Shape Square shape3.draw(); //get printer factory AbstractFactory printerFactory = FactoryProducer.getFactory("printer"); Printer printer1 = printerFactory.getPrinter("Paper"); printer1.print(); Printer printer2 = printerFactory.getPrinter("Web"); printer2.print(); Printer printer3 = printerFactory.getPrinter("Screen"); printer3.print(); &#125;&#125; 测试执行结果： 123456Inside Circle::draw() methodInside Rectangle::draw() methodInside Square::draw methodpaperwebwcreen]]></content>
      <categories>
        <category>java设计模式</category>
      </categories>
      <tags>
        <tag>java设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java设计模式-工厂模式]]></title>
    <url>%2F2019%2F02%2F15%2Fjava%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[例子在以下部分中，我们将展示如何使用工厂模式创建对象。 由工厂模式创建的对象将是形状对象，如圆形，矩形。 首先，我们设计一个接口来表示Shape。 123public interface Shape &#123; void draw();&#125; 然后我们创建实现接口的具体类。 以下代码用于 Rectangle.java 123456public class Rectangle implements Shape &#123; @Override public void draw() &#123; System.out.println("Inside Rectangle::draw() method."); &#125;&#125; Square.java 1234567public class Square implements Shape &#123; @Override public void draw() &#123; System.out.println("Inside Square::draw() method."); &#125;&#125; Circle.java 1234567public class Circle implements Shape &#123; @Override public void draw() &#123; System.out.println("Inside Circle::draw() method."); &#125;&#125; 核心工厂模式是一个Factory类。以下代码显示了如何为Shape对象创建Factory类。 ShapeFactory类基于传递给getShape（）方法的String值创建Shape对象。如果String值为CIRCLE，它将创建一个Circle对象。 1234567891011121314151617public class ShapeFactory &#123; //use getShape method to get object of type shape public Shape getShape(String shapeType)&#123; if(shapeType == null)&#123; return null; &#125; if(shapeType.equalsIgnoreCase("CIRCLE"))&#123; return new Circle(); &#125; else if(shapeType.equalsIgnoreCase("RECTANGLE"))&#123; return new Rectangle(); &#125; else if(shapeType.equalsIgnoreCase("SQUARE"))&#123; return new Square(); &#125; return null; &#125;&#125; 以下代码具有main方法，并且它使用Factory类通过传递类型等信息来获取具体类的对象。 123456789101112131415161718192021222324public class Main &#123; public static void main(String[] args) &#123; ShapeFactory shapeFactory = new ShapeFactory(); //get an object of Circle and call its draw method. Shape shape1 = shapeFactory.getShape("CIRCLE"); //call draw method of Circle shape1.draw(); //get an object of Rectangle and call its draw method. Shape shape2 = shapeFactory.getShape("RECTANGLE"); //call draw method of Rectangle shape2.draw(); //get an object of Square and call its draw method. Shape shape3 = shapeFactory.getShape("SQUARE"); //call draw method of circle shape3.draw(); &#125;&#125; 上面的代码生成以下结果。 12345Inside Circle ::draw() methodinside Rectangle::draw() methodInside Square :: draw() method]]></content>
      <categories>
        <category>java设计模式</category>
      </categories>
      <tags>
        <tag>java设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java23种设计模式]]></title>
    <url>%2F2019%2F02%2F14%2Fjava23%E7%A7%8D%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[一、设计模式的分类总体来说设计模式分为三大类： 创建型模式，共五种：工厂方法模式、抽象工厂模式、单例模式、建造者模式、原型模式。 结构型模式，共七种：适配器模式、装饰器模式、代理模式、外观模式、桥接模式、组合模式、享元模式。 行为型模式，共十一种：策略模式、模板方法模式、观察者模式、迭代子模式、责任链模式、命令模式、备忘录模式、状态模式、访问者模式、中介者模式、解释器模式。 二、设计模式的六大原则总原则：开闭原则（Open Close Principle）开闭原则就是说对扩展开放，对修改关闭。在程序需要进行拓展的时候，不能去修改原有的代码，而是要扩展原有代码，实现一个热插拔的效果。所以一句话概括就是：为了使程序的扩展性好，易于维护和升级。想要达到这样的效果，我们需要使用接口和抽象类等，后面的具体设计中我们会提到这点。 1、单一职责原则不要存在多于一个导致类变更的原因，也就是说每个类应该实现单一的职责，如若不然，就应该把类拆分。 2、里氏替换原则（Liskov Substitution Principle）里氏代换原则(Liskov Substitution Principle LSP)面向对象设计的基本原则之一。 里氏代换原则中说，任何基类可以出现的地方，子类一定可以出现。 LSP是继承复用的基石，只有当衍生类可以替换掉基类，软件单位的功能不受到影响时，基类才能真正被复用，而衍生类也能够在基类的基础上增加新的行为。里氏代换原则是对“开-闭”原则的补充。实现“开-闭”原则的关键步骤就是抽象化。而基类与子类的继承关系就是抽象化的具体实现，所以里氏代换原则是对实现抽象化的具体步骤的规范。—— From Baidu 百科 历史替换原则中，子类对父类的方法尽量不要重写和重载。因为父类代表了定义好的结构，通过这个规范的接口与外界交互，子类不应该随便破坏它。 3、依赖倒转原则（Dependence Inversion Principle）这个是开闭原则的基础，具体内容：面向接口编程，依赖于抽象而不依赖于具体。写代码时用到具体类时，不与具体类交互，而与具体类的上层接口交互。 4、接口隔离原则（Interface Segregation Principle）这个原则的意思是：每个接口中不存在子类用不到却必须实现的方法，如果不然，就要将接口拆分。使用多个隔离的接口，比使用单个接口（多个接口方法集合到一个的接口）要好。 5、迪米特法则（最少知道原则）（Demeter Principle）就是说：一个类对自己依赖的类知道的越少越好。也就是说无论被依赖的类多么复杂，都应该将逻辑封装在方法的内部，通过public方法提供给外部。这样当被依赖的类变化时，才能最小的影响该类。 最少知道原则的另一个表达方式是：只与直接的朋友通信。类之间只要有耦合关系，就叫朋友关系。耦合分为依赖、关联、聚合、组合等。我们称出现为成员变量、方法参数、方法返回值中的类为直接朋友。局部变量、临时变量则不是直接的朋友。我们要求陌生的类不要作为局部变量出现在类中。 6、合成复用原则（Composite Reuse Principle）原则是尽量首先使用合成/聚合的方式，而不是使用继承。 三、Java的23中设计模式A、创建模式从这一块开始，我们详细介绍Java中23种设计模式的概念，应用场景等情况，并结合他们的特点及设计模式的原则进行分析。 首先，简单工厂模式不属于23中涉及模式，简单工厂一般分为：普通简单工厂、多方法简单工厂、静态方法简单工厂。 0、简单工厂模式简单工厂模式模式分为三种： 01、普通就是建立一个工厂类，对实现了同一接口的一些类进行实例的创建。首先看下关系图： 举例如下：（我们举一个发送邮件和短信的例子） 首先，创建二者的共同接口： 123public interface Sender &#123; public void Send(); &#125; 其次，创建实现类： 123456public class MailSender implements Sender &#123; @Override public void Send() &#123; System.out.println("this is mailsender!"); &#125; &#125; 1234567public class SmsSender implements Sender &#123; @Override public void Send() &#123; System.out.println("this is sms sender!"); &#125; &#125; 最后，建工厂类： 12345678910111213public class SendFactory &#123; public Sender produce(String type) &#123; if ("mail".equals(type)) &#123; return new MailSender(); &#125; else if ("sms".equals(type)) &#123; return new SmsSender(); &#125; else &#123; System.out.println("请输入正确的类型!"); return null; &#125; &#125; &#125; 我们来测试下： 12345678public class FactoryTest &#123; public static void main(String[] args) &#123; SendFactory factory = new SendFactory(); Sender sender = factory.produce("sms"); sender.Send(); &#125; &#125; 输出： 1this is sms sender! 02、多个方法是对普通工厂方法模式的改进，在普通工厂方法模式中，如果传递的字符串出错，则不能正确创建对象，而多个工厂方法模式是提供多个工厂方法，分别创建对象。 将上面的代码做下修改，改动下SendFactory类就行，如下： 123456789public class SendFactory &#123; public Sender produceMail()&#123; return new MailSender(); &#125; public Sender produceSms()&#123; return new SmsSender(); &#125; &#125; 测试类如下： 12345678public class FactoryTest &#123; public static void main(String[] args) &#123; SendFactory factory = new SendFactory(); Sender sender = factory.produceMail(); sender.Send(); &#125; &#125; 输出： 1this is mailsender! 03、多个静态方法将上面的多个工厂方法模式里的方法置为静态的，不需要创建实例，直接调用即可。 12345678910public class SendFactory &#123; public static Sender produceMail()&#123; return new MailSender(); &#125; public static Sender produceSms()&#123; return new SmsSender(); &#125; &#125; 1234567public class FactoryTest &#123; public static void main(String[] args) &#123; Sender sender = SendFactory.produceMail(); sender.Send(); &#125; &#125; 输出： 1this is mailsender! 总体来说，工厂模式适合：凡是出现了大量的产品需要创建，并且具有共同的接口时，可以通过工厂方法模式进行创建。在以上的三种模式中，第一种如果传入的字符串有误，不能正确创建对象，第三种相对于第二种，不需要实例化工厂类，所以，大多数情况下，我们会选用第三种——静态工厂方法模式。 1、工厂方法模式（Factory Method）简单工厂模式有一个问题就是，类的创建依赖工厂类，也就是说，如果想要拓展程序，必须对工厂类进行修改，这违背了闭包原则，所以，从设计角度考虑，有一定的问题，如何解决？就用到工厂方法模式，创建一个工厂接口和创建多个工厂实现类，这样一旦需要增加新的功能，直接增加新的工厂类就可以了，不需要修改之前的代码。 请看例子： 123public interface Sender &#123; public void Send(); &#125; 两个实现类： 123456public class MailSender implements Sender &#123; @Override public void Send() &#123; System.out.println("this is mailsender!"); &#125; &#125; 1234567public class SmsSender implements Sender &#123; @Override public void Send() &#123; System.out.println("this is sms sender!"); &#125; &#125; 两个工厂类： 1234567public class SendMailFactory implements Provider &#123; @Override public Sender produce()&#123; return new MailSender(); &#125; &#125; 1234567public class SendSmsFactory implements Provider&#123; @Override public Sender produce() &#123; return new SmsSender(); &#125; &#125; 在提供一个接口： 123public interface Provider &#123; public Sender produce(); &#125; 测试类： 12345678public class Test &#123; public static void main(String[] args) &#123; Provider provider = new SendMailFactory(); Sender sender = provider.produce(); sender.Send(); &#125; &#125; 其实这个模式的好处就是，如果你现在想增加一个功能：发及时信息，则只需做一个实现类，实现Sender接口，同时做一个工厂类，实现Provider接口，就OK了，无需去改动现成的代码。这样做，拓展性较好！ 2、抽象工厂模式工厂方法模式和抽象工厂模式不好分清楚，他们的区别如下： 12345678910111213141516工厂方法模式：一个抽象产品类，可以派生出多个具体产品类。 一个抽象工厂类，可以派生出多个具体工厂类。 每个具体工厂类只能创建一个具体产品类的实例。抽象工厂模式：多个抽象产品类，每个抽象产品类可以派生出多个具体产品类。 一个抽象工厂类，可以派生出多个具体工厂类。 每个具体工厂类可以创建多个具体产品类的实例，也就是创建的是一个产品线下的多个产品。 区别：工厂方法模式只有一个抽象产品类，而抽象工厂模式有多个。 工厂方法模式的具体工厂类只能创建一个具体产品类的实例，而抽象工厂模式可以创建多个。工厂方法创建 &quot;一种&quot; 产品，他的着重点在于&quot;怎么创建&quot;，也就是说如果你开发，你的大量代码很可能围绕着这种产品的构造，初始化这些细节上面。也因为如此，类似的产品之间有很多可以复用的特征，所以会和模版方法相随。 抽象工厂需要创建一些列产品，着重点在于&quot;创建哪些&quot;产品上，也就是说，如果你开发，你的主要任务是划分不同差异的产品线，并且尽量保持每条产品线接口一致，从而可以从同一个抽象工厂继承。 1234567891011对于java来说，你能见到的大部分抽象工厂模式都是这样的：---它的里面是一堆工厂方法，每个工厂方法返回某种类型的对象。比如说工厂可以生产鼠标和键盘。那么抽象工厂的实现类（它的某个具体子类）的对象都可以生产鼠标和键盘，但可能工厂A生产的是罗技的键盘和鼠标，工厂B是微软的。这样A和B就是工厂，对应于抽象工厂；每个工厂生产的鼠标和键盘就是产品，对应于工厂方法；用了工厂方法模式，你替换生成键盘的工厂方法，就可以把键盘从罗技换到微软。但是用了抽象工厂模式，你只要换家工厂，就可以同时替换鼠标和键盘一套。如果你要的产品有几十个，当然用抽象工厂模式一次替换全部最方便（这个工厂会替你用相应的工厂方法）所以说抽象工厂就像工厂，而工厂方法则像是工厂的一种产品生产线 3、单例模式（Singleton）单例对象（Singleton）是一种常用的设计模式。在Java应用中，单例对象能保证在一个JVM中，该对象只有一个实例存在。这样的模式有几个好处： 1、某些类创建比较频繁，对于一些大型的对象，这是一笔很大的系统开销。 2、省去了new操作符，降低了系统内存的使用频率，减轻GC压力。 3、有些类如交易所的核心交易引擎，控制着交易流程，如果该类可以创建多个的话，系统完全乱了。（比如一个军队出现了多个司令员同时指挥，肯定会乱成一团），所以只有使用单例模式，才能保证核心交易服务器独立控制整个流程。 首先我们写一个简单的单例类： 12345678910111213141516171819202122public class Singleton &#123; /* 持有私有静态实例，防止被引用，此处赋值为null，目的是实现延迟加载 */ private static Singleton instance = null; /* 私有构造方法，防止被实例化 */ private Singleton() &#123; &#125; /* 静态工程方法，创建实例 */ public static Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125; /* 如果该对象被用于序列化，可以保证对象在序列化前后保持一致 */ public Object readResolve() &#123; return instance; &#125; &#125; 这个类可以满足基本要求，但是，像这样毫无线程安全保护的类，如果我们把它放入多线程的环境下，肯定就会出现问题了，如何解决？我们首先会想到对getInstance方法加synchronized关键字，如下： 123456public static synchronized Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125; 但是，synchronized关键字锁住的是这个对象，这样的用法，在性能上会有所下降，因为每次调用getInstance()，都要对对象上锁，事实上，只有在第一次创建对象的时候需要加锁，之后就不需要了，所以，这个地方需要改进。我们改成下面这个： 12345678910public static Singleton getInstance() &#123; if (instance == null) &#123; synchronized (instance) &#123; if (instance == null) &#123; instance = new Singleton(); &#125; &#125; &#125; return instance; &#125; 似乎解决了之前提到的问题，将synchronized关键字加在了内部，也就是说当调用的时候是不需要加锁的，只有在instance为null，并创建对象的时候才需要加锁，性能有一定的提升。但是，这样的情况，还是有可能有问题的，看下面的情况：在Java指令中创建对象和赋值操作是分开进行的，也就是说instance = new Singleton();语句是分两步执行的。但是JVM并不保证这两个操作的先后顺序，也就是说有可能JVM会为新的Singleton实例分配空间，然后直接赋值给instance成员，然后再去初始化这个Singleton实例。这样就可能出错了，我们以A、B两个线程为例： a&gt;A、B线程同时进入了第一个if判断 b&gt;A首先进入synchronized块，由于instance为null，所以它执行instance = new Singleton(); c&gt;由于JVM内部的优化机制，JVM先画出了一些分配给Singleton实例的空白内存，并赋值给instance成员（注意此时JVM没有开始初始化这个实例），然后A离开了synchronized块。 d&gt;B进入synchronized块，由于instance此时不是null，因此它马上离开了synchronized块并将结果返回给调用该方法的程序。 e&gt;此时B线程打算使用Singleton实例，却发现它没有被初始化，于是错误发生了。 所以程序还是有可能发生错误，其实程序在运行过程是很复杂的，从这点我们就可以看出，尤其是在写多线程环境下的程序更有难度，有挑战性。我们对该程序做进一步优化： 123456private static class SingletonFactory&#123; private static Singleton instance = new Singleton(); &#125; public static Singleton getInstance()&#123; return SingletonFactory.instance; &#125; 实际情况是，单例模式使用内部类来维护单例的实现，JVM内部的机制能够保证当一个类被加载的时候，这个类的加载过程是线程互斥的。这样当我们第一次调用getInstance的时候，JVM能够帮我们保证instance只被创建一次，并且会保证把赋值给instance的内存初始化完毕，这样我们就不用担心上面的问题。同时该方法也只会在第一次调用的时候使用互斥机制，这样就解决了低性能问题。这样我们暂时总结一个完美的单例模式： 123456789101112131415161718192021public class Singleton &#123; /* 私有构造方法，防止被实例化 */ private Singleton() &#123; &#125; /* 此处使用一个内部类来维护单例 */ private static class SingletonFactory &#123; private static Singleton instance = new Singleton(); &#125; /* 获取实例 */ public static Singleton getInstance() &#123; return SingletonFactory.instance; &#125; /* 如果该对象被用于序列化，可以保证对象在序列化前后保持一致 */ public Object readResolve() &#123; return getInstance(); &#125; &#125; 其实说它完美，也不一定，如果在构造函数中抛出异常，实例将永远得不到创建，也会出错。所以说，十分完美的东西是没有的，我们只能根据实际情况，选择最适合自己应用场景的实现方法。也有人这样实现：因为我们只需要在创建类的时候进行同步，所以只要将创建和getInstance()分开，单独为创建加synchronized关键字，也是可以的： 1234567891011121314151617181920public class SingletonTest &#123; private static SingletonTest instance = null; private SingletonTest() &#123; &#125; private static synchronized void syncInit() &#123; if (instance == null) &#123; instance = new SingletonTest(); &#125; &#125; public static SingletonTest getInstance() &#123; if (instance == null) &#123; syncInit(); &#125; return instance; &#125; &#125; 考虑性能的话，整个程序只需创建一次实例，所以性能也不会有什么影响。 补充：采用”影子实例”的办法为单例对象的属性同步更新 123456789101112131415161718192021222324252627282930public class SingletonTest &#123; private static SingletonTest instance = null; private Vector properties = null; public Vector getProperties() &#123; return properties; &#125; private SingletonTest() &#123; &#125; private static synchronized void syncInit() &#123; if (instance == null) &#123; instance = new SingletonTest(); &#125; &#125; public static SingletonTest getInstance() &#123; if (instance == null) &#123; syncInit(); &#125; return instance; &#125; public void updateProperties() &#123; SingletonTest shadow = new SingletonTest(); properties = shadow.getProperties(); &#125; &#125; 通过单例模式的学习告诉我们： 1、单例模式理解起来简单，但是具体实现起来还是有一定的难度。 2、synchronized关键字锁定的是对象，在用的时候，一定要在恰当的地方使用（注意需要使用锁的对象和过程，可能有的时候并不是整个对象及整个过程都需要锁）。 到这儿，单例模式基本已经讲完了，结尾处，笔者突然想到另一个问题，就是采用类的静态方法，实现单例模式的效果，也是可行的，此处二者有什么不同？ 首先，静态类不能实现接口。（从类的角度说是可以的，但是那样就破坏了静态了。因为接口中不允许有static修饰的方法，所以即使实现了也是非静态的） 其次，单例可以被延迟初始化，静态类一般在第一次加载是初始化。之所以延迟加载，是因为有些类比较庞大，所以延迟加载有助于提升性能。 再次，单例类可以被继承，他的方法可以被覆写。但是静态类内部方法都是static，无法被覆写。 最后一点，单例类比较灵活，毕竟从实现上只是一个普通的Java类，只要满足单例的基本需求，你可以在里面随心所欲的实现一些其它功能，但是静态类不行。从上面这些概括中，基本可以看出二者的区别，但是，从另一方面讲，我们上面最后实现的那个单例模式，内部就是用一个静态类来实现的，所以，二者有很大的关联，只是我们考虑问题的层面不同罢了。两种思想的结合，才能造就出完美的解决方案，就像HashMap采用数组+链表来实现一样，其实生活中很多事情都是这样，单用不同的方法来处理问题，总是有优点也有缺点，最完美的方法是，结合各个方法的优点，才能最好的解决问题！ 4、建造者模式（Builder）5、原型模式（Prototype）原型模式虽然是创建型的模式，但是与工程模式没有关系，从名字即可看出，该模式的思想就是将一个对象作为原型，对其进行复制、克隆，产生一个和原对象类似的新对象。本小结会通过对象的复制，进行讲解。在Java中，复制对象是通过clone()实现的，先创建一个原型类： 1234567public class Prototype implements Cloneable &#123; public Object clone() throws CloneNotSupportedException &#123; Prototype proto = (Prototype) super.clone(); return proto; &#125; &#125; 很简单，一个原型类，只需要实现Cloneable接口，覆写clone方法，此处clone方法可以改成任意的名称，因为Cloneable接口是个空接口，你可以任意定义实现类的方法名，如cloneA或者cloneB，因为此处的重点是super.clone()这句话，super.clone()调用的是Object的clone()方法，而在Object类中，clone()是native的，具体怎么实现，我会在另一篇文章中，关于解读Java中本地方法的调用，此处不再深究。在这儿，我将结合对象的浅复制和深复制来说一下，首先需要了解对象深、浅复制的概念： 浅复制：将一个对象复制后，基本数据类型的变量都会重新创建，而引用类型，指向的还是原对象所指向的。 深复制：将一个对象复制后，不论是基本数据类型还有引用类型，都是重新创建的。简单来说，就是深复制进行了完全彻底的复制，而浅复制不彻底。 此处，写一个深浅复制的例子： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class Prototype implements Cloneable, Serializable &#123; private static final long serialVersionUID = 1L; private String string; private SerializableObject obj; /* 浅复制 */ public Object clone() throws CloneNotSupportedException &#123; Prototype proto = (Prototype) super.clone(); return proto; &#125; /* 深复制 */ public Object deepClone() throws IOException, ClassNotFoundException &#123; /* 写入当前对象的二进制流 */ ByteArrayOutputStream bos = new ByteArrayOutputStream(); ObjectOutputStream oos = new ObjectOutputStream(bos); oos.writeObject(this); /* 读出二进制流产生的新对象 */ ByteArrayInputStream bis = new ByteArrayInputStream(bos.toByteArray()); ObjectInputStream ois = new ObjectInputStream(bis); return ois.readObject(); &#125; public String getString() &#123; return string; &#125; public void setString(String string) &#123; this.string = string; &#125; public SerializableObject getObj() &#123; return obj; &#125; public void setObj(SerializableObject obj) &#123; this.obj = obj; &#125; &#125; class SerializableObject implements Serializable &#123; private static final long serialVersionUID = 1L; &#125; 要实现深复制，需要采用流的形式读入当前对象的二进制输入，再写出二进制数据对应的对象。 B、结构模式（7种）我们接着讨论设计模式，上篇文章我讲完了5种创建型模式，这章开始，我将讲下7种结构型模式：适配器模式、装饰模式、代理模式、外观模式、桥接模式、组合模式、享元模式 6、适配器模式 适配器模式将某个类的接口转换成客户端期望的另一个接口表示，目的是消除由于接口不匹配所造成的类的兼容性问题。主要分为三类：类的适配器模式、对象的适配器模式、接口的适配器模式。 01、类的适配器模式核心思想就是：有一个Source类，拥有一个方法，待适配，目标接口是Targetable，通过Adapter类，将Source的功能扩展到Targetable里，看代码： 123456public class Source &#123; public void method1() &#123; System.out.println("this is original method!"); &#125; &#125; 12345678public interface Targetable &#123; /* 与原类中的方法相同 */ public void method1(); /* 新类的方法 */ public void method2(); &#125; 1234567public class Adapter extends Source implements Targetable &#123; @Override public void method2() &#123; System.out.println("this is the targetable method!"); &#125; &#125; Adapter类继承Source类，实现Targetable接口，下面是测试类： 12345678public class AdapterTest &#123; public static void main(String[] args) &#123; Targetable target = new Adapter(); target.method1(); target.method2(); &#125; &#125; 输出： 12this is original method!this is the targetable method! 这样Targetable接口的实现类就具有了Source类的功能。 02、对象的适配器模式基本思路和类的适配器模式相同，只是将Adapter类作修改，这次不继承Source类，而是持有Source类的实例，以达到解决兼容性的问题 只需要修改Adapter类的源码即可： 123456789101112131415161718public class Wrapper implements Targetable &#123; private Source source; public Wrapper(Source source)&#123; super(); this.source = source; &#125; @Override public void method2() &#123; System.out.println("this is the targetable method!"); &#125; @Override public void method1() &#123; source.method1(); &#125; &#125; 12345678910public class AdapterTest &#123; public static void main(String[] args) &#123; Source source = new Source(); Targetable target = new Wrapper(source); target.method1(); target.method2(); &#125; &#125; 输出与第一种一样，只是适配的方法不同而已。 03、接口的适配器模式第三种适配器模式是接口的适配器模式，接口的适配器是这样的：有时我们写的一个接口中有多个抽象方法，当我们写该接口的实现类时，必须实现该接口的所有方法，这明显有时比较浪费，因为并不是所有的方法都是我们需要的，有时只需要某一些，此处为了解决这个问题，我们引入了接口的适配器模式，借助于一个抽象类，该抽象类实现了该接口，实现了所有的方法，而我们不和原始的接口打交道，只和该抽象类取得联系，所以我们写一个类，继承该抽象类，重写我们需要的方法就行 这个很好理解，在实际开发中，我们也常会遇到这种接口中定义了太多的方法，以致于有时我们在一些实现类中并不是都需要。看代码： 12345public interface Sourceable &#123; public void method1(); public void method2(); &#125; 抽象类Wrapper2： 12345public abstract class Wrapper2 implements Sourceable&#123; public void method1()&#123;&#125; public void method2()&#123;&#125; &#125; 12345public class SourceSub1 extends Wrapper2 &#123; public void method1()&#123; System.out.println("the sourceable interface's first Sub1!"); &#125; &#125; 12345public class SourceSub2 extends Wrapper2 &#123; public void method2()&#123; System.out.println("the sourceable interface's second Sub2!"); &#125; &#125; 123456789101112public class WrapperTest &#123; public static void main(String[] args) &#123; Sourceable source1 = new SourceSub1(); Sourceable source2 = new SourceSub2(); source1.method1(); source1.method2(); source2.method1(); source2.method2(); &#125; &#125; 测试输出： 12the sourceable interface's first Sub1!the sourceable interface's second Sub2! 达到了我们的效果！ 讲了这么多，总结一下三种适配器模式的应用场景： 类的适配器模式：当希望将一个类转换成满足另一个新接口的类时，可以使用类的适配器模式，创建一个新类，继承原有的类，实现新的接口即可。 对象的适配器模式：当希望将一个对象转换成满足另一个新接口的对象时，可以创建一个Wrapper类，持有原类的一个实例，在Wrapper类的方法中，调用实例的方法就行。 接口的适配器模式：当不希望实现一个接口中所有的方法时，可以创建一个抽象类Wrapper，实现所有方法，我们写别的类的时候，继承抽象类即可。 7、装饰模式（Decorator）顾名思义，装饰模式就是给一个对象增加一些新的功能，而且是动态的，要求装饰对象和被装饰对象实现同一个接口，装饰对象持有被装饰对象的实例 Source类是被装饰类，Decorator类是一个装饰类，可以为Source类动态的添加一些功能，代码如下： 123public interface Sourceable &#123; public void method(); &#125; 1234567public class Source implements Sourceable &#123; @Override public void method() &#123; System.out.println("the original method!"); &#125; &#125; 123456789101112131415public class Decorator implements Sourceable &#123; private Sourceable source; public Decorator(Sourceable source)&#123; super(); this.source = source; &#125; @Override public void method() &#123; System.out.println("before decorator!"); source.method(); System.out.println("after decorator!"); &#125; &#125; 测试类： 12345678public class DecoratorTest &#123; public static void main(String[] args) &#123; Sourceable source = new Source(); Sourceable obj = new Decorator(source); obj.method(); &#125; &#125; 输出： 123before decorator!the original method!after decorator! 装饰器模式的应用场景： 1、需要扩展一个类的功能。 2、动态的为一个对象增加功能，而且还能动态撤销。（继承不能做到这一点，继承的功能是静态的，不能动态增删。） 缺点：产生过多相似的对象，不易排错！ 8、代理模式（Proxy）其实每个模式名称就表明了该模式的作用，代理模式就是多一个代理类出来，替原对象进行一些操作，比如我们在租房子的时候回去找中介，为什么呢？因为你对该地区房屋的信息掌握的不够全面，希望找一个更熟悉的人去帮你做，此处的代理就是这个意思。再如我们有的时候打官司，我们需要请律师，因为律师在法律方面有专长，可以替我们进行操作，表达我们的想法 根据上文的阐述，代理模式就比较容易的理解了，我们看下代码： 123public interface Sourceable &#123; public void method(); &#125; 1234567public class Source implements Sourceable &#123; @Override public void method() &#123; System.out.println("the original method!"); &#125; &#125; 1234567891011121314151617181920public class Proxy implements Sourceable &#123; private Source source; public Proxy()&#123; super(); this.source = new Source(); &#125; @Override public void method() &#123; before(); source.method(); atfer(); &#125; private void atfer() &#123; System.out.println("after proxy!"); &#125; private void before() &#123; System.out.println("before proxy!"); &#125; &#125; 测试类： 12345678public class ProxyTest &#123; public static void main(String[] args) &#123; Sourceable source = new Proxy(); source.method(); &#125; &#125; 输出： 123before proxy!the original method!after proxy! 代理模式的应用场景： 如果已有的方法在使用的时候需要对原有的方法进行改进，此时有两种办法： 1、修改原有的方法来适应。这样违反了“对扩展开放，对修改关闭”的原则。 2、就是采用一个代理类调用原有的方法，且对产生的结果进行控制。这种方法就是代理模式。 使用代理模式，可以将功能划分的更加清晰，有助于后期维护！ 9、外观模式（Facade）外观模式是为了解决类与类之家的依赖关系的，像spring一样，可以将类和类之间的关系配置到配置文件中，而外观模式就是将他们的关系放在一个Facade类中，降低了类类之间的耦合度，该模式中没有涉及到接口 我们先看下实现类： 12345678910public class CPU &#123; public void startup()&#123; System.out.println("cpu startup!"); &#125; public void shutdown()&#123; System.out.println("cpu shutdown!"); &#125; &#125; 12345678910public class Memory &#123; public void startup()&#123; System.out.println("memory startup!"); &#125; public void shutdown()&#123; System.out.println("memory shutdown!"); &#125; &#125; 12345678910public class Disk &#123; public void startup()&#123; System.out.println("disk startup!"); &#125; public void shutdown()&#123; System.out.println("disk shutdown!"); &#125; &#125; 123456789101112131415161718192021222324252627public class Computer &#123; private CPU cpu; private Memory memory; private Disk disk; public Computer()&#123; cpu = new CPU(); memory = new Memory(); disk = new Disk(); &#125; public void startup()&#123; System.out.println("start the computer!"); cpu.startup(); memory.startup(); disk.startup(); System.out.println("start computer finished!"); &#125; public void shutdown()&#123; System.out.println("begin to close the computer!"); cpu.shutdown(); memory.shutdown(); disk.shutdown(); System.out.println("computer closed!"); &#125; &#125; User类如下： 12345678public class User &#123; public static void main(String[] args) &#123; Computer computer = new Computer(); computer.startup(); computer.shutdown(); &#125; &#125; 输出： 12345678910start the computer!cpu startup!memory startup!disk startup!start computer finished!begin to close the computer!cpu shutdown!memory shutdown!disk shutdown!computer closed! 如果我们没有Computer类，那么，CPU、Memory、Disk他们之间将会相互持有实例，产生关系，这样会造成严重的依赖，修改一个类，可能会带来其他类的修改，这不是我们想要看到的，有了Computer类，他们之间的关系被放在了Computer类里，这样就起到了解耦的作用，这，就是外观模式！ 10、桥接模式（Bridge）桥接模式就是把事物和其具体实现分开，使他们可以各自独立的变化。桥接的用意是：将抽象化与实现化解耦，使得二者可以独立变化，像我们常用的JDBC桥DriverManager一样，JDBC进行连接数据库的时候，在各个数据库之间进行切换，基本不需要动太多的代码，甚至丝毫不用动，原因就是JDBC提供统一接口，每个数据库提供各自的实现，用一个叫做数据库驱动的程序来桥接就行了 实现代码： 先定义接口： 123public interface Sourceable &#123; public void method(); &#125; 分别定义两个实现类： 1234567public class SourceSub1 implements Sourceable &#123; @Override public void method() &#123; System.out.println("this is the first sub!"); &#125; &#125; 1234567public class SourceSub2 implements Sourceable &#123; @Override public void method() &#123; System.out.println("this is the second sub!"); &#125; &#125; 定义一个桥，持有Sourceable的一个实例： 123456789101112131415public abstract class Bridge &#123; private Sourceable source; public void method()&#123; source.method(); &#125; public Sourceable getSource() &#123; return source; &#125; public void setSource(Sourceable source) &#123; this.source = source; &#125; &#125; 12345public class MyBridge extends Bridge &#123; public void method()&#123; getSource().method(); &#125; &#125; 测试类： 1234567891011121314151617public class BridgeTest &#123; public static void main(String[] args) &#123; Bridge bridge = new MyBridge(); /*调用第一个对象*/ Sourceable source1 = new SourceSub1(); bridge.setSource(source1); bridge.method(); /*调用第二个对象*/ Sourceable source2 = new SourceSub2(); bridge.setSource(source2); bridge.method(); &#125; &#125; 输出： 12this is the first sub!this is the second sub! 这样，就通过对Bridge类的调用，实现了对接口Sourceable的实现类SourceSub1和SourceSub2的调用。接下来我再画个图，大家就应该明白了，因为这个图是我们JDBC连接的原理，有数据库学习基础的，一结合就都懂了。 11、组合模式（Composite）组合模式有时又叫部分-整体模式在处理类似树形结构的问题时比较方便. 直接来看代码： 1234567891011121314151617181920212223242526272829303132333435363738394041public class TreeNode &#123; private String name; private TreeNode parent; private Vector&lt;TreeNode&gt; children = new Vector&lt;TreeNode&gt;(); public TreeNode(String name)&#123; this.name = name; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public TreeNode getParent() &#123; return parent; &#125; public void setParent(TreeNode parent) &#123; this.parent = parent; &#125; //添加孩子节点 public void add(TreeNode node)&#123; children.add(node); &#125; //删除孩子节点 public void remove(TreeNode node)&#123; children.remove(node); &#125; //取得孩子节点 public Enumeration&lt;TreeNode&gt; getChildren()&#123; return children.elements(); &#125; &#125; 123456789101112131415161718public class Tree &#123; TreeNode root = null; public Tree(String name) &#123; root = new TreeNode(name); &#125; public static void main(String[] args) &#123; Tree tree = new Tree("A"); TreeNode nodeB = new TreeNode("B"); TreeNode nodeC = new TreeNode("C"); nodeB.add(nodeC); tree.root.add(nodeB); System.out.println("build the tree finished!"); &#125; &#125; 使用场景：将多个对象组合在一起进行操作，常用于表示树形结构中，例如二叉树，数等。 12、享元模式（Flyweight） 享元模式的主要目的是实现对象的共享，即共享池，当系统中对象多的时候可以减少内存的开销，通常与工厂模式一起使用。 FlyWeightFactory负责创建和管理享元单元，当一个客户端请求时，工厂需要检查当前对象池中是否有符合条件的对象，如果有，就返回已经存在的对象，如果没有，则创建一个新对象，FlyWeight是超类。一提到共享池，我们很容易联想到Java里面的JDBC连接池，想想每个连接的特点，我们不难总结出：适用于作共享的一些个对象，他们有一些共有的属性，就拿数据库连接池来说，url、driverClassName、username、password及dbname，这些属性对于每个连接来说都是一样的，所以就适合用享元模式来处理，建一个工厂类，将上述类似属性作为内部数据，其它的作为外部数据，在方法调用时，当做参数传进来，这样就节省了空间，减少了实例的数量。 看下数据库连接池的代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class ConnectionPool &#123; private Vector&lt;Connection&gt; pool; /*公有属性*/ private String url = "jdbc:mysql://localhost:3306/test"; private String username = "root"; private String password = "root"; private String driverClassName = "com.mysql.jdbc.Driver"; private int poolSize = 100; private static ConnectionPool instance = null; Connection conn = null; /*构造方法，做一些初始化工作*/ private ConnectionPool() &#123; pool = new Vector&lt;Connection&gt;(poolSize); for (int i = 0; i &lt; poolSize; i++) &#123; try &#123; Class.forName(driverClassName); conn = DriverManager.getConnection(url, username, password); pool.add(conn); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; &#125; /* 返回连接到连接池 */ public synchronized void release() &#123; pool.add(conn); &#125; /* 返回连接池中的一个数据库连接 */ public synchronized Connection getConnection() &#123; if (pool.size() &gt; 0) &#123; Connection conn = pool.get(0); pool.remove(conn); return conn; &#125; else &#123; return null; &#125; &#125; &#125; 通过连接池的管理，实现了数据库连接的共享，不需要每一次都重新创建连接，节省了数据库重新创建的开销，提升了系统的性能！ C、关系模式（11种）先来张图，看看这11中模式的关系： 第一类：通过父类与子类的关系进行实现。 第二类：两个类之间。 第三类：类的状态。 第四类：通过中间类 父类与子类关系13、策略模式（strategy）策略模式定义了一系列算法，并将每个算法封装起来，使他们可以相互替换，且算法的变化不会影响到使用算法的客户。需要设计一个接口，为一系列实现类提供统一的方法，多个实现类实现该接口，设计一个抽象类（可有可无，属于辅助类），提供辅助函数. 图中ICalculator提供同意的方法，AbstractCalculator是辅助类，提供辅助方法，接下来，依次实现下每个类： 首先统一接口： 123public interface ICalculator &#123; public int calculate(String exp); &#125; 辅助类： 12345678910public abstract class AbstractCalculator &#123; public int[] split(String exp,String opt)&#123; String array[] = exp.split(opt); int arrayInt[] = new int[2]; arrayInt[0] = Integer.parseInt(array[0]); arrayInt[1] = Integer.parseInt(array[1]); return arrayInt; &#125; &#125; 三个实现类： 12345678public class Plus extends AbstractCalculator implements ICalculator &#123; @Override public int calculate(String exp) &#123; int arrayInt[] = split(exp,"\\+"); return arrayInt[0]+arrayInt[1]; &#125; &#125; 123456789public class Minus extends AbstractCalculator implements ICalculator &#123; @Override public int calculate(String exp) &#123; int arrayInt[] = split(exp,"-"); return arrayInt[0]-arrayInt[1]; &#125; &#125; 12345678public class Multiply extends AbstractCalculator implements ICalculator &#123; @Override public int calculate(String exp) &#123; int arrayInt[] = split(exp,"\\*"); return arrayInt[0]*arrayInt[1]; &#125; &#125; 简单的测试类： 123456789public class StrategyTest &#123; public static void main(String[] args) &#123; String exp = "2+8"; ICalculator cal = new Plus(); int result = cal.calculate(exp); System.out.println(result); &#125; &#125; 输出： 110 策略模式的决定权在用户，系统本身提供不同算法的实现，新增或者删除算法，对各种算法做封装。因此，策略模式多用在算法决策系统中，外部用户只需要决定用哪个算法即可。 14、模板方法模式（Template Method）解释一下模板方法模式，就是指：一个抽象类中，有一个主方法，再定义1…n个方法，可以是抽象的，也可以是实际的方法，定义一个类，继承该抽象类，重写抽象方法，通过调用抽象类，实现对子类的调用. 就是在AbstractCalculator类中定义一个主方法calculate，calculate()调用spilt()等，Plus和Minus分别继承AbstractCalculator类，通过对AbstractCalculator的调用实现对子类的调用，看下面的例子： 12345678910111213141516171819public abstract class AbstractCalculator &#123; /*主方法，实现对本类其它方法的调用*/ public final int calculate(String exp,String opt)&#123; int array[] = split(exp,opt); return calculate(array[0],array[1]); &#125; /*被子类重写的方法*/ abstract public int calculate(int num1,int num2); public int[] split(String exp,String opt)&#123; String array[] = exp.split(opt); int arrayInt[] = new int[2]; arrayInt[0] = Integer.parseInt(array[0]); arrayInt[1] = Integer.parseInt(array[1]); return arrayInt; &#125; &#125; 1234567public class Plus extends AbstractCalculator &#123; @Override public int calculate(int num1,int num2) &#123; return num1 + num2; &#125; &#125; 测试类： 123456789public class StrategyTest &#123; public static void main(String[] args) &#123; String exp = "8+8"; AbstractCalculator cal = new Plus(); int result = cal.calculate(exp, "\\+"); System.out.println(result); &#125; &#125; 我跟踪下这个小程序的执行过程：首先将exp和”\+”做参数，调用AbstractCalculator类里的calculate(String,String)方法，在calculate(String,String)里调用同类的split()，之后再调用calculate(int ,int)方法，从这个方法进入到子类中，执行完return num1 + num2后，将值返回到AbstractCalculator类，赋给result，打印出来。正好验证了我们开头的思路。 类之间的关系15、观察者模式（Observer）包括这个模式在内的接下来的四个模式，都是类和类之间的关系，不涉及到继承，学的时候应该 记得归纳，记得本文最开始的那个图。观察者模式很好理解，类似于邮件订阅和RSS订阅，当我们浏览一些博客或wiki时，经常会看到RSS图标，就这的意思是，当你订阅了该文章，如果后续有更新，会及时通知你。其实，简单来讲就一句话：当一个对象变化时，其它依赖该对象的对象都会收到通知，并且随着变化！对象之间是一种一对多的关系。 我解释下这些类的作用：MySubject类就是我们的主对象，Observer1和Observer2是依赖于MySubject的对象，当MySubject变化时，Observer1和Observer2必然变化。AbstractSubject类中定义着需要监控的对象列表，可以对其进行修改：增加或删除被监控对象，且当MySubject变化时，负责通知在列表内存在的对象。我们看实现代码： 一个Observer接口： 123public interface Observer &#123; public void update(); &#125; 两个实现类： 1234567public class Observer1 implements Observer &#123; @Override public void update() &#123; System.out.println("observer1 has received!"); &#125; &#125; 12345678public class Observer2 implements Observer &#123; @Override public void update() &#123; System.out.println("observer2 has received!"); &#125; &#125; Subject接口及实现类： 1234567891011121314public interface Subject &#123; /*增加观察者*/ public void add(Observer observer); /*删除观察者*/ public void del(Observer observer); /*通知所有的观察者*/ public void notifyObservers(); /*自身的操作*/ public void operation(); &#125; 123456789101112131415161718192021public abstract class AbstractSubject implements Subject &#123; private Vector&lt;Observer&gt; vector = new Vector&lt;Observer&gt;(); @Override public void add(Observer observer) &#123; vector.add(observer); &#125; @Override public void del(Observer observer) &#123; vector.remove(observer); &#125; @Override public void notifyObservers() &#123; Enumeration&lt;Observer&gt; enumo = vector.elements(); while(enumo.hasMoreElements())&#123; enumo.nextElement().update(); &#125; &#125; &#125; 123456789public class MySubject extends AbstractSubject &#123; @Override public void operation() &#123; System.out.println("update self!"); notifyObservers(); &#125; &#125; 测试类： 1234567891011public class ObserverTest &#123; public static void main(String[] args) &#123; Subject sub = new MySubject(); sub.add(new Observer1()); sub.add(new Observer2()); sub.operation(); &#125; &#125; 输出： 123update self!observer1 has received!observer2 has received! 这些东西，其实不难，只是有些抽象，不太容易整体理解，建议读者：根据关系图，新建项目，自己写代码（或者参考我的代码）,按照总体思路走一遍，这样才能体会它的思想，理解起来容易！ 16、迭代子模式（Iterator）顾名思义，迭代器模式就是顺序访问聚集中的对象，一般来说，集合中非常常见，如果对集合类比较熟悉的话，理解本模式会十分轻松。这句话包含两层意思：一是需要遍历的对象，即聚集对象，二是迭代器对象，用于对聚集对象进行遍历访问。 这个思路和我们常用的一模一样，MyCollection中定义了集合的一些操作，MyIterator中定义了一系列迭代操作，且持有Collection实例，我们来看看实现代码： 两个接口： 12345678910public interface Collection &#123; public Iterator iterator(); /*取得集合元素*/ public Object get(int i); /*取得集合大小*/ public int size(); &#125; 1234567891011public interface Iterator &#123; //前移 public Object previous(); //后移 public Object next(); public boolean hasNext(); //取得第一个元素 public Object first(); &#125; 两个实现： 123456789101112131415161718public class MyCollection implements Collection &#123; public String string[] = &#123;"A","B","C","D","E"&#125;; @Override public Iterator iterator() &#123; return new MyIterator(this); &#125; @Override public Object get(int i) &#123; return string[i]; &#125; @Override public int size() &#123; return string.length; &#125; &#125; 1234567891011121314151617181920212223242526272829303132333435363738394041public class MyIterator implements Iterator &#123; private Collection collection; private int pos = -1; public MyIterator(Collection collection)&#123; this.collection = collection; &#125; @Override public Object previous() &#123; if(pos &gt; 0)&#123; pos--; &#125; return collection.get(pos); &#125; @Override public Object next() &#123; if(pos&lt;collection.size()-1)&#123; pos++; &#125; return collection.get(pos); &#125; @Override public boolean hasNext() &#123; if(pos&lt;collection.size()-1)&#123; return true; &#125;else&#123; return false; &#125; &#125; @Override public Object first() &#123; pos = 0; return collection.get(pos); &#125; &#125; 测试类： 1234567891011public class Test &#123; public static void main(String[] args) &#123; Collection collection = new MyCollection(); Iterator it = collection.iterator(); while(it.hasNext())&#123; System.out.println(it.next()); &#125; &#125; &#125; 输出： 1A B C D E 此处我们貌似模拟了一个集合类的过程，感觉是不是很爽？其实JDK中各个类也都是这些基本的东西，加一些设计模式，再加一些优化放到一起的，只要我们把这些东西学会了，掌握好了，我们也可以写出自己的集合类，甚至框架！ 17、责任链模式（Chain of Responsibility）接下来我们将要谈谈责任链模式，有多个对象，每个对象持有对下一个对象的引用，这样就会形成一条链，请求在这条链上传递，直到某一对象决定处理该请求。但是发出者并不清楚到底最终那个对象会处理该请求，所以，责任链模式可以实现，在隐瞒客户端的情况下，对系统进行动态的调整。 Abstracthandler类提供了get和set方法，方便MyHandle类设置和修改引用对象，MyHandle类是核心，实例化后生成一系列相互持有的对象，构成一条链。 123public interface Handler &#123; public void operator(); &#125; 12345678910111213public abstract class AbstractHandler &#123; private Handler handler; public Handler getHandler() &#123; return handler; &#125; public void setHandler(Handler handler) &#123; this.handler = handler; &#125; &#125; 12345678910111213141516public class MyHandler extends AbstractHandler implements Handler &#123; private String name; public MyHandler(String name) &#123; this.name = name; &#125; @Override public void operator() &#123; System.out.println(name+"deal!"); if(getHandler()!=null)&#123; getHandler().operator(); &#125; &#125; &#125; 测试类： 12345678910111213public class Test &#123; public static void main(String[] args) &#123; MyHandler h1 = new MyHandler("h1"); MyHandler h2 = new MyHandler("h2"); MyHandler h3 = new MyHandler("h3"); h1.setHandler(h2); h2.setHandler(h3); h1.operator(); &#125; &#125; 输出： 123h1deal!h2deal!h3deal! 此处强调一点就是，链接上的请求可以是一条链，可以是一个树，还可以是一个环，模式本身不约束这个，需要我们自己去实现，同时，在一个时刻，命令只允许由一个对象传给另一个对象，而不允许传给多个对象。 18、命令模式（Command）命令模式很好理解，举个例子，司令员下令让士兵去干件事情，从整个事情的角度来考虑，司令员的作用是，发出口令，口令经过传递，传到了士兵耳朵里，士兵去执行。这个过程好在，三者相互解耦，任何一方都不用去依赖其他人，只需要做好自己的事儿就行，司令员要的是结果，不会去关注到底士兵是怎么实现的。 Invoker是调用者（司令员），Receiver是被调用者（士兵），MyCommand是命令，实现了Command接口，持有接收对象，看实现代码： 123public interface Command &#123; public void exe(); &#125; 12345678910111213public class MyCommand implements Command &#123; private Receiver receiver; public MyCommand(Receiver receiver) &#123; this.receiver = receiver; &#125; @Override public void exe() &#123; receiver.action(); &#125; &#125; 12345public class Receiver &#123; public void action()&#123; System.out.println("command received!"); &#125; &#125; 123456789101112public class Invoker &#123; private Command command; public Invoker(Command command) &#123; this.command = command; &#125; public void action()&#123; command.exe(); &#125; &#125; 测试类 123456789public class Test &#123; public static void main(String[] args) &#123; Receiver receiver = new Receiver(); Command cmd = new MyCommand(receiver); Invoker invoker = new Invoker(cmd); invoker.action(); &#125; &#125; 输出： 1command received! 这个很哈理解，命令模式的目的就是达到命令的发出者和执行者之间解耦，实现请求和执行分开，熟悉Struts的同学应该知道，Struts其实就是一种将请求和呈现分离的技术，其中必然涉及命令模式的思想！ 其实每个设计模式都是很重要的一种思想，看上去很熟，其实是因为我们在学到的东西中都有涉及，尽管有时我们并不知道，其实在Java本身的设计之中处处都有体现，像AWT、JDBC、集合类、IO管道或者是Web框架，里面设计模式无处不在。因为我们篇幅有限，很难讲每一个设计模式都讲的很详细，不过我会尽我所能，尽量在有限的空间和篇幅内，把意思写清楚了，更好让大家明白。本章不出意外的话，应该是设计模式最后一讲了. 本章讲讲第三类和第四类。 类的状态19、备忘录模式（Memento）主要目的是保存一个对象的某个状态，以便在适当的时候恢复对象，个人觉得叫备份模式更形象些，通俗的讲下：假设有原始类A，A中有各种属性，A可以决定需要备份的属性，备忘录类B是用来存储A的一些内部状态，类C呢，就是一个用来存储备忘录的，且只能存储，不能修改等操 Original类是原始类，里面有需要保存的属性value及创建一个备忘录类，用来保存value值。Memento类是备忘录类，Storage类是存储备忘录的类，持有Memento类的实例，该模式很好理解。直接看源码： 123456789101112131415161718192021222324public class Original &#123; private String value; public String getValue() &#123; return value; &#125; public void setValue(String value) &#123; this.value = value; &#125; public Original(String value) &#123; this.value = value; &#125; public Memento createMemento()&#123; return new Memento(value); &#125; public void restoreMemento(Memento memento)&#123; this.value = memento.getValue(); &#125; &#125; 12345678910111213141516public class Memento &#123; private String value; public Memento(String value) &#123; this.value = value; &#125; public String getValue() &#123; return value; &#125; public void setValue(String value) &#123; this.value = value; &#125; &#125; 12345678910111213141516public class Storage &#123; private Memento memento; public Storage(Memento memento) &#123; this.memento = memento; &#125; public Memento getMemento() &#123; return memento; &#125; public void setMemento(Memento memento) &#123; this.memento = memento; &#125; &#125; 测试类： 1234567891011121314151617181920public class Test &#123; public static void main(String[] args) &#123; // 创建原始类 Original origi = new Original("egg"); // 创建备忘录 Storage storage = new Storage(origi.createMemento()); // 修改原始类的状态 System.out.println("初始化状态为：" + origi.getValue()); origi.setValue("niu"); System.out.println("修改后的状态为：" + origi.getValue()); // 回复原始类的状态 origi.restoreMemento(storage.getMemento()); System.out.println("恢复后的状态为：" + origi.getValue()); &#125; &#125; 输出： 123初始化状态为：egg修改后的状态为：niu恢复后的状态为：egg 简单描述下：新建原始类时，value被初始化为egg，后经过修改，将value的值置为niu，最后倒数第二行进行恢复状态，结果成功恢复了。其实我觉得这个模式叫“备份-恢复”模式最形象。 20、状态模式（State）核心思想就是：当对象的状态改变时，同时改变其行为，很好理解！就拿QQ来说，有几种状态，在线、隐身、忙碌等，每个状态对应不同的操作，而且你的好友也能看到你的状态，所以，状态模式就两点：1、可以通过改变状态来获得不同的行为。2、你的好友能同时看到你的变化。 State类是个状态类，Context类可以实现切换，我们来看看代码： 12345678910111213141516171819202122232425262728package com.xtfggef.dp.state; /** * 状态类的核心类 * 2012-12-1 * @author erqing * */ public class State &#123; private String value; public String getValue() &#123; return value; &#125; public void setValue(String value) &#123; this.value = value; &#125; public void method1()&#123; System.out.println("execute the first opt!"); &#125; public void method2()&#123; System.out.println("execute the second opt!"); &#125; &#125; 12345678910111213141516171819202122232425262728293031package com.xtfggef.dp.state; /** * 状态模式的切换类 2012-12-1 * @author erqing * */ public class Context &#123; private State state; public Context(State state) &#123; this.state = state; &#125; public State getState() &#123; return state; &#125; public void setState(State state) &#123; this.state = state; &#125; public void method() &#123; if (state.getValue().equals("state1")) &#123; state.method1(); &#125; else if (state.getValue().equals("state2")) &#123; state.method2(); &#125; &#125; &#125; 12345678910111213141516public class Test &#123; public static void main(String[] args) &#123; State state = new State(); Context context = new Context(state); //设置第一种状态 state.setValue("state1"); context.method(); //设置第二种状态 state.setValue("state2"); context.method(); &#125; &#125; 输出： 12the first opt!execute the second opt! 根据这个特性，状态模式在日常开发中用的挺多的，尤其是做网站的时候，我们有时希望根据对象的某一属性，区别开他们的一些功能，比如说简单的权限控制等。 通过中间类21、访问者模式（Visitor）访问者模式把数据结构和作用于结构上的操作解耦合，使得操作集合可相对自由地演化。访问者模式适用于数据结构相对稳定算法又易变化的系统。因为访问者模式使得算法操作增加变得容易。若系统数据结构对象易于变化，经常有新的数据对象增加进来，则不适合使用访问者模式。访问者模式的优点是增加操作很容易，因为增加操作意味着增加新的访问者。访问者模式将有关行为集中到一个访问者对象中，其改变不影响系统数据结构。其缺点就是增加新的数据结构很困难。—— From 百科 简单来说，访问者模式就是一种分离对象数据结构与行为的方法，通过这种分离，可达到为一个被访问者动态添加新的操作而无需做其它的修改的效果。 来看看原码：一个Visitor类，存放要访问的对象， 123public interface Visitor &#123; public void visit(Subject sub); &#125; 1234567public class MyVisitor implements Visitor &#123; @Override public void visit(Subject sub) &#123; System.out.println("visit the subject："+sub.getSubject()); &#125; &#125; Subject类，accept方法，接受将要访问它的对象，getSubject()获取将要被访问的属性， 1234public interface Subject &#123; public void accept(Visitor visitor); public String getSubject(); &#125; 123456789101112public class MySubject implements Subject &#123; @Override public void accept(Visitor visitor) &#123; visitor.visit(this); &#125; @Override public String getSubject() &#123; return "love"; &#125; &#125; 测试： 123456789public class Test &#123; public static void main(String[] args) &#123; Visitor visitor = new MyVisitor(); Subject sub = new MySubject(); sub.accept(visitor); &#125; &#125; 输出： 1visit the subject：love 该模式适用场景：如果我们想为一个现有的类增加新功能，不得不考虑几个事情：1、新功能会不会与现有功能出现兼容性问题？2、以后会不会再需要添加？3、如果类不允许修改代码怎么办？面对这些问题，最好的解决方法就是使用访问者模式，访问者模式适用于数据结构相对稳定的系统，把数据结构和算法解耦， 22、中介者模式（Mediator）中介者模式也是用来降低类类之间的耦合的，因为如果类类之间有依赖关系的话，不利于功能的拓展和维护，因为只要修改一个对象，其它关联的对象都得进行修改。如果使用中介者模式，只需关心和Mediator类的关系，具体类类之间的关系及调度交给Mediator就行，这有点像spring容器的作用。 User类统一接口，User1和User2分别是不同的对象，二者之间有关联，如果不采用中介者模式，则需要二者相互持有引用，这样二者的耦合度很高，为了解耦，引入了Mediator类，提供统一接口，MyMediator为其实现类，里面持有User1和User2的实例，用来实现对User1和User2的控制。这样User1和User2两个对象相互独立，他们只需要保持好和Mediator之间的关系就行，剩下的全由MyMediator类来维护！基本实现： 1234public interface Mediator &#123; public void createMediator(); public void workAll(); &#125; 12345678910111213141516171819202122232425public class MyMediator implements Mediator &#123; private User user1; private User user2; public User getUser1() &#123; return user1; &#125; public User getUser2() &#123; return user2; &#125; @Override public void createMediator() &#123; user1 = new User1(this); user2 = new User2(this); &#125; @Override public void workAll() &#123; user1.work(); user2.work(); &#125; &#125; 1234567891011121314public abstract class User &#123; private Mediator mediator; public Mediator getMediator()&#123; return mediator; &#125; public User(Mediator mediator) &#123; this.mediator = mediator; &#125; public abstract void work(); &#125; 1234567891011public class User1 extends User &#123; public User1(Mediator mediator)&#123; super(mediator); &#125; @Override public void work() &#123; System.out.println("user1 exe!"); &#125; &#125; 1234567891011public class User2 extends User &#123; public User2(Mediator mediator)&#123; super(mediator); &#125; @Override public void work() &#123; System.out.println("user2 exe!"); &#125; &#125; 测试类： 12345678public class Test &#123; public static void main(String[] args) &#123; Mediator mediator = new MyMediator(); mediator.createMediator(); mediator.workAll(); &#125; &#125; 输出： 12user1 exe!user2 exe! 23、解释器模式（Interpreter）解释器模式是我们暂时的最后一讲，一般主要应用在OOP开发中的编译器的开发中，所以适用面比较窄。 Context类是一个上下文环境类，Plus和Minus分别是用来计算的实现，代码如下： 123public interface Expression &#123; public int interpret(Context context); &#125; 1234567public class Plus implements Expression &#123; @Override public int interpret(Context context) &#123; return context.getNum1()+context.getNum2(); &#125; &#125; 1234567public class Minus implements Expression &#123; @Override public int interpret(Context context) &#123; return context.getNum1()-context.getNum2(); &#125; &#125; 12345678910111213141516171819202122232425public class Context &#123; private int num1; private int num2; public Context(int num1, int num2) &#123; this.num1 = num1; this.num2 = num2; &#125; public int getNum1() &#123; return num1; &#125; public void setNum1(int num1) &#123; this.num1 = num1; &#125; public int getNum2() &#123; return num2; &#125; public void setNum2(int num2) &#123; this.num2 = num2; &#125; &#125; 测试类： 12345678910public class Test &#123; public static void main(String[] args) &#123; // 计算9+2-8的值 int result = new Minus().interpret((new Context(new Plus() .interpret(new Context(9, 2)), 8))); System.out.println(result); &#125; &#125; 最后输出正确的结果：3。 基本就这样，解释器模式用来做各种各样的解释器，如正则表达式等的解释器等等.]]></content>
      <categories>
        <category>-java设计模式</category>
      </categories>
      <tags>
        <tag>-java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[keepalived]]></title>
    <url>%2F2019%2F02%2F14%2Fkeepalived%2F</url>
    <content type="text"><![CDATA[简介Keepalived是基于vrrp协议的一款高可用软件。Keepailived有一台主服务器和多台备份服务器，在主服务器和备份服务器上面部署相同的服务配置，使用一个虚拟IP地址对外提供服务，当主服务器出现故障时，虚拟IP地址会自动漂移到备份服务器。 VRRP（Virtual Router Redundancy Protocol，虚拟路由器冗余协议），VRRP是为了解决静态路由的高可用。VRRP的基本架构 虚拟路由器由多个路由器组成，每个路由器都有各自的IP和共同的VRID(0-255)，其中一个VRRP路由器通过竞选成为MASTER，占有VIP，对外提供路由服务，其他成为BACKUP，MASTER以IP组播（组播地址：224.0.0.18）形式发送VRRP协议包，与BACKUP保持心跳连接，若MASTER不可用（或BACKUP接收不到VRRP协议包），则BACKUP通过竞选产生新的MASTER并继续对外提供路由服务，从而实现高可用。 vrrp协议的相关术语： 123456789虚拟路由器：Virtual Router 虚拟路由器标识：VRID(0-255)物理路由器： master ：主设备 backup ：备用设备 priority：优先级VIP：Virtual IP VMAC：Virutal MAC (00-00-5e-00-01-VRID)GraciousARP 安全认证： 12简单字符认证、HMAC机制，只对信息做认证MD5（leepalived不支持） 工作模式： 12主/备：单虚拟路径器；主/主：主/备（虚拟路径器），备/主（虚拟路径器） 工作类型: 12抢占式：当出现比现有主服务器优先级高的服务器时，会发送通告抢占角色成为主服务器非抢占式： keepalived核心组件：123456vrrp stack：vrrp协议的实现ipvs wrapper：为集群内的所有节点生成IPVS规则checkers：对IPVS集群的各RS做健康状态检测控制组件：配置文件分析器，用来实现配置文件的分析和加载IO复用器内存管理组件，用来管理keepalived高可用是的内存管理 注意： 各节点时间必须同步 确保各节点的用于集群服务的接口支持MULTICAST通信（组播）； 安装从CentOS 6.4开始keepalived随系统base仓库提供，可以使用yun -y install keepalived安装。 配置文件：12345主配置文件：/etc/keepalived/keepalived.conf主程序文件：/usr/sbin/keepalived提供校验码：/usr/bin/genhashUnit File：keepalived.serviceUnit File的环境配置文件：/etc/sysconfig/keepalived 配置主配置文件详解123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596! Configuration File for keepalivedglobal_defs &#123; notification_email &#123; #发送报警邮件收件地址 acassen@firewall.loc failover@firewall.loc sysadmin@firewall.loc &#125; notification_email_from Alexandre.Cassen@firewall.loc #指明报警邮件的发送地址 smtp_server 192.168.200.1 #邮件服务器地址 smtp_connect_timeout 30 #smtp的超时时间 router_id LVS_DEVEL #物理服务器的主机名 vrrp_mcast_group4 #定义一个组播地址 static_ipaddress &#123; 192.168.1.1/24 dev eth0 scope global &#125; static_routes &#123; 192.168.2.0/24 via 192.168.1.100 dev eth0 &#125;&#125; vrrp_sync_group VG_1 &#123; #定义一个故障组，组内有一个虚拟路由出现故障另一个也会一起跟着转移，适用于LVS的NAT模型。 group &#123; VI1 # name of vrrp_instance (below) VI2 # One for each moveable IP &#125; &#125;vrrp_instance VI_1 &#123; #定义一个虚拟路由 state MASTER|BACKUP #当前节点在此虚拟路由器上的初始状态；只能有一个是MASTER，余下的都应该为BACKUP； interface eth0 #绑定为当前虚拟路由器使用的物理接口； virtual_router_id 51 #当前虚拟路由器的惟一标识，范围是0-255； priority 100 #当前主机在此虚拟路径器中的优先级；范围1-254； advert_int 1 #通告发送间隔，包含主机优先级、心跳等。 authentication &#123; #认证配置 auth_type PASS #认证类型，PASS表示简单字符串认证 auth_pass 1111 #密码,PASS密码最长为8位 virtual_ipaddress &#123; 192.168.200.16 #虚拟路由IP地址，以辅助地址方式设置 192.168.200.18/24 dev eth2 label eth2:1 #以别名的方式设置 &#125;track_interface &#123; eth0 eth1&#125; #配置要监控的网络接口，一旦接口出现故障，则转为FAULT状态；nopreempt #定义工作模式为非抢占模式；preempt_delay 300 #抢占式模式下，节点上线后触发新选举操作的延迟时长； virtual_routes &#123; #配置路由信息，可选项 # src &lt;IPADDR&gt; [to] &lt;IPADDR&gt;/&lt;MASK&gt; via|gw &lt;IPADDR&gt; [or &lt;IPADDR&gt;] dev &lt;STRING&gt; scope &lt;SCOPE&gt; tab src 192.168.100.1 to 192.168.109.0/24 via 192.168.200.254 dev eth1 192.168.112.0/24 via 192.168.100.254 192.168.113.0/24 via 192.168.200.254 or 192.168.100.254 dev eth1 blackhole 192.168.114.0/24 &#125; notify_master &lt;STRING&gt;|&lt;QUOTED-STRING&gt; #当前节点成为主节点时触发的脚本。 notify_backup &lt;STRING&gt;|&lt;QUOTED-STRING&gt; #当前节点转为备节点时触发的脚本。 notify_fault &lt;STRING&gt;|&lt;QUOTED-STRING&gt; #当前节点转为“失败”状态时触发的脚本。 notify &lt;STRING&gt;|&lt;QUOTED-STRING&gt; #通用格式的通知触发机制，一个脚本可完成以上三种状态的转换时的通知。 smtp_alert #如果加入这个选项，将调用前面设置的邮件设置，并自动根据状态发送信息 &#125;virtual_server 192.168.200.100 443 &#123; #LVS配置段 ，设置LVS的VIP地址和端口 delay_loop #服务轮询的时间间隔；检测RS服务器的状态。 lb_algo rr #调度算法，可选rr|wrr|lc|wlc|lblc|sh|dh。 lb_kind NAT #集群类型。 nat_mask 255.255.255.0 #子网掩码，可选项。 persistence_timeout 50 #是否启用持久连接，连接保存时长 protocol TCP #协议，只支持TCP sorry_server &lt;IPADDR&gt; &lt;PORT&gt; #备用服务器地址，可选项。 real_server 192.168.201.100 443 &#123; #配置RS服务器的地址和端口 weight 1 #权重 SSL_GET &#123; #检测RS服务器的状态，发送请求报文 url &#123; path / #请求的URL digest ff20ad2481f97b1754ef3e12ecd3a9cc #对请求的页面进行hash运算，然后和这个hash码进行比对，如果hash码一样就表示状态正常 status_code &lt;INT&gt; #判断上述检测机制为健康状态的响应码,和digest二选一即可。 &#125; #这个hash码可以使用genhash命令请求这个页面生成 connect_timeout 3 #连接超时时间 nb_get_retry 3 #超时重试次数 delay_before_retry 3 #每次超时过后多久再进行连接 connect_ip &lt;IP ADDRESS&gt; #向当前RS的哪个IP地址发起健康状态检测请求 connect_port &lt;PORT&gt; #向当前RS的哪个PORT发起健康状态检测请求 bindto &lt;IP ADDRESS&gt; #发出健康状态检测请求时使用的源地址； bind_port &lt;PORT&gt; #发出健康状态检测请求时使用的源端口； &#125; &#125;&#125; 健康状态检测机制 HTTP_GET SSL_GET TCP_CHECK SMTP_CHECK MISS_CHECK #调用自定义脚本进行检测 1234567TCP_CHECK &#123; connect_ip &lt;IP ADDRESS&gt; #向当前RS的哪个IP地址发起健康状态检测请求; connect_port &lt;PORT&gt; #向当前RS的哪个PORT发起健康状态检测请求; bindto &lt;IP ADDRESS&gt; #发出健康状态检测请求时使用的源地址； bind_port &lt;PORT&gt; #发出健康状态检测请求时使用的源端口； connect_timeout &lt;INTEGER&gt; #连接请求的超时时长；&#125; 实现LVS高可用集群实验主机 虚拟IP:192.168.166.100 2台CentOS 7.3 12CentOS 7.3 主服务器， IP：192.168.166.130CentOS 7.3-1 备份服务器，IP：192.168.166.132 2台CentOS 6.9 12CentOS 6.9 IP：192.168.166.129CentOS6.9-1 IP：192.168.166.131 注：在配置服务前需要注意几台主机的防火墙策略，和SELinux配置。主调度器配置 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677[root@CentOS7.3 ~]#yum -y install keepalived ipvsadm #安装keepalived和LVS管理软件ipvsadm[root@CentOS7.3 ~]#vim /etc/keepalived/keepalived.conf #配置keepalived! Configuration File for keepalivedglobal_defs &#123; notification_email &#123; root@localhost &#125; notification_email_from keepalived@localhost smtp_server 127.0.0.1 #邮件服务器的地址 smtp_connect_timeout 30 router_id CentOS7.3 #主调度器的主机名 vrrp_mcast_group4 224.26.1.1 #发送心跳信息的组播地址 &#125;vrrp_instance VI_1 &#123; state MASTER #主调度器的初始角色 interface eth0 #虚拟IP工作的网卡接口 virtual_router_id 66 #虚拟路由的ID priority 100 #主调度器的选举优先级 advert_int 1 authentication &#123; auth_type PASS #集群主机的认证方式 auth_pass 123456 #密钥,最长8位 &#125; virtual_ipaddress &#123; 192.168.166.100 #虚拟IP &#125;&#125;virtual_server 192.168.166.100 80 &#123; #LVS配置段，VIP delay_loop 6 lb_algo rr #调度算法轮询 lb_kind DR #工作模式DR nat_mask 255.255.255.0# persistence_timeout 50 #持久连接，在测试时需要注释，否则会在设置的时间内把请求都调度到一台RS服务器上面 protocol TCP sorry_server 127.0.0.1 80 #Sorry server的服务器地址及端口#Sorry server就是在后端的服务器全部宕机的情况下紧急提供服务。 real_server 192.168.166.129 80 &#123; #RS服务器地址和端口 weight 1 #RS的权重 HTTP_GET &#123; #健康状态检测方法 url &#123; path / status_code 200 #状态判定规则 &#125; connect_timeout 1 nb_get_retry 3 delay_before_retry 1 &#125; &#125; real_server 192.168.166.131 80 &#123; weight 1 HTTP_GET &#123; url &#123; path / status_code 200 &#125; connect_timeout 1 nb_get_retry 3 delay_before_retry 1 &#125; &#125;&#125;[root@CentOS7.3 keepalived]#systemctl start keepalived #启动keepalived[root@CentOS7.3 keepalived]#ip a l eth0 #查看虚拟路由绑定的网卡 2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 00:0c:29:b9:7d:cb brd ff:ff:ff:ff:ff:ff inet 192.168.166.130/24 brd 192.168.166.255 scope global eth0 valid_lft forever preferred_lft forever inet 192.168.166.100/32 scope global eth0 #虚拟IP已经绑定在了eth网卡上 valid_lft forever preferred_lft forever inet6 fe80::50fe:a3f3:83a0:d38a/64 scope link valid_lft forever preferred_lft forever 备份调度器的配置 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475[root@centos7.3-1 ~]#yum -y install keepalived ipvsadm ! Configuration File for keepalivedglobal_defs &#123; notification_email &#123; root@localhost &#125; notification_email_from keepalived@localhost smtp_server 127.0.0.1 smtp_connect_timeout 30 router_id CentOS7.3-1 #备份调度器的主机名 vrrp_mcast_group4 224.26.1.1 #这个组播地址需与集群内的其他主机相同&#125;vrrp_instance VI_1 &#123; state BACKUP #初始角色，备份服务器需设置为BACKUP interface eth0 virtual_router_id 66 #虚拟路由的ID一定要和集群内的其他主机相同 priority 90 #选举优先级，要比主调度器地一些 advert_int 1 authentication &#123; auth_type PASS auth_pass 123456 #密钥需要和集群内的主服务器相同 &#125; virtual_ipaddress &#123; 192.168.166.100 &#125;&#125; #余下配置和主服务器相同virtual_server 192.168.166.100 80 &#123; delay_loop 6 lb_algo rr lb_kind DR nat_mask 255.255.255.0# persistence_timeout 50 protocol TCP sorry_server 127.0.0.1 80 real_server 192.168.166.129 80 &#123; weight 1 HTTP_GET &#123; url &#123; path / status_code 200 &#125; connect_timeout 1 nb_get_retry 3 delay_before_retry 1 &#125; &#125; real_server 192.168.166.131 80 &#123; weight 1 HTTP_GET &#123; url &#123; path / status_code 200 &#125; connect_timeout 1 nb_get_retry 3 delay_before_retry 1 &#125; &#125;&#125;[root@centos7.3-1 ~]#systemctl start keepalived #启动备份keepalived[root@centos7.3-1 ~]#ip a l eth0 #查看虚拟路由绑定的网卡接口[root@centos7.3-1 ~]#ip a l eth02: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 00:0c:29:7e:ec:ef brd ff:ff:ff:ff:ff:ff inet 192.168.166.132/24 brd 192.168.166.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::9aab:52b3:cc1e:fbef/64 scope link valid_lft forever preferred_lft forever 测试虚拟IP地址漂移 关闭主服务器的keepalived，并查看eth0接口 image.png 查看备份服务器的eth0接口，地址已经漂移到了备份服务器上面 image.png 可以看到上图提示有新邮件。使用mail命令查看邮件列表，都是后端服务器状态检测的邮件，说明配置的报警邮件生效了。应为后端服务器还没有配置所以检测的状态全是down。 image.png 启动主服务器，地址又漂移回了主服务器 image.png 配置RS服务器 RS1配置 12345678910111213141516171819202122232425262728293031323334353637383940[root@CentOS6.9 ~]#yum -y install httpd #安装httpd服务[root@CentOS6.9 ~]#vim lvs.sh #创建一个配置脚本#!/bin/bashvip=192.168.166.100 #VIP地址mask=255.255.255.255 dev=eth0:1case $1 instart) echo 1 &gt; /proc/sys/net/ipv4/conf/all/arp_ignore echo 1 &gt; /proc/sys/net/ipv4/conf/lo/arp_ignore echo 2 &gt; /proc/sys/net/ipv4/conf/all/arp_announce echo 2 &gt; /proc/sys/net/ipv4/conf/lo/arp_announce ifconfig $dev $vip netmask $mask broadcast $vip up route add -host $vip dev $dev;;stop) ifconfig $dev down echo 0 &gt; /proc/sys/net/ipv4/conf/all/arp_ignore echo 0 &gt; /proc/sys/net/ipv4/conf/lo/arp_ignore echo 0 &gt; /proc/sys/net/ipv4/conf/all/arp_announce echo 0 &gt; /proc/sys/net/ipv4/conf/lo/arp_announce;;*) echo &quot;Usage: $(basename $0) start|stop&quot; exit 1;;esac[root@CentOS6.9 ~]#bash lvs.sh start[root@CentOS6.9 ~]#ip a l eth0:02: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 00:0c:29:23:38:c9 brd ff:ff:ff:ff:ff:ff inet 192.168.166.129/24 brd 192.168.166.255 scope global eth0 inet 192.168.166.100/32 brd 192.168.166.100 scope global eth0:1 inet6 fe80::20c:29ff:fe23:38c9/64 scope link valid_lft forever preferred_lft forever[root@CentOS6.9 ~]#echo WebServer1 &gt; /var/www/html/index.html[root@CentOS6.9 ~]#cat /var/www/html/index.html WebServer1[root@CentOS6.9 ~]#service httpd start 使用ipvsadm 命令查看lvs配置信息，RS1服务器已经调度器添加进集群。 image.png RS2配置 123456[root@CentOS6.9-1 ~]#yum -y install httpd[root@CentOS6.9-1 ~]#vim lvs.sh #和上面RS1的lvx.sh内容相同[root@CentOS6.9-1 ~]#echo WebServer2 &gt; /var/www/html/index.html[root@CentOS6.9-1 ~]#cat /var/www/html/index.html WebServer2[root@CentOS6.9-1 ~]#service httpd start 第二台RS服务器上线 image.png 客户端测试 因为使用的是轮询算法，所以会在Web1和Web2之间来回调度。 image.png 我们还可以使用这些主机配置来两套LVS高可用，做一个双主模型第二套LVS信息 VIP：192.168.166.200 第一台调度器为备份服务器 第二台调度器为主服务器 第一台调度器配置 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113! Configuration File for keepalivedglobal_defs &#123; notification_email &#123; root@localhost &#125; notification_email_from keepalived@localhost smtp_server 127.0.0.1 smtp_connect_timeout 30 router_id CentOS7.3 vrrp_mcast_group4 224.26.1.1&#125;vrrp_instance VI_1 &#123; state MASTER interface eth0 virtual_router_id 66 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 123456 &#125; virtual_ipaddress &#123; 192.168.166.100 &#125;&#125;virtual_server 192.168.166.100 80 &#123; delay_loop 6 lb_algo rr lb_kind DR nat_mask 255.255.255.0# persistence_timeout 50 protocol TCP sorry_server 127.0.0.1 80 real_server 192.168.166.129 80 &#123; weight 1 HTTP_GET &#123; url &#123; path / status_code 200 &#125; connect_timeout 1 nb_get_retry 3 delay_before_retry 1 &#125; &#125; real_server 192.168.166.131 80 &#123; weight 1 HTTP_GET &#123; url &#123; path / status_code 200 &#125; connect_timeout 1 nb_get_retry 3 delay_before_retry 1 &#125; &#125;&#125;#第二套虚拟路由vrrp_instance VI_2 &#123; state BACKUP interface eth0 virtual_router_id 88 #ID不要和第一套虚拟路由相同 priority 90 advert_int 1 authentication &#123; auth_type PASS auth_pass 12345678 &#125; virtual_ipaddress &#123; 192.168.166.200 &#125;&#125;virtual_server 192.168.166.200 80 &#123; delay_loop 6 lb_algo rr lb_kind DR nat_mask 255.255.255.0# persistence_timeout 50 protocol TCP sorry_server 127.0.0.1 80 real_server 192.168.166.129 80 &#123; weight 1 HTTP_GET &#123; url &#123; path / status_code 200 &#125; connect_timeout 1 nb_get_retry 3 delay_before_retry 1 &#125; &#125; real_server 192.168.166.131 80 &#123; weight 1 HTTP_GET &#123; url &#123; path / status_code 200 &#125; connect_timeout 1 nb_get_retry 3 delay_before_retry 1 &#125; &#125;&#125; 第二台的配置这里就不列出了,把第一台服务器的配置文件修改一下。下图是配置好的结果。第一台服务器 image.png 第二台服务器 image.png RS配置 12 注：上面这份脚本RS1和RS2通用RS image.png RS2 image.png 调度器LVS规则 image.png 测试 image.png image.png 高可用双主Nginx配置Nginx主机 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475[root@centos7.3 ~]#yum -y install nginx #安装nginx，nginx在epel源。[root@centos7.3 ~]#vim /etc/nginx/nginx.conf #修改nginx主配置文件user nginx;worker_processes auto;error_log /var/log/nginx/error.log;pid /run/nginx.pid;# Load dynamic modules. See /usr/share/nginx/README.dynamic.include /usr/share/nginx/modules/*.conf;events &#123; worker_connections 1024;&#125;http &#123; log_format main &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos; &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos; &apos;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&apos;; access_log /var/log/nginx/access.log main; sendfile on; tcp_nopush on; tcp_nodelay on; keepalive_timeout 65; types_hash_max_size 2048; include /etc/nginx/mime.types; default_type application/octet-stream; upstream web &#123; #在http上下文中添加一个服务器组，web是组名。 server 192.168.166.129:80; #后端服务器的地址和端口 server 192.168.166.132:80;&#125; # Load modular configuration files from the /etc/nginx/conf.d directory. # See http://nginx.org/en/docs/ngx_core_module.html#include # for more information. include /etc/nginx/conf.d/*.conf; server &#123; listen [::]:80 default_server; server_name _; root /usr/share/nginx/html; # Load configuration files for the default server block. include /etc/nginx/default.d/*.conf; location / &#123; &#125; error_page 404 /404.html; location = /40x.html &#123; &#125; include /etc/nginx/conf.d/*.conf; server &#123; listen [::]:80 default_server; server_name _; root /usr/share/nginx/html; # Load configuration files for the default server block. include /etc/nginx/default.d/*.conf; location / &#123; &#125; error_page 404 /404.html; location = /40x.html &#123; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; &#125; &#125; 定义两个Nginx虚拟主机 1234567891011[root@centos7.3 nginx]#vim /etc/nginx/conf.d/host.conf server &#123; server_name www.test.com; listen 80; index index.html; root /app/web; location / &#123; proxy_pass http://web; &#125;&#125; zhu注：以上内容两台主机相同配置keepalived 第一台 1234567891011121314151617181920212223242526272829303132333435363738394041! Configuration File for keepalivedglobal_defs &#123; notification_email &#123; root@localhost &#125; notification_email_from keepalived@localhost smtp_server 127.0.0.1 smtp_connect_timeout 30 router_id CentOS7.3 vrrp_mcast_group4 224.24.1.1&#125;vrrp_instance VI_1 &#123; state MASTER interface eth0 virtual_router_id 66 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 111111 &#125; virtual_ipaddress &#123; 192.168.166.100/24 dev eth0 &#125;&#125;vrrp_instance VI_2 &#123; state BACKUP interface eth0 virtual_router_id 88 priority 90 advert_int 1 authentication &#123; auth_type PASS auth_pass 11111111 &#125; virtual_ipaddress &#123; 192.168.166.200/24 dev eth0 &#125;&#125; 第二台Nginx 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667! Configuration File for keepalivedglobal_defs &#123; notification_email &#123; root@localhost &#125; notification_email_from keepalived@localhost smtp_server 127.0.0.1 smtp_connect_timeout 30 router_id CentOS7.3 vrrp_mcast_group4 224.24.1.1&#125;vrrp_instance VI_1 &#123; state BACKUP interface eth0 virtual_router_id 66 priority 90 advert_int 1 authentication &#123; auth_type PASS auth_pass 111111 &#125; virtual_ipaddress &#123; 192.168.166.100/24 dev eth0 &#125;&#125;vrrp_instance VI_2 &#123; state MASTER interface eth0 virtual_router_id 88 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 11111111 &#125; virtual_ipaddress &#123; 192.168.166.200/24 dev eth0 &#125;&#125;! Configuration File for keepalivedglobal_defs &#123; notification_email &#123; root@localhost &#125; notification_email_from keepalived@localhost smtp_server 127.0.0.1 smtp_connect_timeout 30 router_id CentOS7.3-1 vrrp_mcast_group4 224.24.1.1&#125;vrrp_instance VI_1 &#123; state BACKUP interface eth0 virtual_router_id 66 priority 90 advert_int 1 authentication &#123; auth_type PASS auth_pass 111111 &#125; virtual_ipaddress &#123; 192.168.166.100/24 dev eth0 &#125; 测试 image.png keepalived可以调用外部的辅助脚本进行资源监控，并根据监控的结果状态能实现优先动态调整； 先定义一个脚本 1234567vrrp_script &lt;SCRIPT_NAME&gt; &#123; #定义脚本 script &quot;killall -0 sshd&quot; #可以在引号内调用命令或者脚本路径，如果脚本执行成功则不变，如果失败则执行下面的命令 interval INT #检测间隔时间 weight -INT #减掉权重 fall 2 #检测几次判定为失败 rise 2 #检测几次判定为成功&#125; killall -0 只是测试，并不执行操作，用来测试进程是否运行正常 调用此脚本 1234track_script &#123; SCRIPT_NAME_1 #调用脚本 SCRIPT_NAME_2 weight -2 #如果脚本健康状态检测失败优先级减2&#125; 配置文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051! Configuration File for keepalivedglobal_defs &#123; notification_email &#123; root@localhost &#125; notification_email_from keepalived@localhost smtp_server 127.0.0.1 smtp_connect_timeout 30 router_id CentOS7.3 vrrp_mcast_group4 224.24.1.1&#125;vrrp_script nginx &#123; script &quot;killall -0 nginx &amp;&amp; exit 0 || exit 1&quot; interval 1 weight -15 fall 2 rise 1&#125;vrrp_instance VI_1 &#123; state MASTER interface eth0 virtual_router_id 66 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 111111 &#125; virtual_ipaddress &#123; 192.168.166.100/24 dev eth0 &#125; track_script &#123; nginx &#125;&#125;vrrp_instance VI_2 &#123; state BACKUP interface eth0 virtual_router_id 88 priority 90 advert_int 1 authentication &#123; auth_type PASS auth_pass 11111111 &#125; virtual_ipaddress &#123; 192.168.166.200/24 dev eth0 &#125;&#125; 测试]]></content>
      <categories>
        <category>-keepalived</category>
      </categories>
      <tags>
        <tag>-keepalived -linux运维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx]]></title>
    <url>%2F2019%2F02%2F14%2Fnginx%2F</url>
    <content type="text"><![CDATA[什么是 Nginx由 小路依依 创建， 最后一次修改 2016-08-12 什么是 NginxNginx 是俄罗斯人编写的十分轻量级的 HTTP 服务器,Nginx，它的发音为“engine X”，是一个高性能的HTTP和反向代理服务器，同时也是一个 IMAP/POP3/SMTP 代理服务器。Nginx 是由俄罗斯人 Igor Sysoev 为俄罗斯访问量第二的 Rambler.ru 站点开发的，它已经在该站点运行超过两年半了。Igor Sysoev 在建立的项目时,使用基于 BSD 许可。 英文主页：http://nginx.net 。 到 2013 年，目前有很多国内网站采用 Nginx 作为 Web 服务器，如国内知名的新浪、163、腾讯、Discuz、豆瓣等。据 netcraft 统计，Nginx 排名第 3，约占 15% 的份额(参见：http://news.netcraft.com/archives/category/web-server-survey/ ) Nginx 以事件驱动的方式编写，所以有非常好的性能，同时也是一个非常高效的反向代理、负载平衡。其拥有匹配 Lighttpd 的性能，同时还没有 Lighttpd 的内存泄漏问题，而且 Lighttpd 的 mod_proxy 也有一些问题并且很久没有更新。 现在，Igor 将源代码以类 BSD 许可证的形式发布。Nginx 因为它的稳定性、丰富的模块库、灵活的配置和低系统资源的消耗而闻名．业界一致认为它是 Apache2.2＋mod_proxy_balancer 的轻量级代替者，不仅是因为响应静态页面的速度非常快，而且它的模块数量达到 Apache 的近 2/3。对 proxy 和 rewrite 模块的支持很彻底，还支持 mod_fcgi、ssl、vhosts ，适合用来做 mongrel clusters 的前端 HTTP 响应。 Nginx 的特点由 小路依依 创建， 最后一次修改 2016-08-12 Nginx 特点Nginx 做为 HTTP 服务器，有以下几项基本特性： 处理静态文件，索引文件以及自动索引；打开文件描述符缓冲． 无缓存的反向代理加速，简单的负载均衡和容错． FastCGI，简单的负载均衡和容错． 模块化的结构。包括 gzipping, byte ranges, chunked responses,以及 SSI-filter 等 filter。如果由 FastCGI 或其它代理服务器处理单页中存在的多个 SSI，则这项处理可以并行运行，而不需要相互等待。 支持 SSL 和 TLSSNI． Nginx 专为性能优化而开发，性能是其最重要的考量,实现上非常注重效率 。它支持内核 Poll 模型，能经受高负载的考验,有报告表明能支持高达 50,000 个并发连接数。 Nginx 具有很高的稳定性。其它 HTTP 服务器，当遇到访问的峰值，或者有人恶意发起慢速连接时，也很可能会导致服务器物理内存耗尽频繁交换，失去响应，只能重启服务器。例如当前 apache 一旦上到 200 个以上进程，web响应速度就明显非常缓慢了。而 Nginx 采取了分阶段资源分配技术，使得它的 CPU 与内存占用率非常低。Nginx 官方表示保持 10,000 个没有活动的连接，它只占 2.5M 内存，所以类似 DOS 这样的攻击对 Nginx 来说基本上是毫无用处的。就稳定性而言,Nginx 比 lighthttpd 更胜一筹。 Nginx 支持热部署。它的启动特别容易, 并且几乎可以做到 7*24 不间断运行，即使运行数个月也不需要重新启动。你还能够在不间断服务的情况下，对软件版本进行进行升级。 Nginx 采用 master-slave 模型,能够充分利用 SMP 的优势，且能够减少工作进程在磁盘 I/O 的阻塞延迟。当采用 select()/poll() 调用时，还可以限制每个进程的连接数。 Nginx 代码质量非常高，代码很规范，手法成熟，模块扩展也很容易。特别值得一提的是强大的 Upstream 与 Filter 链。Upstream 为诸如 reverse proxy,与其他服务器通信模块的编写奠定了很好的基础。而 Filter 链最酷的部分就是各个 filter 不必等待前一个 filter 执行完毕。它可以把前一个 filter 的输出做为当前 filter 的输入，这有点像 Unix 的管线。这意味着，一个模块可以开始压缩从后端服务器发送过来的请求，且可以在模块接收完后端服务器的整个请求之前把压缩流转向客户端。 Nginx 采用了一些 os 提供的最新特性如对 sendfile (Linux2.2+)，accept-filter (FreeBSD4.1+)，TCP_DEFER_ACCEPT (Linux 2.4+)的支持，从而大大提高了性能。 当然，Nginx 还很年轻，多多少少存在一些问题，比如：Nginx 是俄罗斯人创建，虽然前几年文档比较少，但是目前文档方面比较全面，英文资料居多，中文的资料也比较多，而且有专门的书籍和资料可供查找。 Nginx 的作者和社区都在不断的努力完善，我们有理由相信 Nginx 将继续以高速的增长率来分享轻量级 HTTP 服务器市场，会有一个更美好的未来。 初探 Nginx 架构由 小路依依 创建， 最后一次修改 2016-08-12 初探 Nginx 架构众所周知，Nginx 性能高，而 Nginx 的高性能与其架构是分不开的。那么 Nginx 究竟是怎么样的呢？这一节我们先来初识一下 Nginx 框架吧。 Nginx 在启动后，在 unix 系统中会以 daemon 的方式在后台运行，后台进程包含一个 master 进程和多个 worker 进程。我们也可以手动地关掉后台模式，让 Nginx 在前台运行，并且通过配置让 Nginx 取消 master 进程，从而可以使 Nginx 以单进程方式运行。很显然，生产环境下我们肯定不会这么做，所以关闭后台模式，一般是用来调试用的，在后面的章节里面，我们会详细地讲解如何调试 Nginx。所以，我们可以看到，Nginx 是以多进程的方式来工作的，当然 Nginx 也是支持多线程的方式的，只是我们主流的方式还是多进程的方式，也是 Nginx 的默认方式。Nginx 采用多进程的方式有诸多好处，所以我就主要讲解 Nginx 的多进程模式吧。 刚才讲到，Nginx 在启动后，会有一个 master 进程和多个 worker 进程。master 进程主要用来管理 worker 进程，包含：接收来自外界的信号，向各 worker 进程发送信号，监控 worker 进程的运行状态，当 worker 进程退出后(异常情况下)，会自动重新启动新的 worker 进程。而基本的网络事件，则是放在 worker 进程中来处理了。多个 worker 进程之间是对等的，他们同等竞争来自客户端的请求，各进程互相之间是独立的。一个请求，只可能在一个 worker 进程中处理，一个 worker 进程，不可能处理其它进程的请求。worker 进程的个数是可以设置的，一般我们会设置与机器cpu核数一致，这里面的原因与 Nginx 的进程模型以及事件处理模型是分不开的。Nginx 的进程模型，可以由下图来表示： 在 Nginx 启动后，如果我们要操作 Nginx，要怎么做呢？从上文中我们可以看到，master 来管理 worker 进程，所以我们只需要与 master 进程通信就行了。master 进程会接收来自外界发来的信号，再根据信号做不同的事情。所以我们要控制 Nginx，只需要通过 kill 向 master 进程发送信号就行了。比如kill -HUP pid，则是告诉 Nginx，从容地重启 Nginx，我们一般用这个信号来重启 Nginx，或重新加载配置，因为是从容地重启，因此服务是不中断的。master 进程在接收到 HUP 信号后是怎么做的呢？首先 master 进程在接到信号后，会先重新加载配置文件，然后再启动新的 worker 进程，并向所有老的 worker 进程发送信号，告诉他们可以光荣退休了。新的 worker 在启动后，就开始接收新的请求，而老的 worker 在收到来自 master 的信号后，就不再接收新的请求，并且在当前进程中的所有未处理完的请求处理完成后，再退出。当然，直接给 master 进程发送信号，这是比较老的操作方式，Nginx 在 0.8 版本之后，引入了一系列命令行参数，来方便我们管理。比如，./nginx -s reload，就是来重启 Nginx，./nginx -s stop，就是来停止 Nginx 的运行。如何做到的呢？我们还是拿 reload 来说，我们看到，执行命令时，我们是启动一个新的 Nginx 进程，而新的 Nginx 进程在解析到 reload 参数后，就知道我们的目的是控制 Nginx 来重新加载配置文件了，它会向 master 进程发送信号，然后接下来的动作，就和我们直接向 master 进程发送信号一样了。 现在，我们知道了当我们在操作 Nginx 的时候，Nginx 内部做了些什么事情，那么，worker 进程又是如何处理请求的呢？我们前面有提到，worker 进程之间是平等的，每个进程，处理请求的机会也是一样的。当我们提供 80 端口的 http 服务时，一个连接请求过来，每个进程都有可能处理这个连接，怎么做到的呢？首先，每个 worker 进程都是从 master 进程 fork 过来，在 master 进程里面，先建立好需要 listen 的 socket（listenfd）之后，然后再 fork 出多个 worker 进程。所有 worker 进程的 listenfd 会在新连接到来时变得可读，为保证只有一个进程处理该连接，所有 worker 进程在注册 listenfd 读事件前抢 accept_mutex，抢到互斥锁的那个进程注册 listenfd 读事件，在读事件里调用 accept 接受该连接。当一个 worker 进程在 accept 这个连接之后，就开始读取请求，解析请求，处理请求，产生数据后，再返回给客户端，最后才断开连接，这样一个完整的请求就是这样的了。我们可以看到，一个请求，完全由 worker 进程来处理，而且只在一个 worker 进程中处理。 那么，Nginx 采用这种进程模型有什么好处呢？当然，好处肯定会很多了。首先，对于每个 worker 进程来说，独立的进程，不需要加锁，所以省掉了锁带来的开销，同时在编程以及问题查找时，也会方便很多。其次，采用独立的进程，可以让互相之间不会影响，一个进程退出后，其它进程还在工作，服务不会中断，master 进程则很快启动新的 worker 进程。当然，worker 进程的异常退出，肯定是程序有 bug 了，异常退出，会导致当前 worker 上的所有请求失败，不过不会影响到所有请求，所以降低了风险。当然，好处还有很多，大家可以慢慢体会。 上面讲了很多关于 Nginx 的进程模型，接下来，我们来看看 Nginx 是如何处理事件的。 有人可能要问了，Nginx 采用多 worker 的方式来处理请求，每个 worker 里面只有一个主线程，那能够处理的并发数很有限啊，多少个 worker 就能处理多少个并发，何来高并发呢？非也，这就是 Nginx 的高明之处，Nginx 采用了异步非阻塞的方式来处理请求，也就是说，Nginx 是可以同时处理成千上万个请求的。想想 apache 的常用工作方式（apache 也有异步非阻塞版本，但因其与自带某些模块冲突，所以不常用），每个请求会独占一个工作线程，当并发数上到几千时，就同时有几千的线程在处理请求了。这对操作系统来说，是个不小的挑战，线程带来的内存占用非常大，线程的上下文切换带来的 cpu 开销很大，自然性能就上不去了，而这些开销完全是没有意义的。 为什么 Nginx 可以采用异步非阻塞的方式来处理呢，或者异步非阻塞到底是怎么回事呢？我们先回到原点，看看一个请求的完整过程。首先，请求过来，要建立连接，然后再接收数据，接收数据后，再发送数据。具体到系统底层，就是读写事件，而当读写事件没有准备好时，必然不可操作，如果不用非阻塞的方式来调用，那就得阻塞调用了，事件没有准备好，那就只能等了，等事件准备好了，你再继续吧。阻塞调用会进入内核等待，cpu 就会让出去给别人用了，对单线程的 worker 来说，显然不合适，当网络事件越多时，大家都在等待呢，cpu 空闲下来没人用，cpu利用率自然上不去了，更别谈高并发了。好吧，你说加进程数，这跟apache的线程模型有什么区别，注意，别增加无谓的上下文切换。所以，在 Nginx 里面，最忌讳阻塞的系统调用了。不要阻塞，那就非阻塞喽。非阻塞就是，事件没有准备好，马上返回 EAGAIN，告诉你，事件还没准备好呢，你慌什么，过会再来吧。好吧，你过一会，再来检查一下事件，直到事件准备好了为止，在这期间，你就可以先去做其它事情，然后再来看看事件好了没。虽然不阻塞了，但你得不时地过来检查一下事件的状态，你可以做更多的事情了，但带来的开销也是不小的。所以，才会有了异步非阻塞的事件处理机制，具体到系统调用就是像 select/poll/epoll/kqueue 这样的系统调用。它们提供了一种机制，让你可以同时监控多个事件，调用他们是阻塞的，但可以设置超时时间，在超时时间之内，如果有事件准备好了，就返回。这种机制正好解决了我们上面的两个问题，拿 epoll 为例(在后面的例子中，我们多以 epoll 为例子，以代表这一类函数)，当事件没准备好时，放到 epoll 里面，事件准备好了，我们就去读写，当读写返回 EAGAIN 时，我们将它再次加入到 epoll 里面。这样，只要有事件准备好了，我们就去处理它，只有当所有事件都没准备好时，才在 epoll 里面等着。这样，我们就可以并发处理大量的并发了，当然，这里的并发请求，是指未处理完的请求，线程只有一个，所以同时能处理的请求当然只有一个了，只是在请求间进行不断地切换而已，切换也是因为异步事件未准备好，而主动让出的。这里的切换是没有任何代价，你可以理解为循环处理多个准备好的事件，事实上就是这样的。与多线程相比，这种事件处理方式是有很大的优势的，不需要创建线程，每个请求占用的内存也很少，没有上下文切换，事件处理非常的轻量级。并发数再多也不会导致无谓的资源浪费（上下文切换）。更多的并发数，只是会占用更多的内存而已。 我之前有对连接数进行过测试，在 24G 内存的机器上，处理的并发请求数达到过 200 万。现在的网络服务器基本都采用这种方式，这也是nginx性能高效的主要原因。 我们之前说过，推荐设置 worker 的个数为 cpu 的核数，在这里就很容易理解了，更多的 worker 数，只会导致进程来竞争 cpu 资源了，从而带来不必要的上下文切换。而且，nginx为了更好的利用多核特性，提供了 cpu 亲缘性的绑定选项，我们可以将某一个进程绑定在某一个核上，这样就不会因为进程的切换带来 cache 的失效。像这种小的优化在 Nginx 中非常常见，同时也说明了 Nginx 作者的苦心孤诣。比如，Nginx 在做 4 个字节的字符串比较时，会将 4 个字符转换成一个 int 型，再作比较，以减少 cpu 的指令数等等。 现在，知道了 Nginx 为什么会选择这样的进程模型与事件模型了。对于一个基本的 Web 服务器来说，事件通常有三种类型，网络事件、信号、定时器。从上面的讲解中知道，网络事件通过异步非阻塞可以很好的解决掉。如何处理信号与定时器？ 首先，信号的处理。对 Nginx 来说，有一些特定的信号，代表着特定的意义。信号会中断掉程序当前的运行，在改变状态后，继续执行。如果是系统调用，则可能会导致系统调用的失败，需要重入。关于信号的处理，大家可以学习一些专业书籍，这里不多说。对于 Nginx 来说，如果nginx正在等待事件（epoll_wait 时），如果程序收到信号，在信号处理函数处理完后，epoll_wait 会返回错误，然后程序可再次进入 epoll_wait 调用。 另外，再来看看定时器。由于 epoll_wait 等函数在调用的时候是可以设置一个超时时间的，所以 Nginx 借助这个超时时间来实现定时器。nginx里面的定时器事件是放在一颗维护定时器的红黑树里面，每次在进入 epoll_wait前，先从该红黑树里面拿到所有定时器事件的最小时间，在计算出 epoll_wait 的超时时间后进入 epoll_wait。所以，当没有事件产生，也没有中断信号时，epoll_wait 会超时，也就是说，定时器事件到了。这时，nginx会检查所有的超时事件，将他们的状态设置为超时，然后再去处理网络事件。由此可以看出，当我们写 Nginx 代码时，在处理网络事件的回调函数时，通常做的第一个事情就是判断超时，然后再去处理网络事件。 我们可以用一段伪代码来总结一下 Nginx 的事件处理模型： 12345678910111213141516171819202122while (true) &#123; for t in run_tasks: t.handler(); update_time(&amp;now); timeout = ETERNITY; for t in wait_tasks: /* sorted already */ if (t.time &lt;= now) &#123; t.timeout_handler(); &#125; else &#123; timeout = t.time - now; break; &#125; nevents = poll_function(events, timeout); for i in nevents: task t; if (events[i].type == READ) &#123; t.handler = read_handler; &#125; else &#123; /* events[i].type == WRITE */ t.handler = write_handler; &#125; run_tasks_add(t);&#125; 好，本节我们讲了进程模型，事件模型，包括网络事件，信号，定时器事件。 Nginx 基础概念由 小路依依 创建， 最后一次修改 2016-08-12 Nginx 基础概念connection在 Nginx 中 connection 就是对 tcp 连接的封装，其中包括连接的 socket，读事件，写事件。利用 Nginx 封装的 connection，我们可以很方便的使用 Nginx 来处理与连接相关的事情，比如，建立连接，发送与接受数据等。而 Nginx 中的 http 请求的处理就是建立在 connection之上的，所以 Nginx 不仅可以作为一个web服务器，也可以作为邮件服务器。当然，利用 Nginx 提供的 connection，我们可以与任何后端服务打交道。 结合一个 tcp 连接的生命周期，我们看看 Nginx 是如何处理一个连接的。首先，Nginx 在启动时，会解析配置文件，得到需要监听的端口与 ip 地址，然后在 Nginx 的 master 进程里面，先初始化好这个监控的 socket(创建 socket，设置 addrreuse 等选项，绑定到指定的 ip 地址端口，再 listen)，然后再 fork 出多个子进程出来，然后子进程会竞争 accept 新的连接。此时，客户端就可以向 Nginx 发起连接了。当客户端与服务端通过三次握手建立好一个连接后，Nginx 的某一个子进程会 accept 成功，得到这个建立好的连接的 socket，然后创建 Nginx 对连接的封装，即 ngx_connection_t 结构体。接着，设置读写事件处理函数并添加读写事件来与客户端进行数据的交换。最后，Nginx 或客户端来主动关掉连接，到此，一个连接就寿终正寝了。 当然，Nginx 也是可以作为客户端来请求其它 server 的数据的（如 upstream 模块），此时，与其它 server 创建的连接，也封装在 ngx_connection_t 中。作为客户端，Nginx 先获取一个 ngx_connection_t 结构体，然后创建 socket，并设置 socket 的属性（ 比如非阻塞）。然后再通过添加读写事件，调用 connect/read/write 来调用连接，最后关掉连接，并释放 ngx_connection_t。 在 Nginx 中，每个进程会有一个连接数的最大上限，这个上限与系统对 fd 的限制不一样。在操作系统中，通过 ulimit -n，我们可以得到一个进程所能够打开的 fd 的最大数，即 nofile，因为每个 socket 连接会占用掉一个 fd，所以这也会限制我们进程的最大连接数，当然也会直接影响到我们程序所能支持的最大并发数，当 fd 用完后，再创建 socket 时，就会失败。Nginx 通过设置 worker_connectons 来设置每个进程支持的最大连接数。如果该值大于 nofile，那么实际的最大连接数是 nofile，Nginx 会有警告。Nginx 在实现时，是通过一个连接池来管理的，每个 worker 进程都有一个独立的连接池，连接池的大小是 worker_connections。这里的连接池里面保存的其实不是真实的连接，它只是一个 worker_connections 大小的一个 ngx_connection_t 结构的数组。并且，Nginx 会通过一个链表 free_connections 来保存所有的空闲 ngx_connection_t，每次获取一个连接时，就从空闲连接链表中获取一个，用完后，再放回空闲连接链表里面。 在这里，很多人会误解 worker_connections 这个参数的意思，认为这个值就是 Nginx 所能建立连接的最大值。其实不然，这个值是表示每个 worker 进程所能建立连接的最大值，所以，一个 Nginx 能建立的最大连接数，应该是worker_connections * worker_processes。当然，这里说的是最大连接数，对于 HTTP 请求本地资源来说，能够支持的最大并发数量是worker_connections * worker_processes，而如果是 HTTP 作为反向代理来说，最大并发数量应该是worker_connections * worker_processes/2。因为作为反向代理服务器，每个并发会建立与客户端的连接和与后端服务的连接，会占用两个连接。 那么，我们前面有说过一个客户端连接过来后，多个空闲的进程，会竞争这个连接，很容易看到，这种竞争会导致不公平，如果某个进程得到 accept 的机会比较多，它的空闲连接很快就用完了，如果不提前做一些控制，当 accept 到一个新的 tcp 连接后，因为无法得到空闲连接，而且无法将此连接转交给其它进程，最终会导致此 tcp 连接得不到处理，就中止掉了。很显然，这是不公平的，有的进程有空余连接，却没有处理机会，有的进程因为没有空余连接，却人为地丢弃连接。那么，如何解决这个问题呢？首先，Nginx 的处理得先打开 accept_mutex 选项，此时，只有获得了 accept_mutex 的进程才会去添加accept事件，也就是说，Nginx会控制进程是否添加 accept 事件。Nginx 使用一个叫 ngx_accept_disabled 的变量来控制是否去竞争 accept_mutex 锁。在第一段代码中，计算 ngx_accept_disabled 的值，这个值是 Nginx 单进程的所有连接总数的八分之一，减去剩下的空闲连接数量，得到的这个 ngx_accept_disabled 有一个规律，当剩余连接数小于总连接数的八分之一时，其值才大于 0，而且剩余的连接数越小，这个值越大。再看第二段代码，当 ngx_accept_disabled 大于 0 时，不会去尝试获取 accept_mutex 锁，并且将 ngx_accept_disabled 减 1，于是，每次执行到此处时，都会去减 1，直到小于 0。不去获取 accept_mutex 锁，就是等于让出获取连接的机会，很显然可以看出，当空余连接越少时，ngx_accept_disable 越大，于是让出的机会就越多，这样其它进程获取锁的机会也就越大。不去 accept，自己的连接就控制下来了，其它进程的连接池就会得到利用，这样，Nginx 就控制了多进程间连接的平衡了。 12345678910111213141516171819202122ngx_accept_disabled = ngx_cycle-&gt;connection_n / 8 - ngx_cycle-&gt;free_connection_n;if (ngx_accept_disabled &gt; 0) &#123; ngx_accept_disabled--;&#125; else &#123; if (ngx_trylock_accept_mutex(cycle) == NGX_ERROR) &#123; return; &#125; if (ngx_accept_mutex_held) &#123; flags |= NGX_POST_EVENTS; &#125; else &#123; if (timer == NGX_TIMER_INFINITE || timer &gt; ngx_accept_mutex_delay) &#123; timer = ngx_accept_mutex_delay; &#125; &#125;&#125; 好了，连接就先介绍到这，本章的目的是介绍基本概念，知道在 Nginx 中连接是个什么东西就行了，而且连接是属于比较高级的用法，在后面的模块开发高级篇会有专门的章节来讲解连接与事件的实现及使用。 request这节我们讲 request，在 Nginx 中我们指的是 http 请求，具体到 Nginx 中的数据结构是ngx_http_request_t。ngx_http_request_t 是对一个 http 请求的封装。 我们知道，一个 http 请求，包含请求行、请求头、请求体、响应行、响应头、响应体。 http 请求是典型的请求-响应类型的的网络协议，而 http 是文本协议，所以我们在分析请求行与请求头，以及输出响应行与响应头，往往是一行一行的进行处理。如果我们自己来写一个 http 服务器，通常在一个连接建立好后，客户端会发送请求过来。然后我们读取一行数据，分析出请求行中包含的 method、uri、http_version 信息。然后再一行一行处理请求头，并根据请求 method 与请求头的信息来决定是否有请求体以及请求体的长度，然后再去读取请求体。得到请求后，我们处理请求产生需要输出的数据，然后再生成响应行，响应头以及响应体。在将响应发送给客户端之后，一个完整的请求就处理完了。当然这是最简单的 webserver 的处理方式，其实 Nginx 也是这样做的，只是有一些小小的区别，比如，当请求头读取完成后，就开始进行请求的处理了。Nginx 通过 ngx_http_request_t 来保存解析请求与输出响应相关的数据。 那接下来，简要讲讲 Nginx 是如何处理一个完整的请求的。对于 Nginx 来说，一个请求是从ngx_http_init_request 开始的，在这个函数中，会设置读事件为 ngx_http_process_request_line，也就是说，接下来的网络事件，会由 ngx_http_process_request_line 来执行。从ngx_http_process_request_line 的函数名，我们可以看到，这就是来处理请求行的，正好与之前讲的，处理请求的第一件事就是处理请求行是一致的。通过 ngx_http_read_request_header 来读取请求数据。然后调用 ngx_http_parse_request_line 函数来解析请求行。Nginx 为提高效率，采用状态机来解析请求行，而且在进行 method 的比较时，没有直接使用字符串比较，而是将四个字符转换成一个整型，然后一次比较以减少 cpu 的指令数，这个前面有说过。很多人可能很清楚一个请求行包含请求的方法，uri，版本，却不知道其实在请求行中，也是可以包含有 host 的。比如一个请求 GET http://www.taobao.com/uri HTTP/1.0 这样一个请求行也是合法的，而且 host 是 www.taobao.com，这个时候，Nginx 会忽略请求头中的 host 域，而以请求行中的这个为准来查找虚拟主机。另外，对于对于 http0.9 版来说，是不支持请求头的，所以这里也是要特别的处理。所以，在后面解析请求头时，协议版本都是 1.0 或 1.1。整个请求行解析到的参数，会保存到 ngx_http_request_t 结构当中。 在解析完请求行后，Nginx 会设置读事件的 handler 为 ngx_http_process_request_headers，然后后续的请求就在 ngx_http_process_request_headers 中进行读取与解析。ngx_http_process_request_headers 函数用来读取请求头，跟请求行一样，还是调用 ngx_http_read_request_header 来读取请求头，调用 ngx_http_parse_header_line 来解析一行请求头，解析到的请求头会保存到 ngx_http_request_t 的域 headers_in 中，headers_in 是一个链表结构，保存所有的请求头。而 HTTP 中有些请求是需要特别处理的，这些请求头与请求处理函数存放在一个映射表里面，即 ngx_http_headers_in，在初始化时，会生成一个 hash 表，当每解析到一个请求头后，就会先在这个 hash 表中查找，如果有找到，则调用相应的处理函数来处理这个请求头。比如:Host 头的处理函数是 ngx_http_process_host。 当 Nginx 解析到两个回车换行符时，就表示请求头的结束，此时就会调用 ngx_http_process_request 来处理请求了。ngx_http_process_request 会设置当前的连接的读写事件处理函数为 ngx_http_request_handler，然后再调用 ngx_http_handler 来真正开始处理一个完整的http请求。这里可能比较奇怪，读写事件处理函数都是ngx_http_request_handler，其实在这个函数中，会根据当前事件是读事件还是写事件，分别调用 ngx_http_request_t 中的 read_event_handler 或者是 write_event_handler。由于此时，我们的请求头已经读取完成了，之前有说过，Nginx 的做法是先不读取请求 body，所以这里面我们设置 read_event_handler 为 ngx_http_block_reading，即不读取数据了。刚才说到，真正开始处理数据，是在 ngx_http_handler 这个函数里面，这个函数会设置 write_event_handler 为 ngx_http_core_run_phases，并执行 ngx_http_core_run_phases 函数。ngx_http_core_run_phases 这个函数将执行多阶段请求处理，Nginx 将一个 http 请求的处理分为多个阶段，那么这个函数就是执行这些阶段来产生数据。因为 ngx_http_core_run_phases 最后会产生数据，所以我们就很容易理解，为什么设置写事件的处理函数为 ngx_http_core_run_phases 了。在这里，我简要说明了一下函数的调用逻辑，我们需要明白最终是调用 ngx_http_core_run_phases 来处理请求，产生的响应头会放在 ngx_http_request_t 的 headers_out 中，这一部分内容，我会放在请求处理流程里面去讲。Nginx 的各种阶段会对请求进行处理，最后会调用 filter 来过滤数据，对数据进行加工，如 truncked 传输、gzip 压缩等。这里的 filter 包括 header filter 与 body filter，即对响应头或响应体进行处理。filter 是一个链表结构，分别有 header filter 与 body filter，先执行 header filter 中的所有 filter，然后再执行 body filter 中的所有 filter。在 header filter 中的最后一个 filter，即 ngx_http_header_filter，这个 filter 将会遍历所有的响应头，最后需要输出的响应头在一个连续的内存，然后调用 ngx_http_write_filter 进行输出。ngx_http_write_filter 是 body filter 中的最后一个，所以 Nginx 首先的 body 信息，在经过一系列的 body filter 之后，最后也会调用 ngx_http_write_filter 来进行输出(有图来说明)。 这里要注意的是，Nginx 会将整个请求头都放在一个 buffer 里面，这个 buffer 的大小通过配置项 client_header_buffer_size 来设置，如果用户的请求头太大，这个 buffer 装不下，那 Nginx 就会重新分配一个新的更大的 buffer 来装请求头，这个大 buffer 可以通过 large_client_header_buffers 来设置，这个 large_buffer 这一组 buffer，比如配置 48k，就是表示有四个 8k 大小的 buffer 可以用。注意，为了保存请求行或请求头的完整性，一个完整的请求行或请求头，需要放在一个连续的内存里面，所以，一个完整的请求行或请求头，只会保存在一个 buffer 里面。这样，如果请求行大于一个 buffer 的大小，就会返回 414 错误，如果一个请求头大小大于一个 buffer 大小，就会返回 400 错误。在了解了这些参数的值，以及 Nginx 实际的做法之后，在应用场景，我们就需要根据实际的需求来调整这些参数，来优化我们的程序了。 处理流程图： 以上这些，就是 Nginx 中一个 http 请求的生命周期了。我们再看看与请求相关的一些概念吧。 keepalive当然，在 Nginx 中，对于 http1.0 与 http1.1 也是支持长连接的。什么是长连接呢？我们知道，http 请求是基于 TCP 协议之上的，那么，当客户端在发起请求前，需要先与服务端建立 TCP 连接，而每一次的 TCP 连接是需要三次握手来确定的，如果客户端与服务端之间网络差一点，这三次交互消费的时间会比较多，而且三次交互也会带来网络流量。当然，当连接断开后，也会有四次的交互，当然对用户体验来说就不重要了。而 http 请求是请求应答式的，如果我们能知道每个请求头与响应体的长度，那么我们是可以在一个连接上面执行多个请求的，这就是所谓的长连接，但前提条件是我们先得确定请求头与响应体的长度。对于请求来说，如果当前请求需要有body，如 POST 请求，那么 Nginx 就需要客户端在请求头中指定 content-length 来表明 body 的大小，否则返回 400 错误。也就是说，请求体的长度是确定的，那么响应体的长度呢？先来看看 http 协议中关于响应 body 长度的确定： 对于 http1.0 协议来说，如果响应头中有 content-length 头，则以 content-length 的长度就可以知道 body 的长度了，客户端在接收 body 时，就可以依照这个长度来接收数据，接收完后，就表示这个请求完成了。而如果没有 content-length 头，则客户端会一直接收数据，直到服务端主动断开连接，才表示 body 接收完了。 而对于 http1.1 协议来说，如果响应头中的 Transfer-encoding 为 chunked 传输，则表示 body 是流式输出，body 会被分成多个块，每块的开始会标识出当前块的长度，此时，body 不需要通过长度来指定。如果是非 chunked 传输，而且有 content-length，则按照 content-length 来接收数据。否则，如果是非 chunked，并且没有 content-length，则客户端接收数据，直到服务端主动断开连接。 从上面，我们可以看到，除了 http1.0 不带 content-length 以及 http1.1 非 chunked 不带 content-length 外，body 的长度是可知的。此时，当服务端在输出完 body 之后，会可以考虑使用长连接。能否使用长连接，也是有条件限制的。如果客户端的请求头中的 connection为close，则表示客户端需要关掉长连接，如果为 keep-alive，则客户端需要打开长连接，如果客户端的请求中没有 connection 这个头，那么根据协议，如果是 http1.0，则默认为 close，如果是 http1.1，则默认为 keep-alive。如果结果为 keepalive，那么，Nginx 在输出完响应体后，会设置当前连接的 keepalive 属性，然后等待客户端下一次请求。当然，Nginx 不可能一直等待下去，如果客户端一直不发数据过来，岂不是一直占用这个连接？所以当 Nginx 设置了 keepalive 等待下一次的请求时，同时也会设置一个最大等待时间，这个时间是通过选项 keepalive_timeout 来配置的，如果配置为 0，则表示关掉 keepalive，此时，http 版本无论是 1.1 还是 1.0，客户端的 connection 不管是 close 还是 keepalive，都会强制为 close。 如果服务端最后的决定是 keepalive 打开，那么在响应的 http 头里面，也会包含有 connection 头域，其值是”Keep-Alive”，否则就是”Close”。如果 connection 值为 close，那么在 Nginx 响应完数据后，会主动关掉连接。所以，对于请求量比较大的 Nginx 来说，关掉 keepalive 最后会产生比较多的 time-wait 状态的 socket。一般来说，当客户端的一次访问，需要多次访问同一个 server 时，打开 keepalive 的优势非常大，比如图片服务器，通常一个网页会包含很多个图片。打开 keepalive 也会大量减少 time-wait 的数量。 pipe在 http1.1 中，引入了一种新的特性，即 pipeline。那么什么是 pipeline 呢？pipeline 其实就是流水线作业，它可以看作为 keepalive 的一种升华，因为 pipeline 也是基于长连接的，目的就是利用一个连接做多次请求。如果客户端要提交多个请求，对于keepalive来说，那么第二个请求，必须要等到第一个请求的响应接收完全后，才能发起，这和 TCP 的停止等待协议是一样的，得到两个响应的时间至少为2*RTT。而对 pipeline 来说，客户端不必等到第一个请求处理完后，就可以马上发起第二个请求。得到两个响应的时间可能能够达到1*RTT。Nginx 是直接支持 pipeline 的，但是，Nginx 对 pipeline 中的多个请求的处理却不是并行的，依然是一个请求接一个请求的处理，只是在处理第一个请求的时候，客户端就可以发起第二个请求。这样，Nginx 利用 pipeline 减少了处理完一个请求后，等待第二个请求的请求头数据的时间。其实 Nginx 的做法很简单，前面说到，Nginx 在读取数据时，会将读取的数据放到一个 buffer 里面，所以，如果 Nginx 在处理完前一个请求后，如果发现 buffer 里面还有数据，就认为剩下的数据是下一个请求的开始，然后就接下来处理下一个请求，否则就设置 keepalive。 lingering_closelingering_close，字面意思就是延迟关闭，也就是说，当 Nginx 要关闭连接时，并非立即关闭连接，而是先关闭 tcp 连接的写，再等待一段时间后再关掉连接的读。为什么要这样呢？我们先来看看这样一个场景。Nginx 在接收客户端的请求时，可能由于客户端或服务端出错了，要立即响应错误信息给客户端，而 Nginx 在响应错误信息后，大分部情况下是需要关闭当前连接。Nginx 执行完 write()系统调用把错误信息发送给客户端，write()系统调用返回成功并不表示数据已经发送到客户端，有可能还在 tcp 连接的 write buffer 里。接着如果直接执行 close()系统调用关闭 tcp 连接，内核会首先检查 tcp 的 read buffer 里有没有客户端发送过来的数据留在内核态没有被用户态进程读取，如果有则发送给客户端 RST 报文来关闭 tcp 连接丢弃 write buffer 里的数据，如果没有则等待 write buffer 里的数据发送完毕，然后再经过正常的 4 次分手报文断开连接。所以,当在某些场景下出现 tcp write buffer 里的数据在 write()系统调用之后到 close()系统调用执行之前没有发送完毕，且 tcp read buffer 里面还有数据没有读，close()系统调用会导致客户端收到 RST 报文且不会拿到服务端发送过来的错误信息数据。那客户端肯定会想，这服务器好霸道，动不动就 reset 我的连接，连个错误信息都没有。 在上面这个场景中，我们可以看到，关键点是服务端给客户端发送了 RST 包，导致自己发送的数据在客户端忽略掉了。所以，解决问题的重点是，让服务端别发 RST 包。再想想，我们发送 RST 是因为我们关掉了连接，关掉连接是因为我们不想再处理此连接了，也不会有任何数据产生了。对于全双工的 TCP 连接来说，我们只需要关掉写就行了，读可以继续进行，我们只需要丢掉读到的任何数据就行了，这样的话，当我们关掉连接后，客户端再发过来的数据，就不会再收到 RST 了。当然最终我们还是需要关掉这个读端的，所以我们会设置一个超时时间，在这个时间过后，就关掉读，客户端再发送数据来就不管了，作为服务端我会认为，都这么长时间了，发给你的错误信息也应该读到了，再慢就不关我事了，要怪就怪你 RP 不好了。当然，正常的客户端，在读取到数据后，会关掉连接，此时服务端就会在超时时间内关掉读端。这些正是 lingering_close 所做的事情。协议栈提供 SO_LINGER 这个选项，它的一种配置情况就是来处理 lingering_close 的情况的，不过 Nginx 是自己实现的 lingering_close。lingering_close 存在的意义就是来读取剩下的客户端发来的数据，所以 Nginx 会有一个读超时时间，通过 lingering_timeout 选项来设置，如果在 lingering_timeout 时间内还没有收到数据，则直接关掉连接。Nginx 还支持设置一个总的读取时间，通过 lingering_time 来设置，这个时间也就是 Nginx 在关闭写之后，保留 socket 的时间，客户端需要在这个时间内发送完所有的数据，否则 Nginx 在这个时间过后，会直接关掉连接。当然，Nginx 是支持配置是否打开 lingering_close 选项的，通过 lingering_close 选项来配置。 那么，我们在实际应用中，是否应该打开 lingering_close 呢？这个就没有固定的推荐值了，如 Maxim Dounin所说，lingering_close 的主要作用是保持更好的客户端兼容性，但是却需要消耗更多的额外资源（比如连接会一直占着）。 这节，我们介绍了 Nginx 中，连接与请求的基本概念，下节，我们讲基本的数据结构。 Nginx 基本数据结构由 小路依依 创建， 最后一次修改 2016-08-12 基本数据结构Nginx 的作者为追求极致的高效，自己实现了很多颇具特色的 Nginx 风格的数据结构以及公共函数。比如，Nginx 提供了带长度的字符串，根据编译器选项优化过的字符串拷贝函数 ngx_copy 等。所以，在我们写 Nginx 模块时，应该尽量调用 Nginx 提供的 api，尽管有些 api 只是对 glibc 的宏定义。本节，我们介绍 string、list、buffer、chain 等一系列最基本的数据结构及相关api的使用技巧以及注意事项。 ngx_str_t在 Nginx 源码目录的 src/core 下面的 ngx_string.h|c 里面，包含了字符串的封装以及字符串相关操作的 api。Nginx 提供了一个带长度的字符串结构 ngx_str_t，它的原型如下： 1234typedef struct &#123; size_t len; u_char *data;&#125; ngx_str_t; 在结构体当中，data 指向字符串数据的第一个字符，字符串的结束用长度来表示，而不是由&#39;\\0&#39;来表示结束。所以，在写 Nginx 代码时，处理字符串的方法跟我们平时使用有很大的不一样，但要时刻记住，字符串不以&#39;\\0&#39;结束，尽量使用 Nginx 提供的字符串操作的 api 来操作字符串。 那么，Nginx 这样做有什么好处呢？首先，通过长度来表示字符串长度，减少计算字符串长度的次数。其次，Nginx 可以重复引用一段字符串内存，data 可以指向任意内存，长度表示结束，而不用去 copy 一份自己的字符串(因为如果要以&#39;\\0&#39;结束，而不能更改原字符串，所以势必要 copy 一段字符串)。我们在 ngx_http_request_t 结构体的成员中，可以找到很多字符串引用一段内存的例子，比如 request_line、uri、args 等等，这些字符串的 data 部分，都是指向在接收数据时创建 buffer 所指向的内存中，uri，args 就没有必要 copy 一份出来。这样的话，减少了很多不必要的内存分配与拷贝。 正是基于此特性，在 Nginx 中，必须谨慎的去修改一个字符串。在修改字符串时需要认真的去考虑：是否可以修改该字符串；字符串修改后，是否会对其它的引用造成影响。在后面介绍 ngx_unescape_uri 函数的时候，就会看到这一点。但是，使用 Nginx 的字符串会产生一些问题，glibc 提供的很多系统 api 函数大多是通过&#39;\\0&#39;来表示字符串的结束，所以我们在调用系统 api 时，就不能直接传入 str-&gt;data 了。此时，通常的做法是创建一段 str-&gt;len + 1 大小的内存，然后 copy 字符串，最后一个字节置为&#39;\\0&#39;。比较 hack 的做法是，将字符串最后一个字符的后一个字符 backup 一个，然后设置为&#39;\\0&#39;，在做完调用后，再由 backup 改回来，但前提条件是，你得确定这个字符是可以修改的，而且是有内存分配，不会越界，但一般不建议这么做。接下来，看看 Nginx 提供的操作字符串相关的 api。 1#define ngx_string(str) &#123; sizeof(str) - 1, (u_char *) str &#125; ngx_string(str) 是一个宏，它通过一个以&#39;\\0&#39;结尾的普通字符串 str 构造一个 Nginx 的字符串，鉴于其中采用 sizeof 操作符计算字符串长度，因此参数必须是一个常量字符串。 1#define ngx_null_string &#123; 0, NULL &#125; 定义变量时，使用 ngx_null_string 初始化字符串为空字符串，符串的长度为 0，data 为 NULL。 12#define ngx_str_set(str, text) \ (str)-&gt;len = sizeof(text) - 1; (str)-&gt;data = (u_char *) text ngx_str_set 用于设置字符串 str 为 text，由于使用 sizeof 计算长度，故 text 必须为常量字符串。 1#define ngx_str_null(str) (str)-&gt;len = 0; (str)-&gt;data = NULL ngx_str_null 用于设置字符串 str 为空串，长度为 0，data 为 NULL。 上面这四个函数，使用时一定要小心，ngx_string 与 ngx_null_string 是“{，}”格式的，故只能用于赋值时初始化，如： 12ngx_str_t str = ngx_string(&quot;hello world&quot;);ngx_str_t str1 = ngx_null_string; 如果向下面这样使用，就会有问题，这里涉及到c语言中对结构体变量赋值操作的语法规则，在此不做介绍。 123ngx_str_t str, str1;str = ngx_string(&quot;hello world&quot;); // 编译出错str1 = ngx_null_string; // 编译出错 这种情况，可以调用 ngx_str_set 与 ngx_str_null 这两个函数来做: 123ngx_str_t str, str1;ngx_str_set(&amp;str, &quot;hello world&quot;); ngx_str_null(&amp;str1); 按照 C99 标准，您也可以这么做： 123ngx_str_t str, str1;str = (ngx_str_t) ngx_string(&quot;hello world&quot;);str1 = (ngx_str_t) ngx_null_string; 另外要注意的是，ngx_string 与 ngx_str_set 在调用时，传进去的字符串一定是常量字符串，否则会得到意想不到的错误(因为 ngx_str_set 内部使用了 sizeof()，如果传入的是 u_char*，那么计算的是这个指针的长度，而不是字符串的长度)。如： 123ngx_str_t str;u_char *a = &quot;hello world&quot;;ngx_str_set(&amp;str, a); // 问题产生 此外，值得注意的是，由于 ngx_str_set 与 ngx_str_null 实际上是两行语句，故在 if/for/while 等语句中单独使用需要用花括号括起来，例如： 123456ngx_str_t str;if (cond) ngx_str_set(&amp;str, &quot;true&quot;); // 问题产生else ngx_str_set(&amp;str, &quot;false&quot;); // 问题产生void ngx_strlow(u_char *dst, u_char *src, size_t n); 将 src 的前 n 个字符转换成小写存放在 dst 字符串当中，调用者需要保证 dst 指向的空间大于等于n，且指向的空间必须可写。操作不会对原字符串产生变动。如要更改原字符串，可以： 12ngx_strlow(str-&gt;data, str-&gt;data, str-&gt;len);ngx_strncmp(s1, s2, n) 区分大小写的字符串比较，只比较前n个字符。 1ngx_strcmp(s1, s2) 区分大小写的不带长度的字符串比较。 1ngx_int_t ngx_strcasecmp(u_char *s1, u_char *s2); 不区分大小写的不带长度的字符串比较。 1ngx_int_t ngx_strncasecmp(u_char *s1, u_char *s2, size_t n); 不区分大小写的带长度的字符串比较，只比较前 n 个字符。 123u_char * ngx_cdecl ngx_sprintf(u_char *buf, const char *fmt, ...);u_char * ngx_cdecl ngx_snprintf(u_char *buf, size_t max, const char *fmt, ...);u_char * ngx_cdecl ngx_slprintf(u_char *buf, u_char *last, const char *fmt, ...); 上面这三个函数用于字符串格式化，ngx_snprintf 的第二个参数 max 指明 buf 的空间大小，ngx_slprintf 则通过 last 来指明 buf 空间的大小。推荐使用第二个或第三个函数来格式化字符串，ngx_sprintf 函数还是比较危险的，容易产生缓冲区溢出漏洞。在这一系列函数中，Nginx 在兼容 glibc 中格式化字符串的形式之外，还添加了一些方便格式化 Nginx 类型的一些转义字符，比如%V用于格式化 ngx_str_t 结构。在 Nginx 源文件的 ngx_string.c 中有说明： 123456789101112131415161718192021222324252627282930/* * supported formats: * %[0][width][x][X]O off_t * %[0][width]T time_t * %[0][width][u][x|X]z ssize_t/size_t * %[0][width][u][x|X]d int/u_int * %[0][width][u][x|X]l long * %[0][width|m][u][x|X]i ngx_int_t/ngx_uint_t * %[0][width][u][x|X]D int32_t/uint32_t * %[0][width][u][x|X]L int64_t/uint64_t * %[0][width|m][u][x|X]A ngx_atomic_int_t/ngx_atomic_uint_t * %[0][width][.width]f double, max valid number fits to %18.15f * %P ngx_pid_t * %M ngx_msec_t * %r rlim_t * %p void * * %V ngx_str_t * * %v ngx_variable_value_t * * %s null-terminated string * %*s length and string * %Z &apos;\0&apos; * %N &apos;\n&apos; * %c char * %% % * * reserved: * %t ptrdiff_t * %S null-terminated wchar string * %C wchar */ 这里特别要提醒的是，我们最常用于格式化 ngx_str_t 结构，其对应的转义符是%V，传给函数的一定要是指针类型，否则程序就会 coredump 掉。这也是我们最容易犯的错。比如： 12345ngx_str_t str = ngx_string(&quot;hello world&quot;);u_char buffer[1024];ngx_snprintf(buffer, 1024, &quot;%V&quot;, &amp;str); // 注意，str取地址void ngx_encode_base64(ngx_str_t *dst, ngx_str_t *src);ngx_int_t ngx_decode_base64(ngx_str_t *dst, ngx_str_t *src); 这两个函数用于对 str 进行 base64 编码与解码，调用前，需要保证 dst 中有足够的空间来存放结果，如果不知道具体大小，可先调用 ngx_base64_encoded_length 与 ngx_base64_decoded_length 来预估最大占用空间。 12uintptr_t ngx_escape_uri(u_char *dst, u_char *src, size_t size, ngx_uint_t type); 对 src 进行编码，根据 type 来按不同的方式进行编码，如果 dst 为 NULL，则返回需要转义的字符的数量，由此可得到需要的空间大小。type 的类型可以是： 1234567#define NGX_ESCAPE_URI 0#define NGX_ESCAPE_ARGS 1#define NGX_ESCAPE_HTML 2#define NGX_ESCAPE_REFRESH 3#define NGX_ESCAPE_MEMCACHED 4#define NGX_ESCAPE_MAIL_AUTH 5void ngx_unescape_uri(u_char **dst, u_char **src, size_t size, ngx_uint_t type); 对 src 进行反编码，type 可以是 0、NGX_UNESCAPE_URI、NGX_UNESCAPE_REDIRECT 这三个值。如果是 0，则表示 src 中的所有字符都要进行转码。如果是 NGX_UNESCAPE_URI 与 NGX_UNESCAPE_REDIRECT，则遇到&#39;?&#39;后就结束了，后面的字符就不管了。而 NGX_UNESCAPE_URI 与 NGX_UNESCAPE_REDIRECT 之间的区别是 NGX_UNESCAPE_URI 对于遇到的需要转码的字符，都会转码，而 NGX_UNESCAPE_REDIRECT 则只会对非可见字符进行转码。 1uintptr_t ngx_escape_html(u_char *dst, u_char *src, size_t size); 对 html 标签进行编码。 当然，我这里只介绍了一些常用的 api 的使用，大家可以先熟悉一下，在实际使用过程中，遇到不明白的，最快最直接的方法就是去看源码，看 api 的实现或看 Nginx 自身调用 api 的地方是怎么做的，代码就是最好的文档。 ngx_pool_tngx_pool_t是一个非常重要的数据结构，在很多重要的场合都有使用，很多重要的数据结构也都在使用它。那么它究竟是一个什么东西呢？简单的说，它提供了一种机制，帮助管理一系列的资源（如内存，文件等），使得对这些资源的使用和释放统一进行，免除了使用过程中考虑到对各种各样资源的什么时候释放，是否遗漏了释放的担心。 例如对于内存的管理，如果我们需要使用内存，那么总是从一个 ngx_pool_t 的对象中获取内存，在最终的某个时刻，我们销毁这个 ngx_pool_t 对象，所有这些内存都被释放了。这样我们就不必要对对这些内存进行 malloc 和 free 的操作，不用担心是否某块被malloc出来的内存没有被释放。因为当 ngx_pool_t 对象被销毁的时候，所有从这个对象中分配出来的内存都会被统一释放掉。 再比如我们要使用一系列的文件，但是我们打开以后，最终需要都关闭，那么我们就把这些文件统一登记到一个 ngx_pool_t 对象中，当这个 ngx_pool_t 对象被销毁的时候，所有这些文件都将会被关闭。 从上面举的两个例子中我们可以看出，使用 ngx_pool_t 这个数据结构的时候，所有的资源的释放都在这个对象被销毁的时刻，统一进行了释放，那么就会带来一个问题，就是这些资源的生存周期（或者说被占用的时间）是跟 ngx_pool_t 的生存周期基本一致（ngx_pool_t 也提供了少量操作可以提前释放资源）。从最高效的角度来说，这并不是最好的。比如，我们需要依次使用 A，B，C 三个资源，且使用完 B 的时候，A 就不会再被使用了，使用C的时候 A 和 B 都不会被使用到。如果不使用 ngx_pool_t 来管理这三个资源，那我们可能从系统里面申请 A，使用 A，然后在释放 A。接着申请 B，使用 B，再释放 B。最后申请 C，使用 C，然后释放 C。但是当我们使用一个 ngx_pool_t 对象来管理这三个资源的时候，A，B 和 C 的释放是在最后一起发生的，也就是在使用完 C 以后。诚然，这在客观上增加了程序在一段时间的资源使用量。但是这也减轻了程序员分别管理三个资源的生命周期的工作。这也就是有所得，必有所失的道理。实际上是一个取舍的问题，要看在具体的情况下，你更在乎的是哪个。 可以看一下在 Nginx 里面一个典型的使用 ngx_pool_t 的场景，对于 Nginx 处理的每个 http request, Nginx 会生成一个 ngx_pool_t 对象与这个 http request 关联，所有处理过程中需要申请的资源都从这个 ngx_pool_t 对象中获取，当这个 http request 处理完成以后，所有在处理过程中申请的资源，都将随着这个关联的 ngx_pool_t 对象的销毁而释放。 ngx_pool_t 相关结构及操作被定义在文件src/core/ngx_palloc.h|c中。 1234567891011typedef struct ngx_pool_s ngx_pool_t; struct ngx_pool_s &#123; ngx_pool_data_t d; size_t max; ngx_pool_t *current; ngx_chain_t *chain; ngx_pool_large_t *large; ngx_pool_cleanup_t *cleanup; ngx_log_t *log;&#125;; 从 ngx_pool_t 的一般使用者的角度来说，可不用关注 ngx_pool_t 结构中各字段作用。所以这里也不会进行详细的解释，当然在说明某些操作函数的使用的时候，如有必要，会进行说明。 下面我们来分别解释下 ngx_pool_t 的相关操作。 1ngx_pool_t *ngx_create_pool(size_t size, ngx_log_t *log); 创建一个初始节点大小为 size 的 pool，log 为后续在该 pool 上进行操作时输出日志的对象。 需要说明的是 size 的选择，size 的大小必须小于等于 NGX_MAX_ALLOC_FROM_POOL，且必须大于 sizeof(ngx_pool_t)。 选择大于 NGX_MAX_ALLOC_FROM_POOL 的值会造成浪费，因为大于该限制的空间不会被用到（只是说在第一个由 ngx_pool_t 对象管理的内存块上的内存，后续的分配如果第一个内存块上的空闲部分已用完，会再分配的）。 选择小于 sizeof(ngx_pool_t)的值会造成程序崩溃。由于初始大小的内存块中要用一部分来存储 ngx_pool_t 这个信息本身。 当一个 ngx_pool_t 对象被创建以后，该对象的 max 字段被赋值为 size-sizeof(ngx_pool_t)和 NGX_MAX_ALLOC_FROM_POOL 这两者中比较小的。后续的从这个 pool 中分配的内存块，在第一块内存使用完成以后，如果要继续分配的话，就需要继续从操作系统申请内存。当内存的大小小于等于 max 字段的时候，则分配新的内存块，链接在 d 这个字段（实际上是 d.next 字段）管理的一条链表上。当要分配的内存块是比 max 大的，那么从系统中申请的内存是被挂接在 large 字段管理的一条链表上。我们暂且把这个称之为大块内存链和小块内存链。 1void *ngx_palloc(ngx_pool_t *pool, size_t size); 从这个 pool 中分配一块为 size 大小的内存。注意，此函数分配的内存的起始地址按照 NGX_ALIGNMENT 进行了对齐。对齐操作会提高系统处理的速度，但会造成少量内存的浪费。 1void *ngx_pnalloc(ngx_pool_t *pool, size_t size); 从这个 pool 中分配一块为 size 大小的内存。但是此函数分配的内存并没有像上面的函数那样进行过对齐。 .. code:: c 1void *ngx_pcalloc(ngx_pool_t *pool, size_t size); 该函数也是分配size大小的内存，并且对分配的内存块进行了清零。内部实际上是转调用ngx_palloc实现的。 1void *ngx_pmemalign(ngx_pool_t *pool, size_t size, size_t alignment); 按照指定对齐大小 alignment 来申请一块大小为 size 的内存。此处获取的内存不管大小都将被置于大内存块链中管理。 1ngx_int_t ngx_pfree(ngx_pool_t *pool, void *p); 对于被置于大块内存链，也就是被 large 字段管理的一列内存中的某块进行释放。该函数的实现是顺序遍历 large 管理的大块内存链表。所以效率比较低下。如果在这个链表中找到了这块内存，则释放，并返回 NGX_OK。否则返回 NGX_DECLINED。 由于这个操作效率比较低下，除非必要，也就是说这块内存非常大，确应及时释放，否则一般不需要调用。反正内存在这个 pool 被销毁的时候，总归会都释放掉的嘛！ 1ngx_pool_cleanup_t *ngx_pool_cleanup_add(ngx_pool_t *p, size_t size); ngx_pool_t 中的 cleanup 字段管理着一个特殊的链表，该链表的每一项都记录着一个特殊的需要释放的资源。对于这个链表中每个节点所包含的资源如何去释放，是自说明的。这也就提供了非常大的灵活性。意味着，ngx_pool_t 不仅仅可以管理内存，通过这个机制，也可以管理任何需要释放的资源，例如，关闭文件，或者删除文件等等。下面我们看一下这个链表每个节点的类型: 12345678typedef struct ngx_pool_cleanup_s ngx_pool_cleanup_t;typedef void (*ngx_pool_cleanup_pt)(void *data);struct ngx_pool_cleanup_s &#123; ngx_pool_cleanup_pt handler; void *data; ngx_pool_cleanup_t *next;&#125;; data: 指明了该节点所对应的资源。 handler: 是一个函数指针，指向一个可以释放 data 所对应资源的函数。该函数只有一个参数，就是 data。 next: 指向该链表中下一个元素。 看到这里，ngx_pool_cleanup_add 这个函数的用法，我相信大家都应该有一些明白了。但是这个参数 size 是起什么作用的呢？这个 size 就是要存储这个 data 字段所指向的资源的大小，该函数会为 data 分配 size 大小的空间。 比如我们需要最后删除一个文件。那我们在调用这个函数的时候，把 size 指定为存储文件名的字符串的大小，然后调用这个函数给 cleanup 链表中增加一项。该函数会返回新添加的这个节点。我们然后把这个节点中的 data 字段拷贝为文件名。把 hander 字段赋值为一个删除文件的函数（当然该函数的原型要按照 void (\*ngx_pool_cleanup_pt)(void \*data)）。 1void ngx_destroy_pool(ngx_pool_t *pool); 该函数就是释放 pool 中持有的所有内存，以及依次调用 cleanup 字段所管理的链表中每个元素的 handler 字段所指向的函数，来释放掉所有该 pool 管理的资源。并且把 pool 指向的 ngx_pool_t 也释放掉了，完全不可用了。 1void ngx_reset_pool(ngx_pool_t *pool); 该函数释放 pool 中所有大块内存链表上的内存，小块内存链上的内存块都修改为可用。但是不会去处理 cleanup链表上的项目。 ngx_array_tngx_array_t 是 Nginx 内部使用的数组结构。Nginx 的数组结构在存储上与大家认知的 C 语言内置的数组有相似性，比如实际上存储数据的区域也是一大块连续的内存。但是数组除了存储数据的内存以外还包含一些元信息来描述相关的一些信息。下面我们从数组的定义上来详细的了解一下。ngx_array_t 的定义位于src/core/ngx_array.c|h里面。 12345678typedef struct ngx_array_s ngx_array_t;struct ngx_array_s &#123; void *elts; ngx_uint_t nelts; size_t size; ngx_uint_t nalloc; ngx_pool_t *pool;&#125;; elts: 指向实际的数据存储区域。 nelts: 数组实际元素个数。 size: 数组单个元素的大小，单位是字节。 nalloc: 数组的容量。表示该数组在不引发扩容的前提下，可以最多存储的元素的个数。当 nelts 增长到达 nalloc 时，如果再往此数组中存储元素，则会引发数组的扩容。数组的容量将会扩展到原有容量的 2 倍大小。实际上是分配新的一块内存，新的一块内存的大小是原有内存大小的 2 倍。原有的数据会被拷贝到新的一块内存中。 pool: 该数组用来分配内存的内存池。 下面介绍 ngx_array_t 相关操作函数。 1ngx_array_t *ngx_array_create(ngx_pool_t *p, ngx_uint_t n, size_t size); 创建一个新的数组对象，并返回这个对象。 p: 数组分配内存使用的内存池； n: 数组的初始容量大小，即在不扩容的情况下最多可以容纳的元素个数。 size: 单个元素的大小，单位是字节。 1void ngx_array_destroy(ngx_array_t *a); 销毁该数组对象，并释放其分配的内存回内存池。 1void *ngx_array_push(ngx_array_t *a); 在数组 a 上新追加一个元素，并返回指向新元素的指针。需要把返回的指针使用类型转换，转换为具体的类型，然后再给新元素本身或者是各字段（如果数组的元素是复杂类型）赋值。 1void *ngx_array_push_n(ngx_array_t *a, ngx_uint_t n); 在数组 a 上追加 n 个元素，并返回指向这些追加元素的首个元素的位置的指针。 1static ngx_inline ngx_int_t ngx_array_init(ngx_array_t *array, ngx_pool_t *pool, ngx_uint_t n, size_t size); 如果一个数组对象是被分配在堆上的，那么当调用 ngx_array_destroy 销毁以后，如果想再次使用，就可以调用此函数。 如果一个数组对象是被分配在栈上的，那么就需要调用此函数，进行初始化的工作以后，才可以使用。 注意事项由于使用 ngx_palloc 分配内存，数组在扩容时，旧的内存不会被释放，会造成内存的浪费。因此，最好能提前规划好数组的容量，在创建或者初始化的时候一次搞定，避免多次扩容，造成内存浪费。 ngx_hash_tngx_hash_t 是 Nginx 自己的 hash 表的实现。定义和实现位于src/core/ngx_hash.h|c中。ngx_hash_t 的实现也与数据结构教科书上所描述的 hash 表的实现是大同小异。对于常用的解决冲突的方法有线性探测，二次探测和开链法等。ngx_hash_t 使用的是最常用的一种，也就是开链法，这也是 STL 中的 hash 表使用的方法。 但是 ngx_hash_t 的实现又有其几个显著的特点: ngx_hash_t 不像其他的 hash 表的实现，可以插入删除元素，它只能一次初始化，就构建起整个 hash 表以后，既不能再删除，也不能在插入元素了。 ngx_hash_t 的开链并不是真的开了一个链表，实际上是开了一段连续的存储空间，几乎可以看做是一个数组。这是因为 ngx_hash_t 在初始化的时候，会经历一次预计算的过程，提前把每个桶里面会有多少元素放进去给计算出来，这样就提前知道每个桶的大小了。那么就不需要使用链表，一段连续的存储空间就足够了。这也从一定程度上节省了内存的使用。 从上面的描述，我们可以看出来，这个值越大，越造成内存的浪费。就两步，首先是初始化，然后就可以在里面进行查找了。下面我们详细来看一下。 ngx_hash_t 的初始化。 12ngx_int_t ngx_hash_init(ngx_hash_init_t *hinit, ngx_hash_key_t *names,ngx_uint_t nelts); 首先我们来看一下初始化函数。该函数的第一个参数 hinit 是初始化的一些参数的一个集合。 names 是初始化一个 ngx_hash_t 所需要的所有 key 的一个数组。而 nelts 就是 key 的个数。下面先看一下 ngx_hash_init_t 类型，该类型提供了初始化一个 hash 表所需要的一些基本信息。 1234567891011typedef struct &#123; ngx_hash_t *hash; ngx_hash_key_pt key; ngx_uint_t max_size; ngx_uint_t bucket_size; char *name; ngx_pool_t *pool; ngx_pool_t *temp_pool;&#125; ngx_hash_init_t; hash: 该字段如果为 NULL，那么调用完初始化函数后，该字段指向新创建出来的 hash 表。如果该字段不为 NULL，那么在初始的时候，所有的数据被插入了这个字段所指的 hash 表中。 key: 指向从字符串生成 hash 值的 hash 函数。Nginx 的源代码中提供了默认的实现函数 ngx_hash_key_lc。 max_size: hash 表中的桶的个数。该字段越大，元素存储时冲突的可能性越小，每个桶中存储的元素会更少，则查询起来的速度更快。当然，这个值越大，越造成内存的浪费也越大，(实际上也浪费不了多少)。 :bucket_size: 每个桶的最大限制大小，单位是字节。如果在初始化一个 hash 表的时候，发现某个桶里面无法存的下所有属于该桶的元素，则 hash 表初始化失败。 name: 该 hash 表的名字。 pool: 该 hash 表分配内存使用的 pool。 temp_pool: 该 hash 表使用的临时 pool，在初始化完成以后，该 pool 可以被释放和销毁掉。 下面来看一下存储 hash 表 key 的数组的结构。 12345typedef struct &#123; ngx_str_t key; ngx_uint_t key_hash; void *value;&#125; ngx_hash_key_t; key 和 value 的含义显而易见，就不用解释了。key_hash 是对 key 使用 hash 函数计算出来的值。 对这两个结构分析完成以后，我想大家应该都已经明白这个函数应该是如何使用了吧。该函数成功初始化一个 hash 表以后，返回 NGX_OK，否则返回 NGX_ERROR。 1void *ngx_hash_find(ngx_hash_t *hash, ngx_uint_t key, u_char *name, size_t len); 在 hash 里面查找 key 对应的 value。实际上这里的 key 是对真正的 key（也就是 name）计算出的 hash 值。len 是 name 的长度。 如果查找成功，则返回指向 value 的指针，否则返回 NULL。 ngx_hash_wildcard_tNginx 为了处理带有通配符的域名的匹配问题，实现了 ngx_hash_wildcard_t 这样的 hash 表。他可以支持两种类型的带有通配符的域名。一种是通配符在前的，例如：\*.abc.com，也可以省略掉星号，直接写成.abc.com。这样的 key，可以匹配 www.abc.com，qqq.www.abc.com 之类的。另外一种是通配符在末尾的，例如：mail.xxx.\*，请特别注意通配符在末尾的不像位于开始的通配符可以被省略掉。这样的通配符，可以匹配 mail.xxx.com、mail.xxx.com.cn、mail.xxx.net 之类的域名。 有一点必须说明，就是一个 ngx_hash_wildcard_t 类型的 hash 表只能包含通配符在前的key或者是通配符在后的key。不能同时包含两种类型的通配符的 key。ngx_hash_wildcard_t 类型变量的构建是通过函数 ngx_hash_wildcard_init 完成的，而查询是通过函数 ngx_hash_find_wc_head 或者 ngx_hash_find_wc_tail 来做的。ngx_hash_find_wc_head 查询包含通配符在前的 key 的 hash 表的，而 ngx_hash_find_wc_tail 是查询包含通配符在后的 key 的 hash 表的。 下面详细说明这几个函数的用法。 12ngx_int_t ngx_hash_wildcard_init(ngx_hash_init_t *hinit, ngx_hash_key_t *names, ngx_uint_t nelts); 该函数用来构建一个可以包含通配符 key 的 hash 表。 hinit: 构造一个通配符 hash 表的一些参数的一个集合。关于该参数对应的类型的说明，请参见 ngx_hash_t 类型中 ngx_hash_init 函数的说明。 names: 构造此 hash 表的所有的通配符 key 的数组。特别要注意的是这里的 key 已经都是被预处理过的。例如：\*.abc.com或者.abc.com被预处理完成以后，变成了com.abc.。而mail.xxx.\*则被预处理为mail.xxx.。为什么会被处理这样？这里不得不简单地描述一下通配符 hash 表的实现原理。当构造此类型的 hash 表的时候，实际上是构造了一个 hash 表的一个“链表”，是通过 hash 表中的 key “链接”起来的。比如：对于\*.abc.com将会构造出 2 个 hash 表，第一个 hash 表中有一个 key 为 com 的表项，该表项的 value 包含有指向第二个 hash 表的指针，而第二个 hash 表中有一个表项 abc，该表项的 value 包含有指向\*.abc.com对应的 value 的指针。那么查询的时候，比如查询 www.abc.com 的时候，先查 com，通过查 com 可以找到第二级的 hash 表，在第二级 hash 表中，再查找 abc，依次类推，直到在某一级的 hash 表中查到的表项对应的 value 对应一个真正的值而非一个指向下一级 hash 表的指针的时候，查询过程结束。这里有一点需要特别注意的，就是 names 数组中元素的 value 值低两位 bit 必须为 0（有特殊用途）。如果不满足这个条件，这个 hash 表查询不出正确结果。 nelts: names 数组元素的个数。 该函数执行成功返回 NGX_OK，否则 NGX_ERROR。 1void *ngx_hash_find_wc_head(ngx_hash_wildcard_t *hwc, u_char *name, size_t len); 该函数查询包含通配符在前的 key 的 hash 表的。 hwc: hash 表对象的指针。 name: 需要查询的域名，例如: www.abc.com。 len: name 的长度。 该函数返回匹配的通配符对应 value。如果没有查到，返回 NULL。 1void *ngx_hash_find_wc_tail(ngx_hash_wildcard_t *hwc, u_char *name, size_t len); 该函数查询包含通配符在末尾的 key 的 hash 表的。 参数及返回值请参加上个函数的说明。 ngx_hash_combined_t组合类型 hash 表，该 hash 表的定义如下： 12345typedef struct &#123; ngx_hash_t hash; ngx_hash_wildcard_t *wc_head; ngx_hash_wildcard_t *wc_tail;&#125; ngx_hash_combined_t; 从其定义显见，该类型实际上包含了三个 hash 表，一个普通 hash 表，一个包含前向通配符的 hash 表和一个包含后向通配符的 hash 表。 Nginx 提供该类型的作用，在于提供一个方便的容器包含三个类型的 hash 表，当有包含通配符的和不包含通配符的一组 key 构建 hash 表以后，以一种方便的方式来查询，你不需要再考虑一个 key 到底是应该到哪个类型的 hash 表里去查了。 构造这样一组合 hash 表的时候，首先定义一个该类型的变量，再分别构造其包含的三个子 hash 表即可。 对于该类型 hash 表的查询，Nginx 提供了一个方便的函数 ngx_hash_find_combined。 12void *ngx_hash_find_combined(ngx_hash_combined_t *hash, ngx_uint_t key,u_char *name, size_t len); 该函数在此组合 hash 表中，依次查询其三个子 hash 表，看是否匹配，一旦找到，立即返回查找结果，也就是说如果有多个可能匹配，则只返回第一个匹配的结果。 hash: 此组合 hash 表对象。 key: 根据 name 计算出的 hash 值。 name: key 的具体内容。 len: name 的长度。 返回查询的结果，未查到则返回 NULL。 ngx_hash_keys_arrays_t大家看到在构建一个 ngx_hash_wildcard_t 的时候，需要对通配符的哪些 key 进行预处理。这个处理起来比较麻烦。而当有一组 key，这些里面既有无通配符的 key，也有包含通配符的 key 的时候。我们就需要构建三个 hash 表，一个包含普通的 key 的 hash 表，一个包含前向通配符的 hash 表，一个包含后向通配符的 hash 表（或者也可以把这三个 hash 表组合成一个 ngx_hash_combined_t）。在这种情况下，为了让大家方便的构造这些 hash 表，Nginx 提供给了此辅助类型。 该类型以及相关的操作函数也定义在src/core/ngx_hash.h|c里。我们先来看一下该类型的定义。 123456789101112131415typedef struct &#123; ngx_uint_t hsize; ngx_pool_t *pool; ngx_pool_t *temp_pool; ngx_array_t keys; ngx_array_t *keys_hash; ngx_array_t dns_wc_head; ngx_array_t *dns_wc_head_hash; ngx_array_t dns_wc_tail; ngx_array_t *dns_wc_tail_hash;&#125; ngx_hash_keys_arrays_t; hsize: 将要构建的 hash 表的桶的个数。对于使用这个结构中包含的信息构建的三种类型的 hash 表都会使用此参数。 pool: 构建这些 hash 表使用的 pool。 temp_pool: 在构建这个类型以及最终的三个 hash 表过程中可能用到临时 pool。该 temp_pool 可以在构建完成以后，被销毁掉。这里只是存放临时的一些内存消耗。 keys: 存放所有非通配符 key 的数组。 keys_hash: 这是个二维数组，第一个维度代表的是 bucket 的编号，那么 keys_hash[i] 中存放的是所有的 key 算出来的 hash 值对 hsize 取模以后的值为 i 的 key。假设有 3 个 key,分别是 key1,key2 和 key3 假设 hash 值算出来以后对 hsize 取模的值都是 i，那么这三个 key 的值就顺序存放在keys_hash[i][0],keys_hash[i][1], keys_hash[i][2]。该值在调用的过程中用来保存和检测是否有冲突的 key 值，也就是是否有重复。 dns_wc_head: 放前向通配符 key 被处理完成以后的值。比如：\*.abc.com 被处理完成以后，变成 “com.abc.” 被存放在此数组中。 dns_wc_tail: 存放后向通配符 key 被处理完成以后的值。比如：mail.xxx.\* 被处理完成以后，变成 “mail.xxx.” 被存放在此数组中。 dns_wc_head_hash: 该值在调用的过程中用来保存和检测是否有冲突的前向通配符的 key 值，也就是是否有重复。 dns_wc_tail_hash: 该值在调用的过程中用来保存和检测是否有冲突的后向通配符的 key 值，也就是是否有重复。 在定义一个这个类型的变量，并对字段 pool 和 temp_pool 赋值以后，就可以调用函数 ngx_hash_add_key 把所有的 key 加入到这个结构中了，该函数会自动实现普通 key，带前向通配符的 key 和带后向通配符的 key 的分类和检查，并将这个些值存放到对应的字段中去，然后就可以通过检查这个结构体中的 keys、dns_wc_head、dns_wc_tail 三个数组是否为空，来决定是否构建普通 hash 表，前向通配符 hash 表和后向通配符 hash 表了（在构建这三个类型的 hash 表的时候，可以分别使用 keys、dns_wc_head、dns_wc_tail三个数组）。 构建出这三个 hash 表以后，可以组合在一个 ngx_hash_combined_t 对象中，使用 ngx_hash_find_combined 进行查找。或者是仍然保持三个独立的变量对应这三个 hash 表，自己决定何时以及在哪个 hash 表中进行查询。 1ngx_int_t ngx_hash_keys_array_init(ngx_hash_keys_arrays_t *ha, ngx_uint_t type); 初始化这个结构，主要是对这个结构中的 ngx_array_t 类型的字段进行初始化，成功返回 NGX_OK。 ha: 该结构的对象指针。 type: 该字段有 2 个值可选择，即 NGX_HASH_SMALL 和 NGX_HASH_LARGE。用来指明将要建立的 hash 表的类型，如果是 NGX_HASH_SMALL，则有比较小的桶的个数和数组元素大小。NGX_HASH_LARGE 则相反。 12ngx_int_t ngx_hash_add_key(ngx_hash_keys_arrays_t *ha, ngx_str_t *key,void *value, ngx_uint_t flags); 一般是循环调用这个函数，把一组键值对加入到这个结构体中。返回 NGX_OK 是加入成功。返回 NGX_BUSY 意味着key值重复。 ha: 该结构的对象指针。 key: 参数名自解释了。 value: 参数名自解释了。 flags: 有两个标志位可以设置，NGX_HASH_WILDCARD_KEY 和 NGX_HASH_READONLY_KEY。同时要设置的使用逻辑与操作符就可以了。NGX_HASH_READONLY_KEY 被设置的时候，在计算 hash 值的时候，key 的值不会被转成小写字符，否则会。NGX_HASH_WILDCARD_KEY 被设置的时候，说明 key 里面可能含有通配符，会进行相应的处理。如果两个标志位都不设置，传 0。 有关于这个数据结构的使用，可以参考src/http/ngx_http.c中的 ngx_http_server_names 函数。 ngx_chain_tNginx 的 filter 模块在处理从别的 filter 模块或者是 handler 模块传递过来的数据（实际上就是需要发送给客户端的 http response）。这个传递过来的数据是以一个链表的形式(ngx_chain_t)。而且数据可能被分多次传递过来。也就是多次调用 filter 的处理函数，以不同的 ngx_chain_t。 该结构被定义在src/core/ngx_buf.h|c。下面我们来看一下 ngx_chain_t 的定义。 123456typedef struct ngx_chain_s ngx_chain_t;struct ngx_chain_s &#123; ngx_buf_t *buf; ngx_chain_t *next;&#125;; 就 2 个字段，next 指向这个链表的下个节点。buf 指向实际的数据。所以在这个链表上追加节点也是非常容易，只要把末尾元素的 next 指针指向新的节点，把新节点的 next 赋值为 NULL 即可。 1ngx_chain_t *ngx_alloc_chain_link(ngx_pool_t *pool); 该函数创建一个 ngx_chain_t 的对象，并返回指向对象的指针，失败返回 NULL。 123#define ngx_free_chain(pool, cl) \ cl-&gt;next = pool-&gt;chain; \pool-&gt;chain = cl 该宏释放一个 ngx_chain_t 类型的对象。如果要释放整个 chain，则迭代此链表，对每个节点使用此宏即可。 注意: 对 ngx_chaint_t 类型的释放，并不是真的释放了内存，而仅仅是把这个对象挂在了这个 pool 对象的一个叫做 chain 的字段对应的 chain 上，以供下次从这个 pool 上分配 ngx_chain_t 类型对象的时候，快速的从这个 pool-&gt;chain上 取下链首元素就返回了，当然，如果这个链是空的，才会真的在这个 pool 上使用 ngx_palloc 函数进行分配。 ngx_buf_t这个 ngx_buf_t 就是这个 ngx_chain_t 链表的每个节点的实际数据。该结构实际上是一种抽象的数据结构，它代表某种具体的数据。这个数据可能是指向内存中的某个缓冲区，也可能指向一个文件的某一部分，也可能是一些纯元数据（元数据的作用在于指示这个链表的读取者对读取的数据进行不同的处理）。 该数据结构位于src/core/ngx_buf.h|c文件中。我们来看一下它的定义。 123456789101112131415161718192021222324252627282930313233343536struct ngx_buf_s &#123; u_char *pos; u_char *last; off_t file_pos; off_t file_last; u_char *start; /* start of buffer */ u_char *end; /* end of buffer */ ngx_buf_tag_t tag; ngx_file_t *file; ngx_buf_t *shadow; /* the buf's content could be changed */ unsigned temporary:1; /* * the buf's content is in a memory cache or in a read only memory * and must not be changed */ unsigned memory:1; /* the buf's content is mmap()ed and must not be changed */ unsigned mmap:1; unsigned recycled:1; unsigned in_file:1; unsigned flush:1; unsigned sync:1; unsigned last_buf:1; unsigned last_in_chain:1; unsigned last_shadow:1; unsigned temp_file:1; /* STUB */ int num;&#125;; pos：当 buf 所指向的数据在内存里的时候，pos 指向的是这段数据开始的位置。 last：当 buf 所指向的数据在内存里的时候，last 指向的是这段数据结束的位置。 file_pos：当 buf 所指向的数据是在文件里的时候，file_pos 指向的是这段数据的开始位置在文件中的偏移量。 file_last：当 buf 所指向的数据是在文件里的时候，file_last 指向的是这段数据的结束位置在文件中的偏移量。 start：当 buf 所指向的数据在内存里的时候，这一整块内存包含的内容可能被包含在多个 buf 中(比如在某段数据中间插入了其他的数据，这一块数据就需要被拆分开)。那么这些 buf 中的 start 和 end 都指向这一块内存的开始地址和结束地址。而 pos 和 last 指向本 buf 所实际包含的数据的开始和结尾。 end：解释参见 start。 tag：实际上是一个void *类型的指针，使用者可以关联任意的对象上去，只要对使用者有意义。 file：当 buf 所包含的内容在文件中时，file字段指向对应的文件对象。 shadow：当这个 buf 完整 copy 了另外一个 buf 的所有字段的时候，那么这两个 buf 指向的实际上是同一块内存，或者是同一个文件的同一部分，此时这两个 buf 的 shadow 字段都是指向对方的。那么对于这样的两个 buf，在释放的时候，就需要使用者特别小心，具体是由哪里释放，要提前考虑好，如果造成资源的多次释放，可能会造成程序崩溃！ temporary：为 1 时表示该 buf 所包含的内容是在一个用户创建的内存块中，并且可以被在 filter 处理的过程中进行变更，而不会造成问题。 memory：为 1 时表示该 buf 所包含的内容是在内存中，但是这些内容却不能被进行处理的 filter 进行变更。 mmap：为 1 时表示该 buf 所包含的内容是在内存中, 是通过 mmap 使用内存映射从文件中映射到内存中的，这些内容却不能被进行处理的 filter 进行变更。 recycled：可以回收的。也就是这个 buf 是可以被释放的。这个字段通常是配合 shadow 字段一起使用的，对于使用 ngx_create_temp_buf 函数创建的 buf，并且是另外一个 buf 的 shadow，那么可以使用这个字段来标示这个buf是可以被释放的。 in_file：为 1 时表示该 buf 所包含的内容是在文件中。 flush：遇到有 flush 字段被设置为 1 的 buf 的 chain，则该 chain 的数据即便不是最后结束的数据（last_buf被设置，标志所有要输出的内容都完了），也会进行输出，不会受 postpone_output 配置的限制，但是会受到发送速率等其他条件的限制。 last_buf：数据被以多个 chain 传递给了过滤器，此字段为 1 表明这是最后一个 buf。 last_in_chain：在当前的 chain 里面，此 buf 是最后一个。特别要注意的是 last_in_chain 的 buf 不一定是last_buf，但是 last_buf 的 buf 一定是 last_in_chain 的。这是因为数据会被以多个 chain 传递给某 个filter 模块。 last_shadow：在创建一个 buf 的 shadow 的时候，通常将新创建的一个 buf 的 last_shadow 置为 1。 temp_file:由于受到内存使用的限制，有时候一些 buf 的内容需要被写到磁盘上的临时文件中去，那么这时，就设置此标志。 对于此对象的创建，可以直接在某个 ngx_pool_t 上分配，然后根据需要，给对应的字段赋值。也可以使用定义好的 2 个宏： 12#define ngx_alloc_buf(pool) ngx_palloc(pool, sizeof(ngx_buf_t))#define ngx_calloc_buf(pool) ngx_pcalloc(pool, sizeof(ngx_buf_t)) 这两个宏使用类似函数，也是不说自明的。 对于创建 temporary 字段为 1 的 buf（就是其内容可以被后续的 filter 模块进行修改），可以直接使用函数 ngx_create_temp_buf 进行创建。 1ngx_buf_t *ngx_create_temp_buf(ngx_pool_t *pool, size_t size); 该函数创建一个 ngx_buf_t 类型的对象，并返回指向这个对象的指针，创建失败返回 NULL。 对于创建的这个对象，它的 start 和 end 指向新分配内存开始和结束的地方。pos 和 last 都指向这块新分配内存的开始处，这样，后续的操作可以在这块新分配的内存上存入数据。 pool: 分配该 buf 和 buf 使用的内存所使用的 pool。 size: 该 buf 使用的内存的大小。 为了配合对 ngx_buf_t 的使用，Nginx 定义了以下的宏方便操作。 1#define ngx_buf_in_memory(b) (b-&gt;temporary || b-&gt;memory || b-&gt;mmap) 返回这个 buf 里面的内容是否在内存里。 1#define ngx_buf_in_memory_only(b) (ngx_buf_in_memory(b) &amp;&amp; !b-&gt;in_file) 返回这个 buf 里面的内容是否仅仅在内存里，并且没有在文件里。 123#define ngx_buf_special(b) \ ((b-&gt;flush || b-&gt;last_buf || b-&gt;sync) \ &amp;&amp; !ngx_buf_in_memory(b) &amp;&amp; !b-&gt;in_file) 返回该 buf 是否是一个特殊的 buf，只含有特殊的标志和没有包含真正的数据。 123#define ngx_buf_sync_only(b) \ (b-&gt;sync \ &amp;&amp; !ngx_buf_in_memory(b) &amp;&amp; !b-&gt;in_file &amp;&amp; !b-&gt;flush &amp;&amp; !b-&gt;last_buf) 返回该 buf 是否是一个只包含 sync 标志而不包含真正数据的特殊 buf。 123#define ngx_buf_size(b) \ (ngx_buf_in_memory(b) ? (off_t) (b-&gt;last - b-&gt;pos): \ (b-&gt;file_last - b-&gt;file_pos)) 返回该 buf 所含数据的大小，不管这个数据是在文件里还是在内存里。 ngx_list_tngx_list_t 顾名思义，看起来好像是一个 list 的数据结构。这样的说法，算对也不算对。因为它符合 list 类型数据结构的一些特点，比如可以添加元素，实现自增长，不会像数组类型的数据结构，受到初始设定的数组容量的限制，并且它跟我们常见的 list 型数据结构也是一样的，内部实现使用了一个链表。 那么它跟我们常见的链表实现的 list 有什么不同呢？不同点就在于它的节点，它的节点不像我们常见的 list 的节点，只能存放一个元素，ngx_list_t 的节点实际上是一个固定大小的数组。 在初始化的时候，我们需要设定元素需要占用的空间大小，每个节点数组的容量大小。在添加元素到这个 list 里面的时候，会在最尾部的节点里的数组上添加元素，如果这个节点的数组存满了，就再增加一个新的节点到这个 list 里面去。 好了，看到这里，大家应该基本上明白这个 list 结构了吧？还不明白也没有关系，下面我们来具体看一下它的定义，这些定义和相关的操作函数定义在src/core/ngx_list.h|c文件中。 1234567typedef struct &#123; ngx_list_part_t *last; ngx_list_part_t part; size_t size; ngx_uint_t nalloc; ngx_pool_t *pool;&#125; ngx_list_t; last: 指向该链表的最后一个节点。 part: 该链表的首个存放具体元素的节点。 size: 链表中存放的具体元素所需内存大小。 nalloc: 每个节点所含的固定大小的数组的容量。 pool: 该 list 使用的分配内存的 pool。 好，我们在看一下每个节点的定义。 123456typedef struct ngx_list_part_s ngx_list_part_t;struct ngx_list_part_s &#123; void *elts; ngx_uint_t nelts; ngx_list_part_t *next;&#125;; elts: 节点中存放具体元素的内存的开始地址。 nelts: 节点中已有元素个数。这个值是不能大于链表头节点 ngx_list_t 类型中的 nalloc 字段的。 next: 指向下一个节点。 我们来看一下提供的一个操作的函数。 1ngx_list_t *ngx_list_create(ngx_pool_t *pool, ngx_uint_t n, size_t size); 该函数创建一个 ngx_list_t 类型的对象，并对该 list 的第一个节点分配存放元素的内存空间。 pool: 分配内存使用的 pool。 n: 每个节点（ngx_list_part_t）固定长度的数组的长度，即最多可以存放的元素个数。 size: 每个元素所占用的内存大小。 返回值: 成功返回指向创建的 ngx_list_t 对象的指针，失败返回 NULL。 1void *ngx_list_push(ngx_list_t *list); 该函数在给定的 list 的尾部追加一个元素，并返回指向新元素存放空间的指针。如果追加失败，则返回 NULL。 12static ngx_inline ngx_int_tngx_list_init(ngx_list_t *list, ngx_pool_t *pool, ngx_uint_t n, size_t size); 该函数是用于 ngx_list_t 类型的对象已经存在，但是其第一个节点存放元素的内存空间还未分配的情况下，可以调用此函数来给这个 list 的首节点来分配存放元素的内存空间。 那么什么时候会出现已经有了 ngx_list_t 类型的对象，而其首节点存放元素的内存尚未分配的情况呢？那就是这个 ngx_list_t 类型的变量并不是通过调用 ngx_list_create 函数创建的。例如：如果某个结构体的一个成员变量是 ngx_list_t 类型的，那么当这个结构体类型的对象被创建出来的时候，这个成员变量也被创建出来了，但是它的首节点的存放元素的内存并未被分配。 总之，如果这个 ngx_list_t 类型的变量，如果不是你通过调用函数 ngx_list_create 创建的，那么就必须调用此函数去初始化，否则，你往这个 list 里追加元素就可能引发不可预知的行为，亦或程序会崩溃! ngx_queue_tngx_queue_t 是 Nginx 中的双向链表，在 Nginx 源码目录src/core下面的ngx_queue.h|c里面。它的原型如下： 123456typedef struct ngx_queue_s ngx_queue_t;struct ngx_queue_s &#123; ngx_queue_t *prev; ngx_queue_t *next;&#125;; 不同于教科书中将链表节点的数据成员声明在链表节点的结构体中，ngx_queue_t 只是声明了前向和后向指针。在使用的时候，我们首先需要定义一个哨兵节点(对于后续具体存放数据的节点，我们称之为数据节点)，比如： 1ngx_queue_t free; 接下来需要进行初始化，通过宏 ngx_queue_init()来实现： 1ngx_queue_init(&amp;free); ngx_queue_init()的宏定义如下： 123#define ngx_queue_init(q) \ (q)-&gt;prev = q; \ (q)-&gt;next = q 可见初始的时候哨兵节点的 prev 和 next 都指向自己，因此其实是一个空链表。ngx_queue_empty()可以用来判断一个链表是否为空，其实现也很简单，就是： 12#define ngx_queue_empty(h) \ (h == (h)-&gt;prev) 那么如何声明一个具有数据元素的链表节点呢？只要在相应的结构体中加上一个 ngx_queue_t 的成员就行了。比如 ngx_http_upstream_keepalive_module 中的 ngx_http_upstream_keepalive_cache_t： 123456789typedef struct &#123; ngx_http_upstream_keepalive_srv_conf_t *conf; ngx_queue_t queue; ngx_connection_t *connection; socklen_t socklen; u_char sockaddr[NGX_SOCKADDRLEN];&#125; ngx_http_upstream_keepalive_cache_t; 对于每一个这样的数据节点，可以通过 ngx_queue_insert_head()来添加到链表中，第一个参数是哨兵节点，第二个参数是数据节点，比如： 12ngx_http_upstream_keepalive_cache_t cache;ngx_queue_insert_head(&amp;free, &amp;cache.queue); 相应的几个宏定义如下： 12345678910111213#define ngx_queue_insert_head(h, x) \ (x)-&gt;next = (h)-&gt;next; \ (x)-&gt;next-&gt;prev = x; \ (x)-&gt;prev = h; \ (h)-&gt;next = x#define ngx_queue_insert_after ngx_queue_insert_head#define ngx_queue_insert_tail(h, x) \ (x)-&gt;prev = (h)-&gt;prev; \ (x)-&gt;prev-&gt;next = x; \ (x)-&gt;next = h; \ (h)-&gt;prev = x ngx_queue_insert_head() 和 ngx_queue_insert_after() 都是往头部添加节点，ngx_queue_insert_tail() 是往尾部添加节点。从代码可以看出哨兵节点的 prev 指向链表的尾数据节点，next 指向链表的头数据节点。另外 ngx_queue_head() 和 ngx_queue_last() 这两个宏分别可以得到头节点和尾节点。 那假如现在有一个 ngx_queue_t *q 指向的是链表中的数据节点的 queue 成员，如何得到ngx_http_upstream_keepalive_cache_t 的数据呢？ Nginx 提供了 ngx_queue_data() 宏来得到ngx_http_upstream_keepalive_cache_t 的指针，例如： 123ngx_http_upstream_keepalive_cache_t *cache = ngx_queue_data(q, ngx_http_upstream_keepalive_cache_t, queue); 也许您已经可以猜到 ngx_queue_data 是通过地址相减来得到的： 12#define ngx_queue_data(q, type, link) \ (type *) ((u_char *) q - offsetof(type, link)) 另外 Nginx 也提供了 ngx_queue_remove()宏来从链表中删除一个数据节点，以及 ngx_queue_add() 用来将一个链表添加到另一个链表。 Nginx 的配置系统由 小路依依 创建， 最后一次修改 2016-08-12 Nginx 的配置系统Nginx 的配置系统由一个主配置文件和其他一些辅助的配置文件构成。这些配置文件均是纯文本文件，全部位于Nginx 安装目录下的 conf 目录下。 配置文件中以#开始的行，或者是前面有若干空格或者 TAB，然后再跟#的行，都被认为是注释，也就是只对编辑查看文件的用户有意义，程序在读取这些注释行的时候，其实际的内容是被忽略的。 由于除主配置文件 nginx.conf 以外的文件都是在某些情况下才使用的，而只有主配置文件是在任何情况下都被使用的。所以在这里我们就以主配置文件为例，来解释 Nginx 的配置系统。 在 nginx.conf 中，包含若干配置项。每个配置项由配置指令和指令参数 2 个部分构成。指令参数也就是配置指令对应的配置值。 指令概述配置指令是一个字符串，可以用单引号或者双引号括起来，也可以不括。但是如果配置指令包含空格，一定要引起来。 指令参数指令的参数使用一个或者多个空格或者 TAB 字符与指令分开。指令的参数有一个或者多个 TOKEN 串组成。TOKEN 串之间由空格或者 TAB 键分隔。 TOKEN 串分为简单字符串或者是复合配置块。复合配置块即是由大括号括起来的一堆内容。一个复合配置块中可能包含若干其他的配置指令。 如果一个配置指令的参数全部由简单字符串构成，也就是不包含复合配置块，那么我们就说这个配置指令是一个简单配置项，否则称之为复杂配置项。例如下面这个是一个简单配置项： 1error_page 500 502 503 504 /50x.html; 对于简单配置，配置项的结尾使用分号结束。对于复杂配置项，包含多个 TOKEN 串的，一般都是简单 TOKEN 串放在前面，复合配置块一般位于最后，而且其结尾，并不需要再添加分号。例如下面这个复杂配置项： 1234location / &#123; root /home/jizhao/nginx-book/build/html; index index.html index.htm;&#125; 指令上下文nginx.conf 中的配置信息，根据其逻辑上的意义，对它们进行了分类，也就是分成了多个作用域，或者称之为配置指令上下文。不同的作用域含有一个或者多个配置项。 当前 Nginx 支持的几个指令上下文： main: Nginx 在运行时与具体业务功能（比如http服务或者email服务代理）无关的一些参数，比如工作进程数，运行的身份等。 http: 与提供 http 服务相关的一些配置参数。例如：是否使用 keepalive 啊，是否使用gzip进行压缩等。 server: http 服务上支持若干虚拟主机。每个虚拟主机一个对应的 server 配置项，配置项里面包含该虚拟主机相关的配置。在提供 mail 服务的代理时，也可以建立若干 server，每个 server 通过监听的地址来区分。 location: http 服务中，某些特定的URL对应的一系列配置项。 mail: 实现 email 相关的 SMTP/IMAP/POP3 代理时，共享的一些配置项（因为可能实现多个代理，工作在多个监听地址上）。 指令上下文，可能有包含的情况出现。例如：通常 http 上下文和 mail 上下文一定是出现在 main 上下文里的。在一个上下文里，可能包含另外一种类型的上下文多次。例如：如果 http 服务，支持了多个虚拟主机，那么在 http 上下文里，就会出现多个 server 上下文。 我们来看一个示例配置： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748user nobody;worker_processes 1;error_log logs/error.log info;events &#123; worker_connections 1024;&#125;http &#123; server &#123; listen 80; server_name www.linuxidc.com; access_log logs/linuxidc.access.log main; location / &#123; index index.html; root /var/www/linuxidc.com/htdocs; &#125; &#125; server &#123; listen 80; server_name www.Androidj.com; access_log logs/androidj.access.log main; location / &#123; index index.html; root /var/www/androidj.com/htdocs; &#125; &#125; &#125;mail &#123; auth_http 127.0.0.1:80/auth.php; pop3_capabilities "TOP" "USER"; imap_capabilities "IMAP4rev1" "UIDPLUS"; server &#123; listen 110; protocol pop3; proxy on; &#125; server &#123; listen 25; protocol smtp; proxy on; smtp_auth login plain; xclient off; &#125;&#125; 在这个配置中，上面提到个五种配置指令上下文都存在。 存在于 main 上下文中的配置指令如下: user worker_processes error_log events http mail 存在于 http 上下文中的指令如下: server 存在于 mail 上下文中的指令如下： server auth_http imap_capabilities 存在于 server 上下文中的配置指令如下： listen server_name access_log location protocol proxy smtp_auth xclient 存在于 location 上下文中的指令如下： index root 当然，这里只是一些示例。具体有哪些配置指令，以及这些配置指令可以出现在什么样的上下文中，需要参考 Nginx 的使用文档 Nginx 的模块化体系结构由 小路依依 创建， 最后一次修改 2016-08-12 Nginx 的模块化体系结构Nginx 的内部结构是由核心部分和一系列的功能模块所组成。这样划分是为了使得每个模块的功能相对简单，便于开发，同时也便于对系统进行功能扩展。为了便于描述，下文中我们将使用 Nginx core 来称呼 Nginx 的核心功能部分。 Nginx 提供了 Web 服务器的基础功能，同时提供了 Web 服务反向代理，Email 服务反向代理功能。Nginx core实现了底层的通讯协议，为其他模块和 Nginx 进程构建了基本的运行时环境，并且构建了其他各模块的协作基础。除此之外，或者说大部分与协议相关的，或者应用相关的功能都是在这些模块中所实现的。 模块概述Nginx 将各功能模块组织成一条链，当有请求到达的时候，请求依次经过这条链上的部分或者全部模块，进行处理。每个模块实现特定的功能。例如，实现对请求解压缩的模块，实现 SSI 的模块，实现与上游服务器进行通讯的模块，实现与 FastCGI 服务进行通讯的模块。 有两个模块比较特殊，他们居于 Nginx core 和各功能模块的中间。这两个模块就是 http 模块和 mail 模块。这 2 个模块在 Nginx core 之上实现了另外一层抽象，处理与 HTTP 协议和 Email 相关协议（SMTP/POP3/IMAP）有关的事件，并且确保这些事件能被以正确的顺序调用其他的一些功能模块。 目前 HTTP 协议是被实现在 http 模块中的，但是有可能将来被剥离到一个单独的模块中，以扩展 Nginx 支持 SPDY 协议。 模块的分类Nginx 的模块根据其功能基本上可以分为以下几种类型： event module: 搭建了独立于操作系统的事件处理机制的框架，及提供了各具体事件的处理。包括 ngx_events_module， ngx_event_core_module和ngx_epoll_module 等。Nginx 具体使用何种事件处理模块，这依赖于具体的操作系统和编译选项。 phase handler: 此类型的模块也被直接称为 handler 模块。主要负责处理客户端请求并产生待响应内容，比如 ngx_http_static_module 模块，负责客户端的静态页面请求处理并将对应的磁盘文件准备为响应内容输出。 output filter: 也称为 filter 模块，主要是负责对输出的内容进行处理，可以对输出进行修改。例如，可以实现对输出的所有 html 页面增加预定义的 footbar 一类的工作，或者对输出的图片的 URL 进行替换之类的工作。 upstream: upstream 模块实现反向代理的功能，将真正的请求转发到后端服务器上，并从后端服务器上读取响应，发回客户端。upstream 模块是一种特殊的 handler，只不过响应内容不是真正由自己产生的，而是从后端服务器上读取的。 load-balancer: 负载均衡模块，实现特定的算法，在众多的后端服务器中，选择一个服务器出来作为某个请求的转发服务器。 Nginx 的请求处理由 小路依依 创建， 最后一次修改 2016-08-12 Nginx 的请求处理Nginx 使用一个多进程模型来对外提供服务，其中一个 master 进程，多个 worker 进程。master 进程负责管理 Nginx 本身和其他 worker 进程。 所有实际上的业务处理逻辑都在 worker 进程。worker 进程中有一个函数，执行无限循环，不断处理收到的来自客户端的请求，并进行处理，直到整个 Nginx 服务被停止。 worker 进程中，ngx_worker_process_cycle()函数就是这个无限循环的处理函数。在这个函数中，一个请求的简单处理流程如下： 操作系统提供的机制（例如 epoll, kqueue 等）产生相关的事件。 接收和处理这些事件，如是接受到数据，则产生更高层的 request 对象。 处理 request 的 header 和 body。 产生响应，并发送回客户端。 完成 request 的处理。 重新初始化定时器及其他事件。 请求的处理流程为了让大家更好的了解 Nginx 中请求处理过程，我们以 HTTP Request 为例，来做一下详细地说明。 从 Nginx 的内部来看，一个 HTTP Request 的处理过程涉及到以下几个阶段。 初始化 HTTP Request（读取来自客户端的数据，生成 HTTP Request 对象，该对象含有该请求所有的信息）。 处理请求头。 处理请求体。 如果有的话，调用与此请求（URL 或者 Location）关联的 handler。 依次调用各 phase handler 进行处理。 在这里，我们需要了解一下 phase handler 这个概念。phase 字面的意思，就是阶段。所以 phase handlers 也就好理解了，就是包含若干个处理阶段的一些 handler。 在每一个阶段，包含有若干个 handler，再处理到某个阶段的时候，依次调用该阶段的 handler 对 HTTP Request 进行处理。 通常情况下，一个 phase handler 对这个 request 进行处理，并产生一些输出。通常 phase handler 是与定义在配置文件中的某个 location 相关联的。 一个 phase handler 通常执行以下几项任务： 获取 location 配置。 产生适当的响应。 发送 response header。 发送 response body。 当 Nginx 读取到一个 HTTP Request 的 header 的时候，Nginx 首先查找与这个请求关联的虚拟主机的配置。如果找到了这个虚拟主机的配置，那么通常情况下，这个 HTTP Request 将会经过以下几个阶段的处理（phase handlers）： NGX_HTTP_POST_READ_PHASE: 读取请求内容阶段 NGX_HTTP_SERVER_REWRITE_PHASE: Server 请求地址重写阶段 NGX_HTTP_FIND_CONFIG_PHASE: 配置查找阶段: NGX_HTTP_REWRITE_PHASE: Location请求地址重写阶段 NGX_HTTP_POST_REWRITE_PHASE: 请求地址重写提交阶段 NGX_HTTP_PREACCESS_PHASE: 访问权限检查准备阶段 NGX_HTTP_ACCESS_PHASE: 访问权限检查阶段 NGX_HTTP_POST_ACCESS_PHASE: 访问权限检查提交阶段 NGX_HTTP_TRY_FILES_PHASE: 配置项 try_files 处理阶段 NGX_HTTP_CONTENT_PHASE: 内容产生阶段 NGX_HTTP_LOG_PHASE: 日志模块处理阶段 在内容产生阶段，为了给一个 request 产生正确的响应，Nginx 必须把这个 request 交给一个合适的 content handler 去处理。如果这个 request 对应的 location 在配置文件中被明确指定了一个 content handler，那么Nginx 就可以通过对 location 的匹配，直接找到这个对应的 handler，并把这个 request 交给这个 content handler 去处理。这样的配置指令包括像，perl，flv，proxy_pass，mp4等。 如果一个 request 对应的 location 并没有直接有配置的 content handler，那么 Nginx 依次尝试: 如果一个 location 里面有配置 random_index on，那么随机选择一个文件，发送给客户端。 如果一个 location 里面有配置 index 指令，那么发送 index 指令指明的文件，给客户端。 如果一个 location 里面有配置 autoindex on，那么就发送请求地址对应的服务端路径下的文件列表给客户端。 如果这个 request 对应的 location 上有设置 gzip_static on，那么就查找是否有对应的.gz文件存在，有的话，就发送这个给客户端（客户端支持 gzip 的情况下）。 请求的 URI 如果对应一个静态文件，static module 就发送静态文件的内容到客户端。 内容产生阶段完成以后，生成的输出会被传递到 filter 模块去进行处理。filter 模块也是与 location 相关的。所有的 fiter 模块都被组织成一条链。输出会依次穿越所有的 filter，直到有一个 filter 模块的返回值表明已经处理完成。 这里列举几个常见的 filter 模块，例如： server-side includes。 XSLT filtering。 图像缩放之类的。 gzip 压缩。 在所有的 filter 中，有几个 filter 模块需要关注一下。按照调用的顺序依次说明如下： write: 写输出到客户端，实际上是写到连接对应的 socket 上。 postpone: 这个 filter 是负责 subrequest 的，也就是子请求的。 copy: 将一些需要复制的 buf(文件或者内存)重新复制一份然后交给剩余的 body filter 处理。 Nginx handler 模块简介由 小路依依 创建， 最后一次修改 2016-08-12 handler 模块简介相信大家在看了前一章的模块概述以后，都对 Nginx 的模块有了一个基本的认识。基本上作为第三方开发者最可能开发的就是三种类型的模块，即 handler，filter 和 load-balancer。Handler 模块就是接受来自客户端的请求并产生输出的模块。有些地方说 upstream 模块实际上也是一种 handler 模块，只不过它产生的内容来自于从后端服务器获取的，而非在本机产生的。 在上一章提到，配置文件中使用 location 指令可以配置 content handler 模块，当 Nginx 系统启动的时候，每个 handler 模块都有一次机会把自己关联到对应的 location上。如果有多个 handler 模块都关联了同一个 location，那么实际上只有一个 handler 模块真正会起作用。当然大多数情况下，模块开发人员都会避免出现这种情况。 handler 模块处理的结果通常有三种情况: 处理成功，处理失败（处理的时候发生了错误）或者是拒绝去处理。在拒绝处理的情况下，这个 location 的处理就会由默认的 handler 模块来进行处理。例如，当请求一个静态文件的时候，如果关联到这个 location 上的一个 handler 模块拒绝处理，就会由默认的 ngx_http_static_module 模块进行处理，该模块是一个典型的 handler 模块。 本章主要讲述的是如何编写 handler 模块，在研究 handler 模块编写之前先来了解一下模块的一些基本数据结构。 Nginx 模块的基本结构由 小路依依 创建， 最后一次修改 2016-08-12 模块的基本结构在这一节我们将会对通常的模块开发过程中，每个模块所包含的一些常用的部分进行说明。这些部分有些是必须的，有些不是必须的。同时这里所列出的这些东西对于其他类型的模块，例如 filter 模块等也都是相同的。 模块配置结构基本上每个模块都会提供一些配置指令，以便于用户可以通过配置来控制该模块的行为。那么这些配置信息怎么存储呢？那就需要定义该模块的配置结构来进行存储。 大家都知道 Nginx 的配置信息分成了几个作用域(scope,有时也称作上下文)，这就是 main，server 以及 location。同样的每个模块提供的配置指令也可以出现在这几个作用域里。那对于这三个作用域的配置信息，每个模块就需要定义三个不同的数据结构去进行存储。当然，不是每个模块都会在这三个作用域都提供配置指令的。那么也就不一定每个模块都需要定义三个数据结构去存储这些配置信息了。视模块的实现而言，需要几个就定义几个。 有一点需要特别注意的就是，在模块的开发过程中，我们最好使用 Nginx 原有的命名习惯。这样跟原代码的契合度更高，看起来也更舒服。 对于模块配置信息的定义，命名习惯是ngx_http_&lt;module name&gt;_(main|srv|loc)_conf_t。这里有个例子，就是从我们后面将要展示给大家的 hello module 中截取的。 12345typedef struct&#123; ngx_str_t hello_string; ngx_int_t hello_counter;&#125;ngx_http_hello_loc_conf_t; 模块配置指令一个模块的配置指令是定义在一个静态数组中的。同样地，我们来看一下从 hello module 中截取的模块配置指令的定义。 12345678910111213141516171819static ngx_command_t ngx_http_hello_commands[] = &#123; &#123; ngx_string("hello_string"), NGX_HTTP_LOC_CONF|NGX_CONF_NOARGS|NGX_CONF_TAKE1, ngx_http_hello_string, NGX_HTTP_LOC_CONF_OFFSET, offsetof(ngx_http_hello_loc_conf_t, hello_string), NULL &#125;, &#123; ngx_string("hello_counter"), NGX_HTTP_LOC_CONF|NGX_CONF_FLAG, ngx_http_hello_counter, NGX_HTTP_LOC_CONF_OFFSET, offsetof(ngx_http_hello_loc_conf_t, hello_counter), NULL &#125;, ngx_null_command&#125;; 其实看这个定义，就基本能看出来一些信息。例如，我们是定义了两个配置指令，一个是叫 hello_string，可以接受一个参数，或者是没有参数。另外一个命令是 hello_counter，接受一个 NGX_CONF_FLAG 类型的参数。除此之外，似乎看起来有点迷惑。没有关系，我们来详细看一下 ngx_command_t，一旦我们了解这个结构的详细信息，那么我相信上述这个定义所表达的所有信息就不言自明了。 ngx_command_t 的定义，位于src/core/ngx_conf_file.h中。 12345678struct ngx_command_s &#123; ngx_str_t name; ngx_uint_t type; char *(*set)(ngx_conf_t *cf, ngx_command_t *cmd, void *conf); ngx_uint_t conf; ngx_uint_t offset; void *post;&#125;; name: 配置指令的名称。 type: 该配置的类型，其实更准确一点说，是该配置指令属性的集合。Nginx 提供了很多预定义的属性值（一些宏定义），通过逻辑或运算符可组合在一起，形成对这个配置指令的详细的说明。下面列出可在这里使用的预定义属性值及说明。 NGX_CONF_NOARGS：配置指令不接受任何参数。 NGX_CONF_TAKE1：配置指令接受 1 个参数。 NGX_CONF_TAKE2：配置指令接受 2 个参数。 NGX_CONF_TAKE3：配置指令接受 3 个参数。 NGX_CONF_TAKE4：配置指令接受 4 个参数。 NGX_CONF_TAKE5：配置指令接受 5 个参数。 NGX_CONF_TAKE6：配置指令接受 6 个参数。 NGX_CONF_TAKE7：配置指令接受 7 个参数。 可以组合多个属性，比如一个指令即可以不填参数，也可以接受1个或者2个参数。那么就是NGX_CONF_NOARGS|NGX_CONF_TAKE1|NGX_CONF_TAKE2。如果写上面三个属性在一起，你觉得麻烦，那么没有关系，Nginx 提供了一些定义，使用起来更简洁。 NGX_CONF_TAKE12：配置指令接受 1 个或者 2 个参数。 NGX_CONF_TAKE13：配置指令接受 1 个或者 3 个参数。 NGX_CONF_TAKE23：配置指令接受 2 个或者 3 个参数。 NGX_CONF_TAKE123：配置指令接受 1 个或者 2 个或者 3 参数。 NGX_CONF_TAKE1234：配置指令接受 1 个或者 2 个或者 3 个或者 4 个参数。 NGX_CONF_1MORE：配置指令接受至少一个参数。 NGX_CONF_2MORE：配置指令接受至少两个参数。 NGX_CONF_MULTI: 配置指令可以接受多个参数，即个数不定。 NGX_CONF_BLOCK：配置指令可以接受的值是一个配置信息块。也就是一对大括号括起来的内容。里面可以再包括很多的配置指令。比如常见的 server 指令就是这个属性的。 NGX_CONF_FLAG：配置指令可以接受的值是”on”或者”off”，最终会被转成 bool 值。 NGX_CONF_ANY：配置指令可以接受的任意的参数值。一个或者多个，或者”on”或者”off”，或者是配置块。 最后要说明的是，无论如何，Nginx 的配置指令的参数个数不可以超过 NGX_CONF_MAX_ARGS 个。目前这个值被定义为 8，也就是不能超过 8 个参数值。 下面介绍一组说明配置指令可以出现的位置的属性。 NGX_DIRECT_CONF：可以出现在配置文件中最外层。例如已经提供的配置指令 daemon，master_process 等。 NGX_MAIN_CONF: http、mail、events、error_log 等。 NGX_ANY_CONF: 该配置指令可以出现在任意配置级别上。 对于我们编写的大多数模块而言，都是在处理http相关的事情，也就是所谓的都是NGX_HTTP_MODULE，对于这样类型的模块，其配置可能出现的位置也是分为直接出现在http里面，以及其他位置。 NGX_HTTP_MAIN_CONF: 可以直接出现在 http 配置指令里。 NGX_HTTP_SRV_CONF: 可以出现在 http 里面的 server 配置指令里。 NGX_HTTP_LOC_CONF: 可以出现在 http server 块里面的 location 配置指令里。 NGX_HTTP_UPS_CONF: 可以出现在 http 里面的 upstream 配置指令里。 NGX_HTTP_SIF_CONF: 可以出现在 http 里面的 server 配置指令里的 if 语句所在的 block 中。 NGX_HTTP_LMT_CONF: 可以出现在 http 里面的 limit_except 指令的 block 中。 NGX_HTTP_LIF_CONF: 可以出现在 http server 块里面的 location 配置指令里的 if 语句所在的 block 中。 set: 这是一个函数指针，当 Nginx 在解析配置的时候，如果遇到这个配置指令，将会把读取到的值传递给这个函数进行分解处理。因为具体每个配置指令的值如何处理，只有定义这个配置指令的人是最清楚的。来看一下这个函数指针要求的函数原型。 1char *(*set)(ngx_conf_t *cf, ngx_command_t *cmd, void *conf); 先看该函数的返回值，处理成功时，返回 NGX_OK，否则返回 NGX_CONF_ERROR 或者是一个自定义的错误信息的字符串。 再看一下这个函数被调用的时候，传入的三个参数。 cf: 该参数里面保存从配置文件读取到的原始字符串以及相关的一些信息。特别注意的是这个参数的args字段是一个 ngx_str_t类型的数组，该数组的首个元素是这个配置指令本身，第二个元素是指令的第一个参数，第三个元素是第二个参数，依次类推。 cmd: 这个配置指令对应的 ngx_command_t 结构。 conf: 就是定义的存储这个配置值的结构体，比如在上面展示的那个 ngx_http_hello_loc_conf_t。当解析这个 hello_string 变量的时候，传入的 conf 就指向一个 ngx_http_hello_loc_conf_t 类型的变量。用户在处理的时候可以使用类型转换，转换成自己知道的类型，再进行字段的赋值。 为了更加方便的实现对配置指令参数的读取，Nginx 已经默认提供了对一些标准类型的参数进行读取的函数，可以直接赋值给 set 字段使用。下面来看一下这些已经实现的 set 类型函数。 ngx_conf_set_flag_slot： 读取 NGX_CONF_FLAG 类型的参数。 ngx_conf_set_str_slot:读取字符串类型的参数。 ngx_conf_set_str_array_slot: 读取字符串数组类型的参数。 ngx_conf_set_keyval_slot： 读取键值对类型的参数。 ngx_conf_set_num_slot: 读取整数类型(有符号整数 ngx_int_t)的参数。 ngx_conf_set_size_slot:读取 size_t 类型的参数，也就是无符号数。 ngx_conf_set_off_slot: 读取 off_t 类型的参数。 ngx_conf_set_msec_slot: 读取毫秒值类型的参数。 ngx_conf_set_sec_slot: 读取秒值类型的参数。 ngx_conf_set_bufs_slot： 读取的参数值是 2 个，一个是 buf 的个数，一个是 buf 的大小。例如： output_buffers 1 128k; ngx_conf_set_enum_slot: 读取枚举类型的参数，将其转换成整数 ngx_uint_t 类型。 ngx_conf_set_bitmask_slot: 读取参数的值，并将这些参数的值以 bit 位的形式存储。例如：HttpDavModule 模块的 dav_methods 指令。 conf: 该字段被 NGX_HTTP_MODULE 类型模块所用 (我们编写的基本上都是 NGX_HTTP_MOUDLE，只有一些 Nginx 核心模块是非 NGX_HTTP_MODULE)，该字段指定当前配置项存储的内存位置。实际上是使用哪个内存池的问题。因为 http 模块对所有 http 模块所要保存的配置信息，划分了 main, server 和 location 三个地方进行存储，每个地方都有一个内存池用来分配存储这些信息的内存。这里可能的值为 NGX_HTTP_MAIN_CONF_OFFSET、NGX_HTTP_SRV_CONF_OFFSET 或 NGX_HTTP_LOC_CONF_OFFSET。当然也可以直接置为 0，就是 NGX_HTTP_MAIN_CONF_OFFSET。 offset: 指定该配置项值的精确存放位置，一般指定为某一个结构体变量的字段偏移。因为对于配置信息的存储，一般我们都是定义个结构体来存储的。那么比如我们定义了一个结构体 A，该项配置的值需要存储到该结构体的 b 字段。那么在这里就可以填写为 offsetof(A, b)。对于有些配置项，它的值不需要保存或者是需要保存到更为复杂的结构中时，这里可以设置为 0。 post: 该字段存储一个指针。可以指向任何一个在读取配置过程中需要的数据，以便于进行配置读取的处理。大多数时候，都不需要，所以简单地设为 0 即可。 看到这里，应该就比较清楚了。ngx_http_hello_commands 这个数组每 5 个元素为一组，用来描述一个配置项的所有情况。那么如果有多个配置项，只要按照需要再增加 5 个对应的元素对新的配置项进行说明。 需要注意的是，就是在ngx_http_hello_commands这个数组定义的最后，都要加一个ngx_null_command作为结尾。 模块上下文结构这是一个 ngx_http_module_t 类型的静态变量。这个变量实际上是提供一组回调函数指针，这些函数有在创建存储配置信息的对象的函数，也有在创建前和创建后会调用的函数。这些函数都将被 Nginx 在合适的时间进行调用。 12345678910111213typedef struct &#123; ngx_int_t (*preconfiguration)(ngx_conf_t *cf); ngx_int_t (*postconfiguration)(ngx_conf_t *cf); void *(*create_main_conf)(ngx_conf_t *cf); char *(*init_main_conf)(ngx_conf_t *cf, void *conf); void *(*create_srv_conf)(ngx_conf_t *cf); char *(*merge_srv_conf)(ngx_conf_t *cf, void *prev, void *conf); void *(*create_loc_conf)(ngx_conf_t *cf); char *(*merge_loc_conf)(ngx_conf_t *cf, void *prev, void *conf);&#125; ngx_http_module_t; preconfiguration: 在创建和读取该模块的配置信息之前被调用。 postconfiguration: 在创建和读取该模块的配置信息之后被调用。 create_main_conf: 调用该函数创建本模块位于 http block 的配置信息存储结构。该函数成功的时候，返回创建的配置对象。失败的话，返回 NULL。 init_main_conf: 调用该函数初始化本模块位于 http block 的配置信息存储结构。该函数成功的时候，返回 NGX_CONF_OK。失败的话，返回 NGX_CONF_ERROR 或错误字符串。 create_srv_conf: 调用该函数创建本模块位于 http server block 的配置信息存储结构，每个 server block 会创建一个。该函数成功的时候，返回创建的配置对象。失败的话，返回 NULL。 merge_srv_conf: 因为有些配置指令既可以出现在 http block，也可以出现在 http server block 中。那么遇到这种情况，每个 server 都会有自己存储结构来存储该 server 的配置，但是在这种情况下 http block 中的配置与 server block 中的配置信息发生冲突的时候，就需要调用此函数进行合并，该函数并非必须提供，当预计到绝对不会发生需要合并的情况的时候，就无需提供。当然为了安全起见还是建议提供。该函数执行成功的时候，返回 NGX_CONF_OK。失败的话，返回 NGX_CONF_ERROR 或错误字符串。 create_loc_conf: 调用该函数创建本模块位于 location block 的配置信息存储结构。每个在配置中指明的 location 创建一个。该函数执行成功，返回创建的配置对象。失败的话，返回 NULL。 merge_loc_conf: 与 merge_srv_conf 类似，这个也是进行配置值合并的地方。该函数成功的时候，返回 NGX_CONF_OK。失败的话，返回 NGX_CONF_ERROR 或错误字符串。 Nginx 里面的配置信息都是上下一层层的嵌套的，对于具体某个 location 的话，对于同一个配置，如果当前层次没有定义，那么就使用上层的配置，否则使用当前层次的配置。 这些配置信息一般默认都应该设为一个未初始化的值，针对这个需求，Nginx 定义了一系列的宏定义来代表各种配置所对应数据类型的未初始化值，如下： 12345#define NGX_CONF_UNSET -1#define NGX_CONF_UNSET_UINT (ngx_uint_t) -1#define NGX_CONF_UNSET_PTR (void *) -1#define NGX_CONF_UNSET_SIZE (size_t) -1#define NGX_CONF_UNSET_MSEC (ngx_msec_t) -1 又因为对于配置项的合并，逻辑都类似，也就是前面已经说过的，如果在本层次已经配置了，也就是配置项的值已经被读取进来了（那么这些配置项的值就不会等于上面已经定义的那些 UNSET 的值），就使用本层次的值作为定义合并的结果，否则，使用上层的值，如果上层的值也是这些UNSET类的值，那就赋值为默认值，否则就使用上层的值作为合并的结果。对于这样类似的操作，Nginx 定义了一些宏操作来做这些事情，我们来看其中一个的定义。 1234#define ngx_conf_merge_uint_value(conf, prev, default) \ if (conf == NGX_CONF_UNSET_UINT) &#123; \ conf = (prev == NGX_CONF_UNSET_UINT) ? default : prev; \ &#125; 显而易见，这个逻辑确实比较简单，所以其它的宏定义也类似，我们就列具其中的一部分吧。 12345ngx_conf_merge_valuengx_conf_merge_ptr_valuengx_conf_merge_uint_valuengx_conf_merge_msec_valuengx_conf_merge_sec_value 等等。 下面来看一下 hello 模块的模块上下文的定义，加深一下印象。 12345678910111213static ngx_http_module_t ngx_http_hello_module_ctx = &#123; NULL, /* preconfiguration */ ngx_http_hello_init, /* postconfiguration */ NULL, /* create main configuration */ NULL, /* init main configuration */ NULL, /* create server configuration */ NULL, /* merge server configuration */ ngx_http_hello_create_loc_conf, /* create location configuration */ NULL /* merge location configuration */&#125;; 注意：这里并没有提供 merge_loc_conf 函数，因为我们这个模块的配置指令已经确定只出现在 NGX_HTTP_LOC_CONF 中这一个层次上，不会发生需要合并的情况。 模块的定义对于开发一个模块来说，我们都需要定义一个 ngx_module_t 类型的变量来说明这个模块本身的信息，从某种意义上来说，这是这个模块最重要的一个信息，它告诉了 Nginx 这个模块的一些信息，上面定义的配置信息，还有模块上下文信息，都是通过这个结构来告诉 Nginx 系统的，也就是加载模块的上层代码，都需要通过定义的这个结构，来获取这些信息。 我们先来看下 ngx_module_t 的定义 12345678910111213141516171819202122232425262728293031323334typedef struct ngx_module_s ngx_module_t;struct ngx_module_s &#123; ngx_uint_t ctx_index; ngx_uint_t index; ngx_uint_t spare0; ngx_uint_t spare1; ngx_uint_t abi_compatibility; ngx_uint_t major_version; ngx_uint_t minor_version; void *ctx; ngx_command_t *commands; ngx_uint_t type; ngx_int_t (*init_master)(ngx_log_t *log); ngx_int_t (*init_module)(ngx_cycle_t *cycle); ngx_int_t (*init_process)(ngx_cycle_t *cycle); ngx_int_t (*init_thread)(ngx_cycle_t *cycle); void (*exit_thread)(ngx_cycle_t *cycle); void (*exit_process)(ngx_cycle_t *cycle); void (*exit_master)(ngx_cycle_t *cycle); uintptr_t spare_hook0; uintptr_t spare_hook1; uintptr_t spare_hook2; uintptr_t spare_hook3; uintptr_t spare_hook4; uintptr_t spare_hook5; uintptr_t spare_hook6; uintptr_t spare_hook7;&#125;;#define NGX_NUMBER_MAJOR 3#define NGX_NUMBER_MINOR 1#define NGX_MODULE_V1 0, 0, 0, 0, \ NGX_DSO_ABI_COMPATIBILITY, NGX_NUMBER_MAJOR, NGX_NUMBER_MINOR#define NGX_MODULE_V1_PADDING 0, 0, 0, 0, 0, 0, 0, 0 再看一下 hello 模块的模块定义。 1234567891011121314ngx_module_t ngx_http_hello_module = &#123; NGX_MODULE_V1, &amp;ngx_http_hello_module_ctx, /* module context */ ngx_http_hello_commands, /* module directives */ NGX_HTTP_MODULE, /* module type */ NULL, /* init master */ NULL, /* init module */ NULL, /* init process */ NULL, /* init thread */ NULL, /* exit thread */ NULL, /* exit process */ NULL, /* exit master */ NGX_MODULE_V1_PADDING&#125;; 模块可以提供一些回调函数给 Nginx，当 Nginx 在创建进程线程或者结束进程线程时进行调用。但大多数模块在这些时刻并不需要做什么，所以都简单赋值为 NULL。 Nginx handler 模块的基本结构由 小路依依 创建， 最后一次修改 2016-08-12 handler 模块的基本结构除了上一节介绍的模块的基本结构以外，handler 模块必须提供一个真正的处理函数，这个函数负责对来自客户端请求的真正处理。这个函数的处理，既可以选择自己直接生成内容，也可以选择拒绝处理，由后续的 handler 去进行处理，或者是选择丢给后续的 filter 进行处理。来看一下这个函数的原型申明。 1typedef ngx_int_t (*ngx_http_handler_pt)(ngx_http_request_t *r); r 是 http 请求。里面包含请求所有的信息，这里不详细说明了，可以参考别的章节的介绍。 该函数处理成功返回 NGX_OK，处理发生错误返回 NGX_ERROR，拒绝处理（留给后续的 handler 进行处理）返回 NGX_DECLINE。 返回 NGX_OK 也就代表给客户端的响应已经生成好了，否则返回 NGX_ERROR 就发生错误了。 Nginx handler 模块的挂载由 小路依依 创建， 最后一次修改 2016-08-12 handler 模块的挂载handler 模块真正的处理函数通过两种方式挂载到处理过程中，一种方式就是按处理阶段挂载;另外一种挂载方式就是按需挂载。 按处理阶段挂载为了更精细地控制对于客户端请求的处理过程，Nginx 把这个处理过程划分成了 11 个阶段。他们从前到后，依次列举如下： NGX_HTTP_POST_READ_PHASE: 读取请求内容阶段 NGX_HTTP_SERVER_REWRITE_PHASE: Server 请求地址重写阶段 NGX_HTTP_FIND_CONFIG_PHASE: 配置查找阶段: NGX_HTTP_REWRITE_PHASE: Location 请求地址重写阶段 NGX_HTTP_POST_REWRITE_PHASE: 请求地址重写提交阶段 NGX_HTTP_PREACCESS_PHASE: 访问权限检查准备阶段 NGX_HTTP_ACCESS_PHASE: 访问权限检查阶段 NGX_HTTP_POST_ACCESS_PHASE: 访问权限检查提交阶段 NGX_HTTP_TRY_FILES_PHASE: 配置项 try_files 处理阶段 NGX_HTTP_CONTENT_PHASE: 内容产生阶段 NGX_HTTP_LOG_PHASE: 日志模块处理阶段 一般情况下，我们自定义的模块，大多数是挂载在 NGX_HTTP_CONTENT_PHASE 阶段的。挂载的动作一般是在模块上下文调用的 postconfiguration 函数中。 注意：有几个阶段是特例，它不调用挂载地任何的handler，也就是你就不用挂载到这几个阶段了： NGX_HTTP_FIND_CONFIG_PHASE NGX_HTTP_POST_ACCESS_PHASE NGX_HTTP_POST_REWRITE_PHASE NGX_HTTP_TRY_FILES_PHASE 所以其实真正是有 7 个 phase 你可以去挂载 handler。 挂载的代码如下（摘自 hello module）: 1234567891011121314151617static ngx_int_tngx_http_hello_init(ngx_conf_t *cf)&#123; ngx_http_handler_pt *h; ngx_http_core_main_conf_t *cmcf; cmcf = ngx_http_conf_get_module_main_conf(cf, ngx_http_core_module); h = ngx_array_push(&amp;cmcf-&gt;phases[NGX_HTTP_CONTENT_PHASE].handlers); if (h == NULL) &#123; return NGX_ERROR; &#125; *h = ngx_http_hello_handler; return NGX_OK;&#125; 使用这种方式挂载的 handler 也被称为 content phase handlers。 按需挂载以这种方式挂载的 handler 也被称为 content handler。 当一个请求进来以后，Nginx 从 NGX_HTTP_POST_READ_PHASE 阶段开始依次执行每个阶段中所有 handler。执行到 NGX_HTTP_CONTENT_PHASE 阶段的时候，如果这个 location 有一个对应的 content handler 模块，那么就去执行这个 content handler 模块真正的处理函数。否则继续依次执行 NGX_HTTP_CONTENT_PHASE 阶段中所有 content phase handlers，直到某个函数处理返回 NGX_OK 或者 NGX_ERROR。 换句话说，当某个 location 处理到 NGX_HTTP_CONTENT_PHASE 阶段时，如果有 content handler 模块，那么 NGX_HTTP_CONTENT_PHASE 挂载的所有 content phase handlers 都不会被执行了。 但是使用这个方法挂载上去的 handler 有一个特点是必须在 NGX_HTTP_CONTENT_PHASE 阶段才能执行到。如果你想自己的 handler 在更早的阶段执行，那就不要使用这种挂载方式。 那么在什么情况会使用这种方式来挂载呢？一般情况下，某个模块对某个 location 进行了处理以后，发现符合自己处理的逻辑，而且也没有必要再调用 NGX_HTTP_CONTENT_PHASE 阶段的其它 handler 进行处理的时候，就动态挂载上这个 handler。 下面来看一下使用这种挂载方式的具体例子（摘自 Emiller’s Guide To Nginx Module Development）。 12345678910static char *ngx_http_circle_gif(ngx_conf_t *cf, ngx_command_t *cmd, void *conf)&#123; ngx_http_core_loc_conf_t *clcf; clcf = ngx_http_conf_get_module_loc_conf(cf, ngx_http_core_module); clcf-&gt;handler = ngx_http_circle_gif_handler; return NGX_CONF_OK;&#125; Nginx handler 的编写步骤由 小路依依 创建， 最后一次修改 2016-08-12 handler 的编写步骤好，到了这里，让我们稍微整理一下思路，回顾一下实现一个 handler 的步骤: 编写模块基本结构。包括模块的定义，模块上下文结构，模块的配置结构等。 实现 handler 的挂载函数。根据模块的需求选择正确的挂载方式。 编写 handler 处理函数。模块的功能主要通过这个函数来完成。 看起来不是那么难，对吧？还是那句老话，世上无难事，只怕有心人! 现在我们来完整的分析前面提到的 hello handler module 示例的功能和代码。 Nginx 示例: hello handler 模块由 小路依依 创建， 最后一次修改 2016-08-12 示例: hello handler 模块在前面已经看到了这个 hello handler module 的部分重要的结构。该模块提供了 2 个配置指令，仅可以出现在 location 指令的作用域中。这两个指令是 hello_string, 该指令接受一个参数来设置显示的字符串。如果没有跟参数，那么就使用默认的字符串作为响应字符串。 另一个指令是 hello_counter，如果设置为 on，则会在响应的字符串后面追加 Visited Times:的字样，以统计请求的次数。 这里有两点注意一下： 对于 flag 类型的配置指令，当值为 off 的时候，使用 ngx_conf_set_flag_slot 函数，会转化为 0，为on，则转化为非 0。 另外一个是，我提供了 merge_loc_conf 函数，但是却没有设置到模块的上下文定义中。这样有一个缺点，就是如果一个指令没有出现在配置文件中的时候，配置信息中的值，将永远会保持在 create_loc_conf 中的初始化的值。那如果，在类似 create_loc_conf 这样的函数中，对创建出来的配置信息的值，没有设置为合理的值的话，后面用户又没有配置，就会出现问题。 下面来完整的给出 ngx_http_hello_module 模块的完整代码。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236#include &lt;ngx_config.h&gt;#include &lt;ngx_core.h&gt;#include &lt;ngx_http.h&gt;typedef struct&#123; ngx_str_t hello_string; ngx_int_t hello_counter;&#125;ngx_http_hello_loc_conf_t;static ngx_int_t ngx_http_hello_init(ngx_conf_t *cf);static void *ngx_http_hello_create_loc_conf(ngx_conf_t *cf);static char *ngx_http_hello_string(ngx_conf_t *cf, ngx_command_t *cmd, void *conf);static char *ngx_http_hello_counter(ngx_conf_t *cf, ngx_command_t *cmd, void *conf);static ngx_command_t ngx_http_hello_commands[] = &#123; &#123; ngx_string("hello_string"), NGX_HTTP_LOC_CONF|NGX_CONF_NOARGS|NGX_CONF_TAKE1, ngx_http_hello_string, NGX_HTTP_LOC_CONF_OFFSET, offsetof(ngx_http_hello_loc_conf_t, hello_string), NULL &#125;, &#123; ngx_string("hello_counter"), NGX_HTTP_LOC_CONF|NGX_CONF_FLAG, ngx_http_hello_counter, NGX_HTTP_LOC_CONF_OFFSET, offsetof(ngx_http_hello_loc_conf_t, hello_counter), NULL &#125;, ngx_null_command&#125;;/* static u_char ngx_hello_default_string[] = "Default String: Hello, world!";*/static int ngx_hello_visited_times = 0; static ngx_http_module_t ngx_http_hello_module_ctx = &#123; NULL, /* preconfiguration */ ngx_http_hello_init, /* postconfiguration */ NULL, /* create main configuration */ NULL, /* init main configuration */ NULL, /* create server configuration */ NULL, /* merge server configuration */ ngx_http_hello_create_loc_conf, /* create location configuration */ NULL /* merge location configuration */&#125;;ngx_module_t ngx_http_hello_module = &#123; NGX_MODULE_V1, &amp;ngx_http_hello_module_ctx, /* module context */ ngx_http_hello_commands, /* module directives */ NGX_HTTP_MODULE, /* module type */ NULL, /* init master */ NULL, /* init module */ NULL, /* init process */ NULL, /* init thread */ NULL, /* exit thread */ NULL, /* exit process */ NULL, /* exit master */ NGX_MODULE_V1_PADDING&#125;;static ngx_int_tngx_http_hello_handler(ngx_http_request_t *r)&#123; ngx_int_t rc; ngx_buf_t *b; ngx_chain_t out; ngx_http_hello_loc_conf_t* my_conf; u_char ngx_hello_string[1024] = &#123;0&#125;; ngx_uint_t content_length = 0; ngx_log_error(NGX_LOG_EMERG, r-&gt;connection-&gt;log, 0, "ngx_http_hello_handler is called!"); my_conf = ngx_http_get_module_loc_conf(r, ngx_http_hello_module); if (my_conf-&gt;hello_string.len == 0 ) &#123; ngx_log_error(NGX_LOG_EMERG, r-&gt;connection-&gt;log, 0, "hello_string is empty!"); return NGX_DECLINED; &#125; if (my_conf-&gt;hello_counter == NGX_CONF_UNSET || my_conf-&gt;hello_counter == 0) &#123; ngx_sprintf(ngx_hello_string, "%s", my_conf-&gt;hello_string.data); &#125; else &#123; ngx_sprintf(ngx_hello_string, "%s Visited Times:%d", my_conf-&gt;hello_string.data, ++ngx_hello_visited_times); &#125; ngx_log_error(NGX_LOG_EMERG, r-&gt;connection-&gt;log, 0, "hello_string:%s", ngx_hello_string); content_length = ngx_strlen(ngx_hello_string); /* we response to 'GET' and 'HEAD' requests only */ if (!(r-&gt;method &amp; (NGX_HTTP_GET|NGX_HTTP_HEAD))) &#123; return NGX_HTTP_NOT_ALLOWED; &#125; /* discard request body, since we don't need it here */ rc = ngx_http_discard_request_body(r); if (rc != NGX_OK) &#123; return rc; &#125; /* set the 'Content-type' header */ /* *r-&gt;headers_out.content_type.len = sizeof("text/html") - 1; *r-&gt;headers_out.content_type.data = (u_char *)"text/html"; */ ngx_str_set(&amp;r-&gt;headers_out.content_type, "text/html"); /* send the header only, if the request type is http 'HEAD' */ if (r-&gt;method == NGX_HTTP_HEAD) &#123; r-&gt;headers_out.status = NGX_HTTP_OK; r-&gt;headers_out.content_length_n = content_length; return ngx_http_send_header(r); &#125; /* allocate a buffer for your response body */ b = ngx_pcalloc(r-&gt;pool, sizeof(ngx_buf_t)); if (b == NULL) &#123; return NGX_HTTP_INTERNAL_SERVER_ERROR; &#125; /* attach this buffer to the buffer chain */ out.buf = b; out.next = NULL; /* adjust the pointers of the buffer */ b-&gt;pos = ngx_hello_string; b-&gt;last = ngx_hello_string + content_length; b-&gt;memory = 1; /* this buffer is in memory */ b-&gt;last_buf = 1; /* this is the last buffer in the buffer chain */ /* set the status line */ r-&gt;headers_out.status = NGX_HTTP_OK; r-&gt;headers_out.content_length_n = content_length; /* send the headers of your response */ rc = ngx_http_send_header(r); if (rc == NGX_ERROR || rc &gt; NGX_OK || r-&gt;header_only) &#123; return rc; &#125; /* send the buffer chain of your response */ return ngx_http_output_filter(r, &amp;out);&#125;static void *ngx_http_hello_create_loc_conf(ngx_conf_t *cf)&#123; ngx_http_hello_loc_conf_t* local_conf = NULL; local_conf = ngx_pcalloc(cf-&gt;pool, sizeof(ngx_http_hello_loc_conf_t)); if (local_conf == NULL) &#123; return NULL; &#125; ngx_str_null(&amp;local_conf-&gt;hello_string); local_conf-&gt;hello_counter = NGX_CONF_UNSET; return local_conf;&#125; /*static char *ngx_http_hello_merge_loc_conf(ngx_conf_t *cf, void *parent, void *child)&#123; ngx_http_hello_loc_conf_t* prev = parent; ngx_http_hello_loc_conf_t* conf = child; ngx_conf_merge_str_value(conf-&gt;hello_string, prev-&gt;hello_string, ngx_hello_default_string); ngx_conf_merge_value(conf-&gt;hello_counter, prev-&gt;hello_counter, 0); return NGX_CONF_OK;&#125;*/static char *ngx_http_hello_string(ngx_conf_t *cf, ngx_command_t *cmd, void *conf)&#123; ngx_http_hello_loc_conf_t* local_conf; local_conf = conf; char* rv = ngx_conf_set_str_slot(cf, cmd, conf); ngx_conf_log_error(NGX_LOG_EMERG, cf, 0, "hello_string:%s", local_conf-&gt;hello_string.data); return rv;&#125;static char *ngx_http_hello_counter(ngx_conf_t *cf, ngx_command_t *cmd, void *conf)&#123; ngx_http_hello_loc_conf_t* local_conf; local_conf = conf; char* rv = NULL; rv = ngx_conf_set_flag_slot(cf, cmd, conf); ngx_conf_log_error(NGX_LOG_EMERG, cf, 0, "hello_counter:%d", local_conf-&gt;hello_counter); return rv; &#125;static ngx_int_tngx_http_hello_init(ngx_conf_t *cf)&#123; ngx_http_handler_pt *h; ngx_http_core_main_conf_t *cmcf; cmcf = ngx_http_conf_get_module_main_conf(cf, ngx_http_core_module); h = ngx_array_push(&amp;cmcf-&gt;phases[NGX_HTTP_CONTENT_PHASE].handlers); if (h == NULL) &#123; return NGX_ERROR; &#125; *h = ngx_http_hello_handler; return NGX_OK;&#125; 通过上面一些介绍，我相信大家都能对整个示例模块有一个比较好的理解。唯一可能感觉有些理解困难的地方在于ngx_http_hello_handler 函数里面产生和设置输出。但其实大家在本书的前面的相关章节都可以看到对 ngx_buf_t 和 request 等相关数据结构的说明。如果仔细看了这些地方的说明的话，应该对这里代码的实现就比较容易理解了。因此，这里不再赘述解释。 Nginx handler 模块的编译和使用由 小路依依 创建， 最后一次修改 2016-08-12 handler 模块的编译和使用模块的功能开发完了之后，模块的使用还需要编译才能够执行，下面我们来看下模块的编译和使用。 config 文件的编写对于开发一个模块，我们是需要把这个模块的 C 代码组织到一个目录里，同时需要编写一个 config 文件。这个 config 文件的内容就是告诉 Nginx 的编译脚本，该如何进行编译。我们来看一下 hello handler module 的 config 文件的内容，然后再做解释。 123ngx_addon_name=ngx_http_hello_moduleHTTP_MODULES=&quot;$HTTP_MODULES ngx_http_hello_module&quot;NGX_ADDON_SRCS=&quot;$NGX_ADDON_SRCS $ngx_addon_dir/ngx_http_hello_module.c&quot; 其实文件很简单，几乎不需要做什么解释。大家一看都懂了。唯一需要说明的是，如果这个模块的实现有多个源文件，那么都在 NGX_ADDON_SRCS 这个变量里，依次写进去就可以。 编译对于模块的编译，Nginx 并不像 apache 一样，提供了单独的编译工具，可以在没有 apache 源代码的情况下来单独编译一个模块的代码。Nginx 必须去到 Nginx 的源代码目录里，通过 configure 指令的参数，来进行编译。下面看一下 hello module 的 configure 指令： 1./configure --prefix=/usr/local/nginx-1.3.1 --add-module=/home/jizhao/open_source/book_module 我写的这个示例模块的代码和 config 文件都放在/home/jizhao/open_source/book_module这个目录下。所以一切都很明了，也没什么好说的了。 使用使用一个模块需要根据这个模块定义的配置指令来做。比如我们这个简单的 hello handler module 的使用就很简单。在我的测试服务器的配置文件里，就是在 http 里面的默认的 server 里面加入如下的配置： 1234location /test &#123; hello_string jizhao; hello_counter on;&#125; 当我们访问这个地址的时候, lynx http://127.0.0.1/test 的时候，就可以看到返回的结果。 1jizhao Visited Times:1 当然你访问多次，这个次数是会增加的。 Nginx 更多 handler 模块示例分析由 小路依依 创建， 最后一次修改 2016-08-12 更多 handler 模块示例分析http access module该模块的代码位于src/http/modules/ngx_http_access_module.c中。该模块的作用是提供对于特定 host 的客户端的访问控制。可以限定特定 host 的客户端对于服务端全部，或者某个 server，或者是某个 location 的访问。 该模块的实现非常简单，总共也就只有几个函数。 1234567891011121314static ngx_int_t ngx_http_access_handler(ngx_http_request_t *r);static ngx_int_t ngx_http_access_inet(ngx_http_request_t *r, ngx_http_access_loc_conf_t *alcf, in_addr_t addr);#if (NGX_HAVE_INET6)static ngx_int_t ngx_http_access_inet6(ngx_http_request_t *r, ngx_http_access_loc_conf_t *alcf, u_char *p);#endifstatic ngx_int_t ngx_http_access_found(ngx_http_request_t *r, ngx_uint_t deny);static char *ngx_http_access_rule(ngx_conf_t *cf, ngx_command_t *cmd, void *conf);static void *ngx_http_access_create_loc_conf(ngx_conf_t *cf);static char *ngx_http_access_merge_loc_conf(ngx_conf_t *cf, void *parent, void *child);static ngx_int_t ngx_http_access_init(ngx_conf_t *cf); 对于与配置相关的几个函数都不需要做解释了，需要提一下的是函数 ngx_http_access_init，该函数在实现上把本模块挂载到了 NGX_HTTP_ACCESS_PHASE 阶段的 handler 上，从而使自己的被调用时机发生在了 NGX_HTTP_CONTENT_PHASE 等阶段前。因为进行客户端地址的限制检查，根本不需要等到这么后面。 另外看一下这个模块的主处理函数 ngx_http_access_handler。这个函数的逻辑也非常简单，主要是根据客户端地址的类型，来分别选择 ipv4 类型的处理函数 ngx_http_access_inet 还是 ipv6 类型的处理函数 ngx_http_access_inet6。 而这个两个处理函数内部也非常简单，就是循环检查每个规则，检查是否有匹配的规则，如果有就返回匹配的结果，如果都没有匹配，就默认拒绝。 http static module从某种程度上来说，此模块可以算的上是“最正宗的”，“最古老”的 content handler。因为本模块的作用就是读取磁盘上的静态文件，并把文件内容作为产生的输出。在Web技术发展的早期，只有静态页面，没有服务端脚本来动态生成 HTML 的时候。恐怕开发个 Web 服务器的时候，第一个要开发就是这样一个 content handler。 http static module 的代码位于src/http/modules/ngx_http_static_module.c中，总共只有两百多行近三百行。可以说是非常短小。 我们首先来看一下该模块的模块上下文的定义。 12345678910111213ngx_http_module_t ngx_http_static_module_ctx = &#123; NULL, /* preconfiguration */ ngx_http_static_init, /* postconfiguration */ NULL, /* create main configuration */ NULL, /* init main configuration */ NULL, /* create server configuration */ NULL, /* merge server configuration */ NULL, /* create location configuration */ NULL /* merge location configuration */&#125;; 是非常的简洁吧，连任何与配置相关的函数都没有。对了，因为该模块没有提供任何配置指令。大家想想也就知道了，这个模块做的事情实在是太简单了，也确实没什么好配置的。唯一需要调用的函数是一个 ngx_http_static_init 函数。好了，来看一下这个函数都干了写什么。 1234567891011121314151617static ngx_int_tngx_http_static_init(ngx_conf_t *cf)&#123; ngx_http_handler_pt *h; ngx_http_core_main_conf_t *cmcf; cmcf = ngx_http_conf_get_module_main_conf(cf, ngx_http_core_module); h = ngx_array_push(&amp;cmcf-&gt;phases[NGX_HTTP_CONTENT_PHASE].handlers); if (h == NULL) &#123; return NGX_ERROR; &#125; *h = ngx_http_static_handler; return NGX_OK;&#125; 仅仅是挂载这个 handler 到 NGX_HTTP_CONTENT_PHASE 处理阶段。简单吧？ 下面我们就看一下这个模块最核心的处理逻辑所在的 ngx_http_static_handler 函数。该函数大概占了这个模块代码量的百分之八九十。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220static ngx_int_tngx_http_static_handler(ngx_http_request_t *r)&#123; u_char *last, *location; size_t root, len; ngx_str_t path; ngx_int_t rc; ngx_uint_t level; ngx_log_t *log; ngx_buf_t *b; ngx_chain_t out; ngx_open_file_info_t of; ngx_http_core_loc_conf_t *clcf; if (!(r-&gt;method &amp; (NGX_HTTP_GET|NGX_HTTP_HEAD|NGX_HTTP_POST))) &#123; return NGX_HTTP_NOT_ALLOWED; &#125; if (r-&gt;uri.data[r-&gt;uri.len - 1] == &apos;/&apos;) &#123; return NGX_DECLINED; &#125; log = r-&gt;connection-&gt;log; /* * ngx_http_map_uri_to_path() allocates memory for terminating &apos;\0&apos; * so we do not need to reserve memory for &apos;/&apos; for possible redirect */ last = ngx_http_map_uri_to_path(r, &amp;path, &amp;root, 0); if (last == NULL) &#123; return NGX_HTTP_INTERNAL_SERVER_ERROR; &#125; path.len = last - path.data; ngx_log_debug1(NGX_LOG_DEBUG_HTTP, log, 0, &quot;http filename: \&quot;%s\&quot;&quot;, path.data); clcf = ngx_http_get_module_loc_conf(r, ngx_http_core_module); ngx_memzero(&amp;of, sizeof(ngx_open_file_info_t)); of.read_ahead = clcf-&gt;read_ahead; of.directio = clcf-&gt;directio; of.valid = clcf-&gt;open_file_cache_valid; of.min_uses = clcf-&gt;open_file_cache_min_uses; of.errors = clcf-&gt;open_file_cache_errors; of.events = clcf-&gt;open_file_cache_events; if (ngx_http_set_disable_symlinks(r, clcf, &amp;path, &amp;of) != NGX_OK) &#123; return NGX_HTTP_INTERNAL_SERVER_ERROR; &#125; if (ngx_open_cached_file(clcf-&gt;open_file_cache, &amp;path, &amp;of, r-&gt;pool) != NGX_OK) &#123; switch (of.err) &#123; case 0: return NGX_HTTP_INTERNAL_SERVER_ERROR; case NGX_ENOENT: case NGX_ENOTDIR: case NGX_ENAMETOOLONG: level = NGX_LOG_ERR; rc = NGX_HTTP_NOT_FOUND; break; case NGX_EACCES:#if (NGX_HAVE_OPENAT) case NGX_EMLINK: case NGX_ELOOP:#endif level = NGX_LOG_ERR; rc = NGX_HTTP_FORBIDDEN; break; default: level = NGX_LOG_CRIT; rc = NGX_HTTP_INTERNAL_SERVER_ERROR; break; &#125; if (rc != NGX_HTTP_NOT_FOUND || clcf-&gt;log_not_found) &#123; ngx_log_error(level, log, of.err, &quot;%s \&quot;%s\&quot; failed&quot;, of.failed, path.data); &#125; return rc; &#125; r-&gt;root_tested = !r-&gt;error_page; ngx_log_debug1(NGX_LOG_DEBUG_HTTP, log, 0, &quot;http static fd: %d&quot;, of.fd); if (of.is_dir) &#123; ngx_log_debug0(NGX_LOG_DEBUG_HTTP, log, 0, &quot;http dir&quot;); ngx_http_clear_location(r); r-&gt;headers_out.location = ngx_palloc(r-&gt;pool, sizeof(ngx_table_elt_t)); if (r-&gt;headers_out.location == NULL) &#123; return NGX_HTTP_INTERNAL_SERVER_ERROR; &#125; len = r-&gt;uri.len + 1; if (!clcf-&gt;alias &amp;&amp; clcf-&gt;root_lengths == NULL &amp;&amp; r-&gt;args.len == 0) &#123; location = path.data + clcf-&gt;root.len; *last = &apos;/&apos;; &#125; else &#123; if (r-&gt;args.len) &#123; len += r-&gt;args.len + 1; &#125; location = ngx_pnalloc(r-&gt;pool, len); if (location == NULL) &#123; return NGX_HTTP_INTERNAL_SERVER_ERROR; &#125; last = ngx_copy(location, r-&gt;uri.data, r-&gt;uri.len); *last = &apos;/&apos;; if (r-&gt;args.len) &#123; *++last = &apos;?&apos;; ngx_memcpy(++last, r-&gt;args.data, r-&gt;args.len); &#125; &#125; /* * we do not need to set the r-&gt;headers_out.location-&gt;hash and * r-&gt;headers_out.location-&gt;key fields */ r-&gt;headers_out.location-&gt;value.len = len; r-&gt;headers_out.location-&gt;value.data = location; return NGX_HTTP_MOVED_PERMANENTLY; &#125;#if !(NGX_WIN32) /* the not regular files are probably Unix specific */ if (!of.is_file) &#123; ngx_log_error(NGX_LOG_CRIT, log, 0, &quot;\&quot;%s\&quot; is not a regular file&quot;, path.data); return NGX_HTTP_NOT_FOUND; &#125;#endif if (r-&gt;method &amp; NGX_HTTP_POST) &#123; return NGX_HTTP_NOT_ALLOWED; &#125; rc = ngx_http_discard_request_body(r); if (rc != NGX_OK) &#123; return rc; &#125; log-&gt;action = &quot;sending response to client&quot;; r-&gt;headers_out.status = NGX_HTTP_OK; r-&gt;headers_out.content_length_n = of.size; r-&gt;headers_out.last_modified_time = of.mtime; if (ngx_http_set_content_type(r) != NGX_OK) &#123; return NGX_HTTP_INTERNAL_SERVER_ERROR; &#125; if (r != r-&gt;main &amp;&amp; of.size == 0) &#123; return ngx_http_send_header(r); &#125; r-&gt;allow_ranges = 1; /* we need to allocate all before the header would be sent */ b = ngx_pcalloc(r-&gt;pool, sizeof(ngx_buf_t)); if (b == NULL) &#123; return NGX_HTTP_INTERNAL_SERVER_ERROR; &#125; b-&gt;file = ngx_pcalloc(r-&gt;pool, sizeof(ngx_file_t)); if (b-&gt;file == NULL) &#123; return NGX_HTTP_INTERNAL_SERVER_ERROR; &#125; rc = ngx_http_send_header(r); if (rc == NGX_ERROR || rc &gt; NGX_OK || r-&gt;header_only) &#123; return rc; &#125; b-&gt;file_pos = 0; b-&gt;file_last = of.size; b-&gt;in_file = b-&gt;file_last ? 1: 0; b-&gt;last_buf = (r == r-&gt;main) ? 1: 0; b-&gt;last_in_chain = 1; b-&gt;file-&gt;fd = of.fd; b-&gt;file-&gt;name = path; b-&gt;file-&gt;log = log; b-&gt;file-&gt;directio = of.is_directio; out.buf = b; out.next = NULL; return ngx_http_output_filter(r, &amp;out);&#125; 首先是检查客户端的 http 请求类型（r-&gt;method），如果请求类型为NGX_HTTP_GET|NGX_HTTP_HEAD|NGX_HTTP_POST，则继续进行处理，否则一律返回 NGX_HTTP_NOT_ALLOWED 从而拒绝客户端的发起的请求。 其次是检查请求的 url 的结尾字符是不是斜杠/，如果是说明请求的不是一个文件，给后续的 handler 去处理，比如后续的 ngx_http_autoindex_handler（如果是请求的是一个目录下面，可以列出这个目录的文件），或者是 ngx_http_index_handler（如果请求的路径下面有个默认的 index 文件，直接返回 index 文件的内容）。 然后接下来调用了一个 ngx_http_map_uri_to_path 函数，该函数的作用是把请求的 http 协议的路径转化成一个文件系统的路径。 然后根据转化出来的具体路径，去打开文件，打开文件的时候做了 2 种检查，一种是，如果请求的文件是个 symbol link，根据配置，是否允许符号链接，不允许返回错误。还有一个检查是，如果请求的是一个名称，是一个目录的名字，也返回错误。如果都没有错误，就读取文件，返回内容。其实说返回内容可能不是特别准确，比较准确的说法是，把产生的内容传递给后续的 filter 去处理。 http log module该模块提供了对于每一个 http 请求进行记录的功能，也就是我们见到的 access.log。当然这个模块对于 log 提供了一些配置指令，使得可以比较方便的定制 access.log。 这个模块的代码位于src/http/modules/ngx_http_log_module.c，虽然这个模块的代码有接近 1400 行，但是主要的逻辑在于对日志本身格式啊，等细节的处理。我们在这里进行分析主要是关注，如何编写一个 log handler 的问题。 由于 log handler 的时候，拿到的参数也是 request 这个东西，那么也就意味着我们如果需要，可以好好研究下这个结构，把我们需要的所有信息都记录下来。 对于 log handler，有一点特别需要注意的就是，log handler 是无论如何都会被调用的，就是只要服务端接受到了一个客户端的请求，也就是产生了一个 request 对象，那么这些个 log handler 的处理函数都会被调用的，就是在释放 request 的时候被调用的（ngx_http_free_request函数）。 那么当然绝对不能忘记的就是 log handler 最好，也是建议被挂载在 NGX_HTTP_LOG_PHASE 阶段。因为挂载在其他阶段，有可能在某些情况下被跳过，而没有执行到，导致你的 log 模块记录的信息不全。 还有一点要说明的是，由于 Nginx 是允许在某个阶段有多个 handler 模块存在的，根据其处理结果，确定是否要调用下一个 handler。但是对于挂载在 NGX_HTTP_LOG_PHASE 阶段的 handler，则根本不关注这里 handler 的具体处理函数的返回值，所有的都被调用。如下，位于src/http/ngx_http_request.c中的 ngx_http_log_request 函数。 12345678910111213141516static voidngx_http_log_request(ngx_http_request_t *r)&#123; ngx_uint_t i, n; ngx_http_handler_pt *log_handler; ngx_http_core_main_conf_t *cmcf; cmcf = ngx_http_get_module_main_conf(r, ngx_http_core_module); log_handler = cmcf-&gt;phases[NGX_HTTP_LOG_PHASE].handlers.elts; n = cmcf-&gt;phases[NGX_HTTP_LOG_PHASE].handlers.nelts; for (i = 0; i &lt; n; i++) &#123; log_handler[i](r); &#125;&#125; Nginx 过滤模块简介由 小路依依 创建， 最后一次修改 2016-08-12 过滤模块简介执行时间和内容过滤（filter）模块是过滤响应头和内容的模块，可以对回复的头和内容进行处理。它的处理时间在获取回复内容之后，向用户发送响应之前。它的处理过程分为两个阶段，过滤 HTTP 回复的头部和主体，在这两个阶段可以分别对头部和主体进行修改。 在代码中有类似的函数： 12ngx_http_top_header_filter(r);ngx_http_top_body_filter(r, in); 就是分别对头部和主体进行过滤的函数。所有模块的响应内容要返回给客户端，都必须调用这两个接口。 执行顺序过滤模块的调用是有顺序的，它的顺序在编译的时候就决定了。控制编译的脚本位于 auto/modules 中，当你编译完 Nginx 以后，可以在 objs 目录下面看到一个 ngx_modules.c 的文件。打开这个文件，有类似的代码： 1234567891011121314151617ngx_module_t *ngx_modules[] = &#123; ... &amp;ngx_http_write_filter_module, &amp;ngx_http_header_filter_module, &amp;ngx_http_chunked_filter_module, &amp;ngx_http_range_header_filter_module, &amp;ngx_http_gzip_filter_module, &amp;ngx_http_postpone_filter_module, &amp;ngx_http_ssi_filter_module, &amp;ngx_http_charset_filter_module, &amp;ngx_http_userid_filter_module, &amp;ngx_http_headers_filter_module, &amp;ngx_http_copy_filter_module, &amp;ngx_http_range_body_filter_module, &amp;ngx_http_not_modified_filter_module, NULL&#125;; 从 write_filter 到 not_modified_filter，模块的执行顺序是反向的。也就是说最早执行的是 not_modified_filter，然后各个模块依次执行。一般情况下，第三方过滤模块的 config 文件会将模块名追加到变量 HTTP_AUX_FILTER_MODULES 中，此时该模块只能加入到 copy_filter 和 headers_filter 模块之间执行。 Nginx 执行的时候是怎么按照次序依次来执行各个过滤模块呢？它采用了一种很隐晦的方法，即通过局部的全局变量。比如，在每个 filter 模块，很可能看到如下代码： 12345678910static ngx_http_output_header_filter_pt ngx_http_next_header_filter;static ngx_http_output_body_filter_pt ngx_http_next_body_filter;...ngx_http_next_header_filter = ngx_http_top_header_filter;ngx_http_top_header_filter = ngx_http_example_header_filter;ngx_http_next_body_filter = ngx_http_top_body_filter;ngx_http_top_body_filter = ngx_http_example_body_filter; ngx_http_top_header_filter 是一个全局变量。当编译进一个 filter 模块的时候，就被赋值为当前 filter 模块的处理函数。而 ngx_http_next_header_filter 是一个局部全局变量，它保存了编译前上一个 filter 模块的处理函数。所以整体看来，就像用全局变量组成的一条单向链表。 每个模块想执行下一个过滤函数，只要调用一下 ngx_http_next_header_filter 这个局部变量。而整个过滤模块链的入口，需要调用 ngx_http_top_header_filter 这个全局变量。ngx_http_top_body_filter 的行为与 header fitler 类似。 响应头和响应体过滤函数的执行顺序如下所示： 这图只表示了 head_filter 和 body_filter 之间的执行顺序，在 header_filter 和 body_filter 处理函数之间，在 body_filter 处理函数之间，可能还有其他执行代码。 模块编译Nginx 可以方便的加入第三方的过滤模块。在过滤模块的目录里，首先需要加入 config 文件，文件的内容如下： 123ngx_addon_name=ngx_http_example_filter_moduleHTTP_AUX_FILTER_MODULES=&quot;$HTTP_AUX_FILTER_MODULES ngx_http_example_filter_module&quot;NGX_ADDON_SRCS=&quot;$NGX_ADDON_SRCS $ngx_addon_dir/ngx_http_example_filter_module.c&quot; 说明把这个名为 ngx_http_example_filter_module 的过滤模块加入，ngx_http_example_filter_module.c 是该模块的源代码。 注意 HTTP_AUX_FILTER_MODULES 这个变量与一般的内容处理模块不同。 Nginx 过滤模块的分析由 小路依依 创建， 最后一次修改 2016-08-12 过滤模块的分析相关结构体ngx_chain_t 结构非常简单，是一个单向链表： 123456typedef struct ngx_chain_s ngx_chain_t;struct ngx_chain_s &#123; ngx_buf_t *buf; ngx_chain_t *next;&#125;; 在过滤模块中，所有输出的内容都是通过一条单向链表所组成。这种单向链表的设计，正好应和了 Nginx 流式的输出模式。每次 Nginx 都是读到一部分的内容，就放到链表，然后输出出去。这种设计的好处是简单，非阻塞，但是相应的问题就是跨链表的内容操作非常麻烦，如果需要跨链表，很多时候都只能缓存链表的内容。 单链表负载的就是 ngx_buf_t，这个结构体使用非常广泛，先让我们看下该结构体的代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748struct ngx_buf_s &#123; u_char *pos; /* 当前buffer真实内容的起始位置 */ u_char *last; /* 当前buffer真实内容的结束位置 */ off_t file_pos; /* 在文件中真实内容的起始位置 */ off_t file_last; /* 在文件中真实内容的结束位置 */ u_char *start; /* buffer内存的开始分配的位置 */ u_char *end; /* buffer内存的结束分配的位置 */ ngx_buf_tag_t tag; /* buffer属于哪个模块的标志 */ ngx_file_t *file; /* buffer所引用的文件 */ /* 用来引用替换过后的buffer，以便当所有buffer输出以后， * 这个影子buffer可以被释放。 */ ngx_buf_t *shadow; /* the buf&apos;s content could be changed */ unsigned temporary:1; /* * the buf&apos;s content is in a memory cache or in a read only memory * and must not be changed */ unsigned memory:1; /* the buf&apos;s content is mmap()ed and must not be changed */ unsigned mmap:1; unsigned recycled:1; /* 内存可以被输出并回收 */ unsigned in_file:1; /* buffer的内容在文件中 */ /* 马上全部输出buffer的内容, gzip模块里面用得比较多 */ unsigned flush:1; /* 基本上是一段输出链的最后一个buffer带的标志，标示可以输出， * 有些零长度的buffer也可以置该标志 */ unsigned sync:1; /* 所有请求里面最后一块buffer，包含子请求 */ unsigned last_buf:1; /* 当前请求输出链的最后一块buffer */ unsigned last_in_chain:1; /* shadow链里面的最后buffer，可以释放buffer了 */ unsigned last_shadow:1; /* 是否是暂存文件 */ unsigned temp_file:1; /* 统计用，表示使用次数 */ /* STUB */ int num;&#125;; 一般 buffer 结构体可以表示一块内存，内存的起始和结束地址分别用 start 和 end 表示，pos 和 last 表示实际的内容。如果内容已经处理过了，pos 的位置就可以往后移动。如果读取到新的内容，last 的位置就会往后移动。所以 buffer 可以在多次调用过程中使用。如果 last 等于 end，就说明这块内存已经用完了。如果 pos 等于 last，说明内存已经处理完了。下面是一个简单的示意图，说明 buffer 中指针的用法： 响应头过滤函数响应头过滤函数主要的用处就是处理 HTTP 响应的头，可以根据实际情况对于响应头进行修改或者添加删除。响应头过滤函数先于响应体过滤函数，而且只调用一次，所以一般可作过滤模块的初始化工作。 响应头过滤函数的入口只有一个： 1234567ngx_int_tngx_http_send_header(ngx_http_request_t *r)&#123; ... return ngx_http_top_header_filter(r);&#125; 该函数向客户端发送回复的时候调用，然后按前一节所述的执行顺序。该函数的返回值一般是 NGX_OK，NGX_ERROR 和 NGX_AGAIN，分别表示处理成功，失败和未完成。 你可以把 HTTP 响应头的存储方式想象成一个 hash 表，在 Nginx 内部可以很方便地查找和修改各个响应头部，ngx_http_header_filter_module 过滤模块把所有的 HTTP 头组合成一个完整的 buffer，最终 ngx_http_write_filter_module 过滤模块把 buffer 输出。 按照前一节过滤模块的顺序，依次讲解如下： filter module description ngx_http_not_modified_filter_module 默认打开，如果请求的 if-modified-since 等于回复的 last-modified 间值，说明回复没有变化，清空所有回复的内容，返回 304。 ngx_http_range_body_filter_module 默认打开，只是响应体过滤函数，支持 range 功能，如果请求包含range请求，那就只发送range请求的一段内容。 ngx_http_copy_filter_module 始终打开，只是响应体过滤函数， 主要工作是把文件中内容读到内存中，以便进行处理。 ngx_http_headers_filter_module 始终打开，可以设置 expire 和 Cache-control 头，可以添加任意名称的头 ngx_http_userid_filter_module 默认关闭，可以添加统计用的识别用户的 cookie。 ngx_http_charset_filter_module 默认关闭，可以添加 charset，也可以将内容从一种字符集转换到另外一种字符集，不支持多字节字符集。 ngx_http_ssi_filter_module 默认关闭，过滤 SSI 请求，可以发起子请求，去获取include进来的文件 ngx_http_postpone_filter_module 始终打开，用来将子请求和主请求的输出链合并 ngx_http_gzip_filter_module 默认关闭，支持流式的压缩内容 ngx_http_range_header_filter_module 默认打开，只是响应头过滤函数，用来解析range头，并产生range响应的头。 ngx_http_chunked_filter_module 默认打开，对于 HTTP/1.1 和缺少 content-length 的回复自动打开。 ngx_http_header_filter_module 始终打开，用来将所有 header 组成一个完整的 HTTP 头。 ngx_http_write_filter_module 始终打开，将输出链拷贝到 r-&gt;out中，然后输出内容。 响应体过滤函数响应体过滤函数是过滤响应主体的函数。ngx_http_top_body_filter 这个函数每个请求可能会被执行多次，它的入口函数是 ngx_http_output_filter，比如： 1234567891011121314151617ngx_int_tngx_http_output_filter(ngx_http_request_t *r, ngx_chain_t *in)&#123; ngx_int_t rc; ngx_connection_t *c; c = r-&gt;connection; rc = ngx_http_top_body_filter(r, in); if (rc == NGX_ERROR) &#123; /* NGX_ERROR may be returned by any filter */ c-&gt;error = 1; &#125; return rc;&#125; ngx_http_output_filter 可以被一般的静态处理模块调用，也有可能是在 upstream 模块里面被调用，对于整个请求的处理阶段来说，他们处于的用处都是一样的，就是把响应内容过滤，然后发给客户端。 具体模块的响应体过滤函数的格式类似这样： 1234567static int ngx_http_example_body_filter(ngx_http_request_t *r, ngx_chain_t *in)&#123; ... return ngx_http_next_body_filter(r, in);&#125; 该函数的返回值一般是 NGX_OK，NGX_ERROR 和 NGX_AGAIN，分别表示处理成功，失败和未完成。 主要功能介绍响应的主体内容就存于单链表 in，链表一般不会太长，有时 in 参数可能为 NULL。in中存有buf结构体中，对于静态文件，这个buf大小默认是 32K；对于反向代理的应用，这个buf可能是4k或者8k。为了保持内存的低消耗，Nginx一般不会分配过大的内存，处理的原则是收到一定的数据，就发送出去。一个简单的例子，可以看看Nginx的chunked_filter模块，在没有 content-length 的情况下，chunk 模块可以流式（stream）的加上长度，方便浏览器接收和显示内容。 在响应体过滤模块中，尤其要注意的是 buf 的标志位，完整描述可以在“相关结构体”这个节中看到。如果 buf 中包含 last 标志，说明是最后一块 buf，可以直接输出并结束请求了。如果有 flush 标志，说明这块 buf 需要马上输出，不能缓存。如果整块 buffer 经过处理完以后，没有数据了，你可以把 buffer 的 sync 标志置上，表示只是同步的用处。 当所有的过滤模块都处理完毕时，在最后的 write_fitler 模块中，Nginx 会将 in 输出链拷贝到 r-&gt;out 输出链的末尾，然后调用 sendfile 或者 writev 接口输出。由于 Nginx 是非阻塞的 socket 接口，写操作并不一定会成功，可能会有部分数据还残存在 r-&gt;out。在下次的调用中，Nginx 会继续尝试发送，直至成功。 发出子请求Nginx 过滤模块一大特色就是可以发出子请求，也就是在过滤响应内容的时候，你可以发送新的请求，Nginx 会根据你调用的先后顺序，将多个回复的内容拼接成正常的响应主体。一个简单的例子可以参考 addition 模块。 Nginx 是如何保证父请求和子请求的顺序呢？当 Nginx 发出子请求时，就会调用 ngx_http_subrequest 函数，将子请求插入父请求的 r-&gt;postponed 链表中。子请求会在主请求执行完毕时获得依次调用。子请求同样会有一个请求所有的生存期和处理过程，也会进入过滤模块流程。 关键点是在 postpone_filter 模块中，它会拼接主请求和子请求的响应内容。r-&gt;postponed 按次序保存有父请求和子请求，它是一个链表，如果前面一个请求未完成，那后一个请求内容就不会输出。当前一个请求完成时并输出时，后一个请求才可输出，当所有的子请求都完成时，所有的响应内容也就输出完毕了。 一些优化措施Nginx 过滤模块涉及到的结构体，主要就是 chain 和 buf，非常简单。在日常的过滤模块中，这两类结构使用非常频繁，Nginx采用类似 freelist 重复利用的原则，将使用完毕的 chain 或者 buf 结构体，放置到一个固定的空闲链表里，以待下次使用。 比如，在通用内存池结构体中，pool-&gt;chain 变量里面就保存着释放的 chain。而一般的 buf 结构体，没有模块间公用的空闲链表池，都是保存在各模块的缓存空闲链表池里面。对于 buf 结构体，还有一种 busy 链表，表示该链表中的 buf 都处于输出状态，如果 buf 输出完毕，这些 buf 就可以释放并重复利用了。 功能 函数名 chain 分配 ngx_alloc_chain_link chain 释放 ngx_free_chain buf 分配 ngx_chain_get_free_buf buf 释放 ngx_chain_update_chains 过滤内容的缓存由于 Nginx 设计流式的输出结构，当我们需要对响应内容作全文过滤的时候，必须缓存部分的 buf 内容。该类过滤模块往往比较复杂，比如 sub，ssi，gzip 等模块。这类模块的设计非常灵活，我简单讲一下设计原则： 输入链 in 需要拷贝操作，经过缓存的过滤模块，输入输出链往往已经完全不一样了，所以需要拷贝，通过 ngx_chain_add_copy 函数完成。 一般有自己的 free 和 busy 缓存链表池，可以提高 buf 分配效率。 如果需要分配大块内容，一般分配固定大小的内存卡，并设置 recycled 标志，表示可以重复利用。 原有的输入 buf 被替换缓存时，必须将其 buf-&gt;pos 设为 buf-&gt;last，表明原有的 buf 已经被输出完毕。或者在新建立的 buf，将 buf-&gt;shadow 指向旧的 buf，以便输出完毕时及时释放旧的 buf。 Nginx upstream 模块简介由 小路依依 创建，Loen 最后一次修改 2016-08-12 upstream 模块简介Nginx 模块一般被分成三大类：handler、filter 和 upstream。前面的章节中，读者已经了解了 handler、filter。利用这两类模块，可以使 Nginx 轻松完成任何单机工作。而本章介绍的 upstream 模块，将使 Nginx 跨越单机的限制，完成网络数据的接收、处理和转发。 数据转发功能，为 Nginx 提供了跨越单机的横向处理能力，使 Nginx 摆脱只能为终端节点提供单一功能的限制，而使它具备了网路应用级别的拆分、封装和整合的战略功能。在云模型大行其道的今天，数据转发是 Nginx 有能力构建一个网络应用的关键组件。当然，鉴于开发成本的问题，一个网络应用的关键组件一开始往往会采用高级编程语言开发。但是当系统到达一定规模，并且需要更重视性能的时候，为了达到所要求的性能目标，高级语言开发出的组件必须进行结构化修改。此时，对于修改代价而言，Nginx 的 upstream 模块呈现出极大的吸引力，因为它天生就快。作为附带，Nginx 的配置系统提供的层次化和松耦合使得系统的扩展性也达到比较高的程度。 言归正传，下面介绍 upstream 的写法。 upstream 模块接口从本质上说，upstream 属于 handler，只是他不产生自己的内容，而是通过请求后端服务器得到内容，所以才称为 upstream（上游）。请求并取得响应内容的整个过程已经被封装到 Nginx 内部，所以 upstream 模块只需要开发若干回调函数，完成构造请求和解析响应等具体的工作。 这些回调函数如下表所示： SN 描述 create_request 生成发送到后端服务器的请求缓冲（缓冲链），在初始化 upstream 时使用。 reinit_request 在某台后端服务器出错的情况，Nginx会尝试另一台后端服务器。Nginx 选定新的服务器以后，会先调用此函数，以重新初始化 upstream 模块的工作状态，然后再次进行 upstream 连接。 process_header 处理后端服务器返回的信息头部。所谓头部是与 upstreamserver 通信的协议规定的，比如 HTTP 协议的 header 部分，或者 memcached 协议的响应状态部分。 abort_request 在客户端放弃请求时被调用。不需要在函数中实现关闭后端服务器连接的功能，系统会自动完成关闭连接的步骤，所以一般此函数不会进行任何具体工作。 finalize_request 正常完成与后端服务器的请求后调用该函数，与 abort_request 相同，一般也不会进行任何具体工作。 input_filter 处理后端服务器返回的响应正文。Nginx 默认的 input_filter 会将收到的内容封装成为缓冲区链 ngx_chain。该链由 upstream 的 out_bufs 指针域定位，所以开发人员可以在模块以外通过该指针 得到后端服务器返回的正文数据。memcached 模块实现了自己的 input_filter，在后面会具体分析这个模块。 input_filter_init 初始化 input filter 的上下文。Nginx 默认的 input_filter_init 直接返回。 memcached 模块分析memcache 是一款高性能的分布式 cache 系统，得到了非常广泛的应用。memcache 定义了一套私有通信协议，使得不能通过 HTTP 请求来访问 memcache。但协议本身简单高效，而且 memcache 使用广泛，所以大部分现代开发语言和平台都提供了 memcache 支持，方便开发者使用 memcache。 Nginx 提供了 ngx_http_memcached 模块，提供从 memcache 读取数据的功能，而不提供向 memcache 写数据的功能。作为 Web 服务器，这种设计是可以接受的。 下面，我们开始分析 ngx_http_memcached 模块，一窥 upstream 的奥秘。 Handler 模块？初看 memcached 模块，大家可能觉得并无特别之处。如果稍微细看，甚至觉得有点像 handler 模块，当大家看到这段代码以后，必定疑惑为什么会跟 handler 模块一模一样。 12clcf = ngx_http_conf_get_module_loc_conf(cf, ngx_http_core_module);clcf-&gt;handler = ngx_http_memcached_handler; 因为 upstream 模块使用的就是 handler 模块的接入方式。同时，upstream 模块的指令系统的设计也是遵循 handler 模块的基本规则：配置该模块才会执行该模块。 123456&#123; ngx_string(&quot;memcached_pass&quot;), NGX_HTTP_LOC_CONF|NGX_HTTP_LIF_CONF|NGX_CONF_TAKE1, ngx_http_memcached_pass, NGX_HTTP_LOC_CONF_OFFSET, 0, NULL &#125; 所以大家觉得眼熟是好事，说明大家对 Handler 的写法已经很熟悉了。 Upstream 模块那么，upstream 模块的特别之处究竟在哪里呢？答案是就在模块处理函数的实现中。upstream 模块的处理函数进行的操作都包含一个固定的流程。在 memcached 的例子中，可以观察 ngx_http_memcached_handler 的代码，可以发现，这个固定的操作流程是： 创建 upstream 数据结构。 123if (ngx_http_upstream_create(r) != NGX_OK) &#123; return NGX_HTTP_INTERNAL_SERVER_ERROR;&#125; 设置模块的 tag 和 schema。schema 现在只会用于日志，tag 会用于 buf_chain 管理。 1234u = r-&gt;upstream;ngx_str_set(&amp;u-&gt;schema, &quot;memcached://&quot;);u-&gt;output.tag = (ngx_buf_tag_t) &amp;ngx_http_memcached_module; 设置 upstream 的后端服务器列表数据结构。 12mlcf = ngx_http_get_module_loc_conf(r, ngx_http_memcached_module);u-&gt;conf = &amp;mlcf-&gt;upstream; 设置 upstream 回调函数。在这里列出的代码稍稍调整了代码顺序。 1234567u-&gt;create_request = ngx_http_memcached_create_request;u-&gt;reinit_request = ngx_http_memcached_reinit_request;u-&gt;process_header = ngx_http_memcached_process_header;u-&gt;abort_request = ngx_http_memcached_abort_request;u-&gt;finalize_request = ngx_http_memcached_finalize_request;u-&gt;input_filter_init = ngx_http_memcached_filter_init;u-&gt;input_filter = ngx_http_memcached_filter; 创建并设置 upstream 环境数据结构。 1234567891011ctx = ngx_palloc(r-&gt;pool, sizeof(ngx_http_memcached_ctx_t));if (ctx == NULL) &#123; return NGX_HTTP_INTERNAL_SERVER_ERROR;&#125;ctx-&gt;rest = NGX_HTTP_MEMCACHED_END;ctx-&gt;request = r;ngx_http_set_ctx(r, ctx, ngx_http_memcached_module);u-&gt;input_filter_ctx = ctx; 完成 upstream 初始化并进行收尾工作。 123r-&gt;main-&gt;count++;ngx_http_upstream_init(r);return NGX_DONE; 任何 upstream 模块，简单如 memcached，复杂如 proxy、fastcgi 都是如此。不同的 upstream 模块在这 6 步中的最大差别会出现在第 2、3、4、5 上。其中第 2、4 两步很容易理解，不同的模块设置的标志和使用的回调函数肯定不同。第 5 步也不难理解，只有第3步是最为晦涩的，不同的模块在取得后端服务器列表时，策略的差异非常大，有如 memcached 这样简单明了的，也有如 proxy 那样逻辑复杂的。这个问题先记下来，等把memcached剖析清楚了，再单独讨论。 第 6 步是一个常态。将 count 加 1，然后返回 NGX_DONE。Nginx 遇到这种情况，虽然会认为当前请求的处理已经结束，但是不会释放请求使用的内存资源，也不会关闭与客户端的连接。之所以需要这样，是因为 Nginx 建立了 upstream 请求和客户端请求之间一对一的关系，在后续使用 ngx_event_pipe 将 upstream 响应发送回客户端时，还要使用到这些保存着客户端信息的数据结构。这部分会在后面的原理篇做具体介绍，这里不再展开。 将 upstream 请求和客户端请求进行一对一绑定，这个设计有优势也有缺陷。优势就是简化模块开发，可以将精力集中在模块逻辑上，而缺陷同样明显，一对一的设计很多时候都不能满足复杂逻辑的需要。对于这一点，将会在后面的原理篇来阐述。 回调函数前面剖析了 memcached 模块的骨架，现在开始逐个解决每个回调函数。 ngx_http_memcached_create_request：很简单的按照设置的内容生成一个 key，接着生成一个“get $key”的请求，放在 r-&gt;upstream-&gt;request_bufs 里面。 ngx_http_memcached_reinit_request：无需初始化。 ngx_http_memcached_abort_request：无需额外操作。 ngx_http_memcached_finalize_request：无需额外操作。 ngx_http_memcached_process_header：模块的业务重点函数。memcache 协议的头部信息被定义为第一行文本，可以找到这段代码证明： 1234for (p = u-&gt;buffer.pos; p &lt; u-&gt;buffer.last; p++) &#123; if ( * p == LF) &#123; goto found;&#125; 如果在已读入缓冲的数据中没有发现 LF(‘\n’)字符，函数返回 NGX_AGAIN，表示头部未完全读入，需要继续读取数据。Nginx 在收到新的数据以后会再次调用该函数。 Nginx 处理后端服务器的响应头时只会使用一块缓存，所有数据都在这块缓存中，所以解析头部信息时不需要考虑头部信息跨越多块缓存的情况。而如果头部过大，不能保存在这块缓存中，Nginx 会返回错误信息给客户端，并记录 error log，提示缓存不够大。 process_header 的重要职责是将后端服务器返回的状态翻译成返回给客户端的状态。例如，在 ngx_http_memcached_process_header 中，有这样几段代码： 1234567r-&gt;headers_out.content_length_n = ngx_atoof(len, p - len - 1);u-&gt;headers_in.status_n = 200;u-&gt;state-&gt;status = 200;u-&gt;headers_in.status_n = 404;u-&gt;state-&gt;status = 404; u-&gt;state 用于计算 upstream 相关的变量。比如 u-&gt;state-&gt;status 将被用于计算变量“upstream_status”的值。u-&gt;headers_in 将被作为返回给客户端的响应返回状态码。而第一行则是设置返回给客户端的响应的长度。 在这个函数中不能忘记的一件事情是处理完头部信息以后需要将读指针 pos 后移，否则这段数据也将被复制到返回给客户端的响应的正文中，进而导致正文内容不正确。 1u-&gt;buffer.pos = p + 1; process_header 函数完成响应头的正确处理，应该返回 NGX_OK。如果返回 NGX_AGAIN，表示未读取完整数据，需要从后端服务器继续读取数据。返回 NGX_DECLINED 无意义，其他任何返回值都被认为是出错状态，Nginx 将结束 upstream 请求并返回错误信息。 ngx_http_memcached_filter_init：修正从后端服务器收到的内容长度。因为在处理 header 时没有加上这部分长度。 ngx_http_memcached_filter：memcached 模块是少有的带有处理正文的回调函数的模块。因为 memcached 模块需要过滤正文末尾 CRLF “END” CRLF，所以实现了自己的 filter 回调函数。处理正文的实际意义是将从后端服务器收到的正文有效内容封装成 ngx_chain_t，并加在 u-&gt;out_bufs 末尾。Nginx 并不进行数据拷贝，而是建立 ngx_buf_t 数据结构指向这些数据内存区，然后由 ngx_chain_t 组织这些 buf。这种实现避免了内存大量搬迁，也是 Nginx 高效的奥秘之一。 本节回顾这一节介绍了 upstream 模块的基本组成。upstream 模块是从 handler 模块发展而来，指令系统和模块生效方式与 handler 模块无异。不同之处在于，upstream 模块在 handler 函数中设置众多回调函数。实际工作都是由这些回调函数完成的。每个回调函数都是在 upstream 的某个固定阶段执行，各司其职，大部分回调函数一般不会真正用到。upstream 最重要的回调函数是 create_request、process_header 和 input_filter，他们共同实现了与后端服务器的协议的解析部分。 Nginx 负载均衡模块由 小路依依 创建， 最后一次修改 2016-08-12 负载均衡模块负载均衡模块用于从upstream指令定义的后端主机列表中选取一台主机。Nginx 先使用负载均衡模块找到一台主机，再使用 upstream 模块实现与这台主机的交互。为了方便介绍负载均衡模块，做到言之有物，以下选取 Nginx 内置的 ip hash 模块作为实际例子进行分析。 配置要了解负载均衡模块的开发方法，首先需要了解负载均衡模块的使用方法。因为负载均衡模块与之前书中提到的模块差别比较大，所以我们从配置入手比较容易理解。 在配置文件中，我们如果需要使用 ip hash 的负载均衡算法。我们需要写一个类似下面的配置： 123456upstream test &#123; ip_hash; server 192.168.0.1; server 192.168.0.2;&#125; 从配置我们可以看出负载均衡模块的使用场景： 核心指令ip_hash只能在 upstream {}中使用。这条指令用于通知 Nginx 使用 ip hash 负载均衡算法。如果没加这条指令，Nginx 会使用默认的 round robin 负载均衡模块。请各位读者对比 handler 模块的配置，是不是有共同点？ upstream {}中的指令可能出现在server指令前，可能出现在server指令后，也可能出现在两条server指令之间。各位读者可能会有疑问，有什么差别么？那么请各位读者尝试下面这个配置： 12345upstream test &#123; server 192.168.0.1 weight=5; ip_hash; server 192.168.0.2 weight=7;&#125; 神奇的事情出现了： 12nginx: [emerg] invalid parameter &quot;weight=7&quot; in nginx.conf:103configuration file nginx.conf test failed 可见 ip_hash 指令的确能影响到配置的解析。 指令配置决定指令系统，现在就来看 ip_hash 的指令定义： 1234567891011static ngx_command_t ngx_http_upstream_ip_hash_commands[] = &#123; &#123; ngx_string(&quot;ip_hash&quot;), NGX_HTTP_UPS_CONF|NGX_CONF_NOARGS, ngx_http_upstream_ip_hash, 0, 0, NULL &#125;, ngx_null_command&#125;; 没有特别的东西，除了指令属性是 NGX_HTTP_UPS_CONF。这个属性表示该指令的适用范围是 upstream{}。 钩子以从前面的章节得到的经验，大家应该知道这里就是模块的切入点了。负载均衡模块的钩子代码都是有规律的，这里通过 ip_hash 模块来分析这个规律。 12345678910111213141516static char *ngx_http_upstream_ip_hash(ngx_conf_t *cf, ngx_command_t *cmd, void *conf)&#123; ngx_http_upstream_srv_conf_t *uscf; uscf = ngx_http_conf_get_module_srv_conf(cf, ngx_http_upstream_module); uscf-&gt;peer.init_upstream = ngx_http_upstream_init_ip_hash; uscf-&gt;flags = NGX_HTTP_UPSTREAM_CREATE |NGX_HTTP_UPSTREAM_MAX_FAILS |NGX_HTTP_UPSTREAM_FAIL_TIMEOUT |NGX_HTTP_UPSTREAM_DOWN; return NGX_CONF_OK;&#125; 这段代码中有两点值得我们注意。一个是 uscf-&gt;flags 的设置，另一个是设置 init_upstream 回调。 设置 uscf-&gt;flags NGX_HTTP_UPSTREAM_CREATE：创建标志，如果含有创建标志的话，Nginx 会检查重复创建，以及必要参数是否填写； NGX_HTTP_UPSTREAM_MAX_FAILS：可以在 server 中使用 max_fails 属性； NGX_HTTP_UPSTREAM_FAIL_TIMEOUT：可以在 server 中使用 fail_timeout 属性； NGX_HTTP_UPSTREAM_DOWN：可以在 server 中使用 down 属性； NGX_HTTP_UPSTREAM_WEIGHT：可以在 server 中使用 weight 属性； NGX_HTTP_UPSTREAM_BACKUP：可以在 server 中使用 backup 属性。 聪明的读者如果联想到刚刚遇到的那个神奇的配置错误，可以得出一个结论：在负载均衡模块的指令处理函数中可以设置并修改 upstream{} 中server指令支持的属性。这是一个很重要的性质，因为不同的负载均衡模块对各种属性的支持情况都是不一样的，那么就需要在解析配置文件的时候检测出是否使用了不支持的负载均衡属性并给出错误提示，这对于提升系统维护性是很有意义的。但是，这种机制也存在缺陷，正如前面的例子所示，没有机制能够追加检查在更新支持属性之前已经配置了不支持属性的server指令。 设置 init_upstream 回调Nginx 初始化 upstream 时，会在 ngx_http_upstream_init_main_conf 函数中调用设置的回调函数初始化负载均衡模块。这里不太好理解的是 uscf 的具体位置。通过下面的示意图，说明 upstream 负载均衡模块的配置的内存布局。 从图上可以看出，MAIN_CONF 中 ngx_upstream_module 模块的配置项中有一个指针数组 upstreams，数组中的每个元素对应就是配置文件中每一个 upstream{}的信息。更具体的将会在后面的原理篇讨论。 初始化配置init_upstream 回调函数执行时需要初始化负载均衡模块的配置，还要设置一个新钩子，这个钩子函数会在 Nginx 处理每个请求时作为初始化函数调用，关于这个新钩子函数的功能，后面会有详细的描述。这里，我们先分析 IP hash 模块初始化配置的代码： 12ngx_http_upstream_init_round_robin(cf, us);us-&gt;peer.init = ngx_http_upstream_init_ip_hash_peer; 这段代码非常简单：IP hash 模块首先调用另一个负载均衡模块 Round Robin 的初始化函数，然后再设置自己的处理请求阶段初始化钩子。实际上几个负载均衡模块可以组成一条链表，每次都是从链首的模块开始进行处理。如果模块决定不处理，可以将处理权交给链表中的下一个模块。这里，IP hash 模块指定 Round Robin 模块作为自己的后继负载均衡模块，所以在自己的初始化配置函数中也对 Round Robin 模块进行初始化。 初始化请求Nginx 收到一个请求以后，如果发现需要访问 upstream，就会执行对应的 peer.init 函数。这是在初始化配置时设置的回调函数。这个函数最重要的作用是构造一张表，当前请求可以使用的 upstream 服务器被依次添加到这张表中。之所以需要这张表，最重要的原因是如果 upstream 服务器出现异常，不能提供服务时，可以从这张表中取得其他服务器进行重试操作。此外，这张表也可以用于负载均衡的计算。之所以构造这张表的行为放在这里而不是在前面初始化配置的阶段，是因为upstream需要为每一个请求提供独立隔离的环境。 为了讨论 peer.init 的核心，我们还是看 IP hash 模块的实现： 12345r-&gt;upstream-&gt;peer.data = &amp;iphp-&gt;rrp;ngx_http_upstream_init_round_robin_peer(r, us);r-&gt;upstream-&gt;peer.get = ngx_http_upstream_get_ip_hash_peer; 第一行是设置数据指针，这个指针就是指向前面提到的那张表； 第二行是调用 Round Robin 模块的回调函数对该模块进行请求初始化。面前已经提到，一个负载均衡模块可以调用其他负载均衡模块以提供功能的补充。 第三行是设置一个新的回调函数get。该函数负责从表中取出某个服务器。除了 get 回调函数，还有另一个r-&gt;upstream-&gt;peer.free的回调函数。该函数在 upstream 请求完成后调用，负责做一些善后工作。比如我们需要维护一个 upstream 服务器访问计数器，那么可以在 get 函数中对其加 1，在 free 中对其减 1。如果是 SSL 的话，Nginx 还提供两个回调函数 peer.set_session 和 peer.save_session。一般来说，有两个切入点实现负载均衡算法，其一是在这里，其二是在 get 回调函数中。 peer.get 和 peer.free 回调函数这两个函数是负载均衡模块最底层的函数，负责实际获取一个连接和回收一个连接的预备操作。之所以说是预备操作，是因为在这两个函数中，并不实际进行建立连接或者释放连接的动作，而只是执行获取连接的地址或维护连接状态的操作。需要理解的清楚一点，在 peer.get 函数中获取连接的地址信息，并不代表这时连接一定没有被建立，相反的，通过 get 函数的返回值，Nginx 可以了解是否存在可用连接，连接是否已经建立。这些返回值总结如下： 返回值 说明 Nginx 后续动作 NGX_DONE 得到了连接地址信息，并且连接已经建立。 直接使用连接，发送数据。 NGX_OK 得到了连接地址信息，但连接并未建立。 建立连接，如连接不能立即建立，设置事件， 暂停执行本请求，执行别的请求。 NGX_BUSY 所有连接均不可用。 返回502错误至客户端。 各位读者看到上面这张表，可能会有几个问题浮现出来： Q: 什么时候连接是已经建立的？ A: 使用后端 keepalive 连接的时候，连接在使用完以后并不关闭，而是存放在一个队列中，新的请求只需要从队列中取出连接，这些连接都是已经准备好的。 Q: 什么叫所有连接均不可用？ A: 初始化请求的过程中，建立了一张表，get 函数负责每次从这张表中不重复的取出一个连接，当无法从表中取得一个新的连接时，即所有连接均不可用。 Q: 对于一个请求，peer.get 函数可能被调用多次么？ A: 正式如此。当某次 peer.get 函数得到的连接地址连接不上，或者请求对应的服务器得到异常响应，Nginx 会执行 ngx_http_upstream_next，然后可能再次调用 peer.get 函数尝试别的连接。upstream 整体流程如下： 本节回顾这一节介绍了负载均衡模块的基本组成。负载均衡模块的配置区集中在 upstream{}块中。负载均衡模块的回调函数体系是以 init_upstream 为起点，经历 init_peer，最终到达 peer.get 和 peer.free。其中 init_peer 负责建立每个请求使用的 server 列表，peer.get 负责从 server 列表中选择某个 server（一般是不重复选择），而 peer.free 负责 server 释放前的资源释放工作。最后，这一节通过一张图将 upstream 模块和负载均衡模块在请求处理过程中的相互关系展现出来。 Nginx core 模块由 小路依依 创建， 最后一次修改 2016-08-12 core 模块Nginx 的启动模块启动模块从启动 Nginx 进程开始，做了一系列的初始化工作，源代码位于src/core/nginx.c，从 main 函数开始: 时间、正则、错误日志、ssl 等初始化 读入命令行参数 OS 相关初始化 读入并解析配置 核心模块初始化 创建各种暂时文件和目录 创建共享内存 打开 listen 的端口 所有模块初始化 启动 worker 进程 Nginx event 模块由 小路依依 创建， 最后一次修改 2016-08-12 event 模块event 的类型和功能Nginx 是以 event（事件）处理模型为基础的模块。它为了支持跨平台，抽象出了 event 模块。它支持的 event 处理类型有：AIO（异步IO），/dev/poll（Solaris 和 Unix 特有），epoll（Linux 特有），eventport（Solaris 10 特有），kqueue（BSD 特有），poll，rtsig（实时信号），select 等。 event 模块的主要功能就是，监听 accept 后建立的连接，对读写事件进行添加删除。事件处理模型和 Nginx 的非阻塞 IO 模型结合在一起使用。当 IO 可读可写的时候，相应的读写事件就会被唤醒，此时就会去处理事件的回调函数。 特别对于 Linux，Nginx 大部分 event 采用 epoll EPOLLET（边沿触发）的方法来触发事件，只有 listen 端口的读事件是 EPOLLLT（水平触发）。对于边沿触发，如果出现了可读事件，必须及时处理，否则可能会出现读事件不再触发，连接饿死的情况。 1234567891011121314151617181920typedef struct &#123; /* 添加删除事件 */ ngx_int_t (*add)(ngx_event_t *ev, ngx_int_t event, ngx_uint_t flags); ngx_int_t (*del)(ngx_event_t *ev, ngx_int_t event, ngx_uint_t flags); ngx_int_t (*enable)(ngx_event_t *ev, ngx_int_t event, ngx_uint_t flags); ngx_int_t (*disable)(ngx_event_t *ev, ngx_int_t event, ngx_uint_t flags); /* 添加删除连接，会同时监听读写事件 */ ngx_int_t (*add_conn)(ngx_connection_t *c); ngx_int_t (*del_conn)(ngx_connection_t *c, ngx_uint_t flags); ngx_int_t (*process_changes)(ngx_cycle_t *cycle, ngx_uint_t nowait); /* 处理事件的函数 */ ngx_int_t (*process_events)(ngx_cycle_t *cycle, ngx_msec_t timer, ngx_uint_t flags); ngx_int_t (*init)(ngx_cycle_t *cycle, ngx_msec_t timer); void (*done)(ngx_cycle_t *cycle);&#125; ngx_event_actions_t; 上述是 event 处理抽象出来的关键结构体，可以看到，每个 event 处理模型，都需要实现部分功能。最关键的是 add 和 del 功能，就是最基本的添加和删除事件的函数。 accept 锁Nginx 是多进程程序，80 端口是各进程所共享的，多进程同时 listen 80 端口，势必会产生竞争，也产生了所谓的“惊群”效应。当内核 accept 一个连接时，会唤醒所有等待中的进程，但实际上只有一个进程能获取连接，其他的进程都是被无效唤醒的。所以 Nginx 采用了自有的一套 accept 加锁机制，避免多个进程同时调用 accept。Nginx 多进程的锁在底层默认是通过 CPU 自旋锁来实现。如果操作系统不支持自旋锁，就采用文件锁。 Nginx 事件处理的入口函数是 ngx_process_events_and_timers()，下面是部分代码，可以看到其加锁的过程： 123456789101112131415161718192021if (ngx_use_accept_mutex) &#123; if (ngx_accept_disabled &gt; 0) &#123; ngx_accept_disabled--; &#125; else &#123; if (ngx_trylock_accept_mutex(cycle) == NGX_ERROR) &#123; return; &#125; if (ngx_accept_mutex_held) &#123; flags |= NGX_POST_EVENTS; &#125; else &#123; if (timer == NGX_TIMER_INFINITE || timer &gt; ngx_accept_mutex_delay) &#123; timer = ngx_accept_mutex_delay; &#125; &#125; &#125;&#125; 在 ngx_trylock_accept_mutex()函数里面，如果拿到了锁，Nginx 会把 listen 的端口读事件加入 event 处理，该进程在有新连接进来时就可以进行 accept 了。注意 accept 操作是一个普通的读事件。下面的代码说明了这点： 123456789(void) ngx_process_events(cycle, timer, flags);if (ngx_posted_accept_events) &#123; ngx_event_process_posted(cycle, &amp;ngx_posted_accept_events);&#125;if (ngx_accept_mutex_held) &#123; ngx_shmtx_unlock(&amp;ngx_accept_mutex);&#125; ngx_process_events()函数是所有事件处理的入口，它会遍历所有的事件。抢到了 accept 锁的进程跟一般进程稍微不同的是，它被加上了 NGX_POST_EVENTS 标志，也就是说在 ngx_process_events() 函数里面只接受而不处理事件，并加入 post_events 的队列里面。直到 ngx_accept_mutex 锁去掉以后才去处理具体的事件。为什么这样？因为 ngx_accept_mutex 是全局锁，这样做可以尽量减少该进程抢到锁以后，从 accept 开始到结束的时间，以便其他进程继续接收新的连接，提高吞吐量。 ngx_posted_accept_events 和 ngx_posted_events 就分别是 accept 延迟事件队列和普通延迟事件队列。可以看到 ngx_posted_accept_events 还是放到 ngx_accept_mutex 锁里面处理的。该队列里面处理的都是 accept 事件，它会一口气把内核 backlog 里等待的连接都 accept 进来，注册到读写事件里。 而 ngx_posted_events 是普通的延迟事件队列。一般情况下，什么样的事件会放到这个普通延迟队列里面呢？我的理解是，那些 CPU 耗时比较多的都可以放进去。因为 Nginx 事件处理都是根据触发顺序在一个大循环里依次处理的，因为 Nginx 一个进程同时只能处理一个事件，所以有些耗时多的事件会把后面所有事件的处理都耽搁了。 除了加锁，Nginx 也对各进程的请求处理的均衡性作了优化，也就是说，如果在负载高的时候，进程抢到的锁过多，会导致这个进程被禁止接受请求一段时间。 比如，在 ngx_event_accept 函数中，有类似代码： 12ngx_accept_disabled = ngx_cycle-&gt;connection_n / 8 - ngx_cycle-&gt;free_connection_n; ngx_cycle-&gt;connection_n 是进程可以分配的连接总数，ngx_cycle-&gt;free_connection_n 是空闲的进程数。上述等式说明了，当前进程的空闲进程数小于 1/8 的话，就会被禁止 accept 一段时间。 定时器Nginx 在需要用到超时的时候，都会用到定时器机制。比如，建立连接以后的那些读写超时。Nginx 使用红黑树来构造定期器，红黑树是一种有序的二叉平衡树，其查找插入和删除的复杂度都为 O(logn)，所以是一种比较理想的二叉树。 定时器的机制就是，二叉树的值是其超时时间，每次查找二叉树的最小值，如果最小值已经过期，就删除该节点，然后继续查找，直到所有超时节点都被删除。]]></content>
      <categories>
        <category>-linux</category>
      </categories>
      <tags>
        <tag>-nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx 配置文件nginx.conf中文详解]]></title>
    <url>%2F2018%2F02%2F14%2Fnginx1%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332######Nginx配置文件nginx.conf中文详解######定义Nginx运行的用户和用户组user www www;#nginx进程数，建议设置为等于CPU总核心数。worker_processes 8; #全局错误日志定义类型，[ debug | info | notice | warn | error | crit ]error_log /usr/local/nginx/logs/error.log info;#进程pid文件pid /usr/local/nginx/logs/nginx.pid;#指定进程可以打开的最大描述符：数目#工作模式与连接数上限#这个指令是指当一个nginx进程打开的最多文件描述符数目，理论值应该是最多打开文件数（ulimit -n）与nginx进程数相除，但是nginx分配请求并不是那么均匀，所以最好与ulimit -n 的值保持一致。#现在在linux 2.6内核下开启文件打开数为65535，worker_rlimit_nofile就相应应该填写65535。#这是因为nginx调度时分配请求到进程并不是那么的均衡，所以假如填写10240，总并发量达到3-4万时就有进程可能超过10240了，这时会返回502错误。worker_rlimit_nofile 65535;events&#123; #参考事件模型，use [ kqueue | rtsig | epoll | /dev/poll | select | poll ]; epoll模型 #是Linux 2.6以上版本内核中的高性能网络I/O模型，linux建议epoll，如果跑在FreeBSD上面，就用kqueue模型。 #补充说明： #与apache相类，nginx针对不同的操作系统，有不同的事件模型 #A）标准事件模型 #Select、poll属于标准事件模型，如果当前系统不存在更有效的方法，nginx会选择select或poll #B）高效事件模型 #Kqueue：使用于FreeBSD 4.1+, OpenBSD 2.9+, NetBSD 2.0 和 MacOS X.使用双处理器的MacOS X系统使用kqueue可能会造成内核崩溃。 #Epoll：使用于Linux内核2.6版本及以后的系统。 #/dev/poll：使用于Solaris 7 11/99+，HP/UX 11.22+ (eventport)，IRIX 6.5.15+ 和 Tru64 UNIX 5.1A+。 #Eventport：使用于Solaris 10。 为了防止出现内核崩溃的问题， 有必要安装安全补丁。 use epoll; #单个进程最大连接数（最大连接数=连接数*进程数） #根据硬件调整，和前面工作进程配合起来用，尽量大，但是别把cpu跑到100%就行。每个进程允许的最多连接数，理论上每台nginx服务器的最大连接数为。 worker_connections 65535; #keepalive超时时间。 keepalive_timeout 60; #客户端请求头部的缓冲区大小。这个可以根据你的系统分页大小来设置，一般一个请求头的大小不会超过1k，不过由于一般系统分页都要大于1k，所以这里设置为分页大小。 #分页大小可以用命令getconf PAGESIZE 取得。 #[root@web001 ~]# getconf PAGESIZE #4096 #但也有client_header_buffer_size超过4k的情况，但是client_header_buffer_size该值必须设置为“系统分页大小”的整倍数。 client_header_buffer_size 4k; #这个将为打开文件指定缓存，默认是没有启用的，max指定缓存数量，建议和打开文件数一致，inactive是指经过多长时间文件没被请求后删除缓存。 open_file_cache max=65535 inactive=60s; #这个是指多长时间检查一次缓存的有效信息。 #语法:open_file_cache_valid time 默认值:open_file_cache_valid 60 使用字段:http, server, location 这个指令指定了何时需要检查open_file_cache中缓存项目的有效信息. open_file_cache_valid 80s; #open_file_cache指令中的inactive参数时间内文件的最少使用次数，如果超过这个数字，文件描述符一直是在缓存中打开的，如上例，如果有一个文件在inactive时间内一次没被使用，它将被移除。 #语法:open_file_cache_min_uses number 默认值:open_file_cache_min_uses 1 使用字段:http, server, location 这个指令指定了在open_file_cache指令无效的参数中一定的时间范围内可以使用的最小文件数,如果使用更大的值,文件描述符在cache中总是打开状态. open_file_cache_min_uses 1; #语法:open_file_cache_errors on | off 默认值:open_file_cache_errors off 使用字段:http, server, location 这个指令指定是否在搜索一个文件时记录cache错误. open_file_cache_errors on;&#125; #设定http服务器，利用它的反向代理功能提供负载均衡支持http&#123; #文件扩展名与文件类型映射表 include mime.types; #默认文件类型 default_type application/octet-stream; #默认编码 #charset utf-8; #服务器名字的hash表大小 #保存服务器名字的hash表是由指令server_names_hash_max_size 和server_names_hash_bucket_size所控制的。参数hash bucket size总是等于hash表的大小，并且是一路处理器缓存大小的倍数。在减少了在内存中的存取次数后，使在处理器中加速查找hash表键值成为可能。如果hash bucket size等于一路处理器缓存的大小，那么在查找键的时候，最坏的情况下在内存中查找的次数为2。第一次是确定存储单元的地址，第二次是在存储单元中查找键 值。因此，如果Nginx给出需要增大hash max size 或 hash bucket size的提示，那么首要的是增大前一个参数的大小. server_names_hash_bucket_size 128; #客户端请求头部的缓冲区大小。这个可以根据你的系统分页大小来设置，一般一个请求的头部大小不会超过1k，不过由于一般系统分页都要大于1k，所以这里设置为分页大小。分页大小可以用命令getconf PAGESIZE取得。 client_header_buffer_size 32k; #客户请求头缓冲大小。nginx默认会用client_header_buffer_size这个buffer来读取header值，如果header过大，它会使用large_client_header_buffers来读取。 large_client_header_buffers 4 64k; #设定通过nginx上传文件的大小 client_max_body_size 8m; #开启高效文件传输模式，sendfile指令指定nginx是否调用sendfile函数来输出文件，对于普通应用设为 on，如果用来进行下载等应用磁盘IO重负载应用，可设置为off，以平衡磁盘与网络I/O处理速度，降低系统的负载。注意：如果图片显示不正常把这个改成off。 #sendfile指令指定 nginx 是否调用sendfile 函数（zero copy 方式）来输出文件，对于普通应用，必须设为on。如果用来进行下载等应用磁盘IO重负载应用，可设置为off，以平衡磁盘与网络IO处理速度，降低系统uptime。 sendfile on; #开启目录列表访问，合适下载服务器，默认关闭。 autoindex on; #此选项允许或禁止使用socke的TCP_CORK的选项，此选项仅在使用sendfile的时候使用 tcp_nopush on; tcp_nodelay on; #长连接超时时间，单位是秒 keepalive_timeout 120; #FastCGI相关参数是为了改善网站的性能：减少资源占用，提高访问速度。下面参数看字面意思都能理解。 fastcgi_connect_timeout 300; fastcgi_send_timeout 300; fastcgi_read_timeout 300; fastcgi_buffer_size 64k; fastcgi_buffers 4 64k; fastcgi_busy_buffers_size 128k; fastcgi_temp_file_write_size 128k; #gzip模块设置 gzip on; #开启gzip压缩输出 gzip_min_length 1k; #最小压缩文件大小 gzip_buffers 4 16k; #压缩缓冲区 gzip_http_version 1.0; #压缩版本（默认1.1，前端如果是squid2.5请使用1.0） gzip_comp_level 2; #压缩等级 gzip_types text/plain application/x-javascript text/css application/xml; #压缩类型，默认就已经包含textml，所以下面就不用再写了，写上去也不会有问题，但是会有一个warn。 gzip_vary on; #开启限制IP连接数的时候需要使用 #limit_zone crawler $binary_remote_addr 10m; #负载均衡配置 upstream jh.w3cschool.cn &#123; #upstream的负载均衡，weight是权重，可以根据机器配置定义权重。weigth参数表示权值，权值越高被分配到的几率越大。 server 192.168.80.121:80 weight=3; server 192.168.80.122:80 weight=2; server 192.168.80.123:80 weight=3; #nginx的upstream目前支持4种方式的分配 #1、轮询（默认） #每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器down掉，能自动剔除。 #2、weight #指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况。 #例如： #upstream bakend &#123; # server 192.168.0.14 weight=10; # server 192.168.0.15 weight=10; #&#125; #2、ip_hash #每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器，可以解决session的问题。 #例如： #upstream bakend &#123; # ip_hash; # server 192.168.0.14:88; # server 192.168.0.15:80; #&#125; #3、fair（第三方） #按后端服务器的响应时间来分配请求，响应时间短的优先分配。 #upstream backend &#123; # server server1; # server server2; # fair; #&#125; #4、url_hash（第三方） #按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，后端服务器为缓存时比较有效。 #例：在upstream中加入hash语句，server语句中不能写入weight等其他的参数，hash_method是使用的hash算法 #upstream backend &#123; # server squid1:3128; # server squid2:3128; # hash $request_uri; # hash_method crc32; #&#125; #tips: #upstream bakend&#123;#定义负载均衡设备的Ip及设备状态&#125;&#123; # ip_hash; # server 127.0.0.1:9090 down; # server 127.0.0.1:8080 weight=2; # server 127.0.0.1:6060; # server 127.0.0.1:7070 backup; #&#125; #在需要使用负载均衡的server中增加 proxy_pass http://bakend/; #每个设备的状态设置为: #1.down表示单前的server暂时不参与负载 #2.weight为weight越大，负载的权重就越大。 #3.max_fails：允许请求失败的次数默认为1.当超过最大次数时，返回proxy_next_upstream模块定义的错误 #4.fail_timeout:max_fails次失败后，暂停的时间。 #5.backup： 其它所有的非backup机器down或者忙的时候，请求backup机器。所以这台机器压力会最轻。 #nginx支持同时设置多组的负载均衡，用来给不用的server来使用。 #client_body_in_file_only设置为On 可以讲client post过来的数据记录到文件中用来做debug #client_body_temp_path设置记录文件的目录 可以设置最多3层目录 #location对URL进行匹配.可以进行重定向或者进行新的代理 负载均衡 &#125; #虚拟主机的配置 server &#123; #监听端口 listen 80; #域名可以有多个，用空格隔开 server_name www.w3cschool.cn w3cschool.cn; index index.html index.htm index.php; root /data/www/w3cschool; #对******进行负载均衡 location ~ .*.(php|php5)?$ &#123; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; include fastcgi.conf; &#125; #图片缓存时间设置 location ~ .*.(gif|jpg|jpeg|png|bmp|swf)$ &#123; expires 10d; &#125; #JS和CSS缓存时间设置 location ~ .*.(js|css)?$ &#123; expires 1h; &#125; #日志格式设定 #$remote_addr与$http_x_forwarded_for用以记录客户端的ip地址； #$remote_user：用来记录客户端用户名称； #$time_local： 用来记录访问时间与时区； #$request： 用来记录请求的url与http协议； #$status： 用来记录请求状态；成功是200， #$body_bytes_sent ：记录发送给客户端文件主体内容大小； #$http_referer：用来记录从那个页面链接访问过来的； #$http_user_agent：记录客户浏览器的相关信息； #通常web服务器放在反向代理的后面，这样就不能获取到客户的IP地址了，通过$remote_add拿到的IP地址是反向代理服务器的iP地址。反向代理服务器在转发请求的http头信息中，可以增加x_forwarded_for信息，用以记录原有客户端的IP地址和原来客户端的请求的服务器地址。 log_format access '$remote_addr - $remote_user [$time_local] "$request" ' '$status $body_bytes_sent "$http_referer" ' '"$http_user_agent" $http_x_forwarded_for'; #定义本虚拟主机的访问日志 access_log /usr/local/nginx/logs/host.access.log main; access_log /usr/local/nginx/logs/host.access.404.log log404; #对 "/" 启用反向代理 location / &#123; proxy_pass http://127.0.0.1:88; proxy_redirect off; proxy_set_header X-Real-IP $remote_addr; #后端的Web服务器可以通过X-Forwarded-For获取用户真实IP proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; #以下是一些反向代理的配置，可选。 proxy_set_header Host $host; #允许客户端请求的最大单文件字节数 client_max_body_size 10m; #缓冲区代理缓冲用户端请求的最大字节数， #如果把它设置为比较大的数值，例如256k，那么，无论使用firefox还是IE浏览器，来提交任意小于256k的图片，都很正常。如果注释该指令，使用默认的client_body_buffer_size设置，也就是操作系统页面大小的两倍，8k或者16k，问题就出现了。 #无论使用firefox4.0还是IE8.0，提交一个比较大，200k左右的图片，都返回500 Internal Server Error错误 client_body_buffer_size 128k; #表示使nginx阻止HTTP应答代码为400或者更高的应答。 proxy_intercept_errors on; #后端服务器连接的超时时间_发起握手等候响应超时时间 #nginx跟后端服务器连接超时时间(代理连接超时) proxy_connect_timeout 90; #后端服务器数据回传时间(代理发送超时) #后端服务器数据回传时间_就是在规定时间之内后端服务器必须传完所有的数据 proxy_send_timeout 90; #连接成功后，后端服务器响应时间(代理接收超时) #连接成功后_等候后端服务器响应时间_其实已经进入后端的排队之中等候处理（也可以说是后端服务器处理请求的时间） proxy_read_timeout 90; #设置代理服务器（nginx）保存用户头信息的缓冲区大小 #设置从被代理服务器读取的第一部分应答的缓冲区大小，通常情况下这部分应答中包含一个小的应答头，默认情况下这个值的大小为指令proxy_buffers中指定的一个缓冲区的大小，不过可以将其设置为更小 proxy_buffer_size 4k; #proxy_buffers缓冲区，网页平均在32k以下的设置 #设置用于读取应答（来自被代理服务器）的缓冲区数目和大小，默认情况也为分页大小，根据操作系统的不同可能是4k或者8k proxy_buffers 4 32k; #高负荷下缓冲大小（proxy_buffers*2） proxy_busy_buffers_size 64k; #设置在写入proxy_temp_path时数据的大小，预防一个工作进程在传递文件时阻塞太长 #设定缓存文件夹大小，大于这个值，将从upstream服务器传 proxy_temp_file_write_size 64k; &#125; #设定查看Nginx状态的地址 location /NginxStatus &#123; stub_status on; access_log on; auth_basic "NginxStatus"; auth_basic_user_file confpasswd; #htpasswd文件的内容可以用apache提供的htpasswd工具来产生。 &#125; #本地动静分离反向代理配置 #所有jsp的页面均交由tomcat或resin处理 location ~ .(jsp|jspx|do)?$ &#123; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://127.0.0.1:8080; &#125; #所有静态文件由nginx直接读取不经过tomcat或resin location ~ .*.(htm|html|gif|jpg|jpeg|png|bmp|swf|ioc|rar|zip|txt|flv|mid|doc|ppt| pdf|xls|mp3|wma)$ &#123; expires 15d; &#125; location ~ .*.(js|css)?$ &#123; expires 1h; &#125; &#125;&#125;######Nginx配置文件nginx.conf中文详解#####]]></content>
      <categories>
        <category>-nginx</category>
      </categories>
      <tags>
        <tag>-nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell教程]]></title>
    <url>%2F2018%2F02%2F14%2Fshell%2F</url>
    <content type="text"><![CDATA[Shell 脚本Shell 脚本（shell script），是一种为shell编写的脚本程序。 业界所说的shell通常都是指shell脚本，但读者朋友要知道，shell和shell script是两个不同的概念。 由于习惯的原因，简洁起见，本文出现的”shell编程”都是指shell脚本编程，不是指开发shell自身。 Shell 环境Shell 编程跟java、php编程一样，只要有一个能编写代码的文本编辑器和一个能解释执行的脚本解释器就可以了。 Linux的Shell种类众多，常见的有： Bourne Shell（/usr/bin/sh或/bin/sh） Bourne Again Shell（/bin/bash） C Shell（/usr/bin/csh） K Shell（/usr/bin/ksh） Shell for Root（/sbin/sh） …… 本教程关注的是 Bash，也就是 Bourne Again Shell，由于易用和免费，Bash在日常工作中被广泛使用。同时，Bash也是大多数Linux系统默认的Shell。 在一般情况下，人们并不区分 Bourne Shell 和 Bourne Again Shell，所以，像 #!/bin/sh，它同样也可以改为#!/bin/bash。 #!告诉系统其后路径所指定的程序即是解释此脚本文件的Shell程序。 第一个shell脚本打开文本编辑器(可以使用vi/vim命令来创建文件)，新建一个文件test.sh，扩展名为sh（sh代表shell），扩展名并不影响脚本执行，见名知意就好，如果你用php写shell 脚本，扩展名就用php好了。 输入一些代码，第一行一般是这样： 实例12#!/bin/bashecho "Hello World !" “#!” 是一个约定的标记，它告诉系统这个脚本需要什么解释器来执行，即使用哪一种Shell。 echo命令用于向窗口输出文本。 运行Shell脚本有两种方法：1、作为可执行程序将上面的代码保存为test.sh，并cd到相应目录： 12chmod +x ./test.sh #使脚本具有执行权限./test.sh #执行脚本 注意，一定要写成./test.sh，而不是test.sh，运行其它二进制的程序也一样，直接写test.sh，linux系统会去PATH里寻找有没有叫test.sh的，而只有/bin, /sbin, /usr/bin，/usr/sbin等在PATH里，你的当前目录通常不在PATH里，所以写成test.sh是会找不到命令的，要用./test.sh告诉系统说，就在当前目录找。 2、作为解释器参数这种运行方式是，直接运行解释器，其参数就是shell脚本的文件名，如： 12/bin/sh test.sh/bin/php test.php 这种方式运行的脚本，不需要在第一行指定解释器信息，写了也没用。 Shell 变量Shell 变量定义变量时，变量名不加美元符号（$，PHP语言中变量需要），如： 1your_name="123" 注意，变量名和等号之间不能有空格，这可能和你熟悉的所有编程语言都不一样。同时，变量名的命名须遵循如下规则： 首个字符必须为字母（a-z，A-Z）。 中间不能有空格，可以使用下划线（_）。 不能使用标点符号。 不能使用bash里的关键字（可用help命令查看保留关键字）。 除了显式地直接赋值，还可以用语句给变量赋值，如： 1for file in `ls /etc` 以上语句将 /etc 下目录的文件名循环出来。 使用变量使用一个定义过的变量，只要在变量名前面加美元符号即可，如： 123your_name="qinjx"echo $your_nameecho $&#123;your_name&#125; 变量名外面的花括号是可选的，加不加都行，加花括号是为了帮助解释器识别变量的边界，比如下面这种情况： 123for skill in Ada Coffe Action Java do echo &quot;I am good at $&#123;skill&#125;Script&quot;done 如果不给skill变量加花括号，写成echo “I am good at $skillScript”，解释器就会把$skillScript当成一个变量（其值为空），代码执行结果就不是我们期望的样子了。 推荐给所有变量加上花括号，这是个好的编程习惯。 已定义的变量，可以被重新定义，如： 1234your_name="tom"echo $your_nameyour_name="alibaba"echo $your_name 这样写是合法的，但注意，第二次赋值的时候不能写$your_name=”alibaba”，使用变量的时候才加美元符（$）。 Shell 字符串字符串是shell编程中最常用最有用的数据类型（除了数字和字符串，也没啥其它类型好用了），字符串可以用单引号，也可以用双引号，也可以不用引号。单双引号的区别跟PHP类似。 单引号1str='this is a string' 单引号字符串的限制： 单引号里的任何字符都会原样输出，单引号字符串中的变量是无效的； 单引号字串中不能出现单引号（对单引号使用转义符后也不行）。 双引号12your_name='qinjx'str="Hello, I know your are \"$your_name\"! \n" 双引号的优点： 双引号里可以有变量 双引号里可以出现转义字符 拼接字符串1234your_name="qinjx"greeting="hello, "$your_name" !"greeting_1="hello, $&#123;your_name&#125; !"echo $greeting $greeting_1 获取字符串长度12string="abcd"echo $&#123;#string&#125; #输出 4 提取子字符串12string="alibaba is a great company"echo $&#123;string:1:4&#125; #输出liba 查找子字符串12string="alibaba is a great company"echo `expr index "$string" is` 注意： 以上脚本中 “`” 是反引号，而不是单引号 “‘“，不要看错了哦。 Shell 数组bash支持一维数组（不支持多维数组），并且没有限定数组的大小。 类似与C语言，数组元素的下标由0开始编号。获取数组中的元素要利用下标，下标可以是整数或算术表达式，其值应大于或等于0。 定义数组在Shell中，用括号来表示数组，数组元素用”空格”符号分割开。定义数组的一般形式为： 1数组名=(值1 值2 ... 值n) 例如： 1array_name=(value0 value1 value2 value3) 或者 123456array_name=(value0value1value2value3) 还可以单独定义数组的各个分量： 123array_name[0]=value0array_name[1]=value1array_name[n]=valuen 可以不使用连续的下标，而且下标的范围没有限制。 读取数组读取数组元素值的一般格式是： 1$&#123;数组名[下标]&#125; 例如： 1valuen=$&#123;array_name[n]&#125; 使用@符号可以获取数组中的所有元素，例如： 1echo $&#123;array_name[@]&#125; 获取数组的长度获取数组长度的方法与获取字符串长度的方法相同，例如： 123456# 取得数组元素的个数length=$&#123;#array_name[@]&#125;# 或者length=$&#123;#array_name[*]&#125;# 取得数组单个元素的长度lengthn=$&#123;#array_name[n]&#125; Shell 注释以”#”开头的行就是注释，会被解释器忽略。 sh里没有多行注释，只能每一行加一个#号。只能像这样： 12345678910111213#--------------------------------------------# 这是一个自动打ipa的脚本，基于webfrogs的ipa-build书写：# https://github.com/webfrogs/xcode_shell/blob/master/ipa-build# 功能：自动为etao ios app打包，产出物为14个渠道的ipa包# 特色：全自动打包，不需要输入任何参数#--------------------------------------------##### 用户配置区 开始 ######## 项目根目录，推荐将此脚本放在项目的根目录，这里就不用改了# 应用名，确保和Xcode里Product下的target_name.app名字一致###### 用户配置区 结束 ##### 如果在开发过程中，遇到大段的代码需要临时注释起来，过一会儿又取消注释，怎么办呢？ 每一行加个#符号太费力了，可以把这一段要注释的代码用一对花括号括起来，定义成一个函数，没有地方调用这个函数，这块代码就不会执行，达到了和注释一样的效果。 Shell 传递参数Shell 传递参数我们可以在执行 Shell 脚本时，向脚本传递参数，脚本内获取参数的格式为：$n。n 代表一个数字，1 为执行脚本的第一个参数，2 为执行脚本的第二个参数，以此类推…… 实例以下实例我们向脚本传递三个参数，并分别输出，其中 $0 为执行的文件名： 123456789#!/bin/bash# author:W3Cschool教程# url:www.123echo "Shell 传递参数实例！";echo "执行的文件名：$0";echo "第一个参数为：$1";echo "第二个参数为：$2";echo "第三个参数为：$3"; 为脚本设置可执行权限，并执行脚本，输出结果如下所示： 1234567$ chmod +x test.sh $ ./test.sh 1 2 3Shell 传递参数实例！执行的文件名：test.sh第一个参数为：1第二个参数为：2第三个参数为：3 另外，还有几个特殊字符用来处理参数： 参数处理 说明 $# 传递到脚本的参数个数 $* 以一个单字符串显示所有向脚本传递的参数。 如”$*”用「”」括起来的情况、以”$1 $2 … $n”的形式输出所有参数。 $$ 脚本运行的当前进程ID号 $! 后台运行的最后一个进程的ID号 $@ 与$*相同，但是使用时加引号，并在引号中返回每个参数。 如”$@”用「”」括起来的情况、以”$1” “$2” … “$n” 的形式输出所有参数。 $- 显示Shell使用的当前选项，与set命令功能相同。 $? 显示最后命令的退出状态。0表示没有错误，其他任何值表明有错误。 123456789#!/bin/bash# author:W3Cschool教程# url:www.123echo "Shell 传递参数实例！";echo "第一个参数为：$1";echo "参数个数为：$#";echo "传递的参数作为一个字符串显示：$*"; 执行脚本，输出结果如下所示： 123456$ chmod +x test.sh $ ./test.sh 1 2 3Shell 传递参数实例！第一个参数为：1参数个数为：3传递的参数作为一个字符串显示：1 2 3 $* 与 $@ 区别： 相同点：都是引用所有参数。 不同点：只有在双引号中体现出来。假设在脚本运行时写了三个参数 1、2、3，，则 “ * “ 等价于 “1 2 3”（传递了一个参数），而 “@” 等价于 “1” “2” “3”（传递了三个参数）。 12345678910111213#!/bin/bash# author:W3Cschool教程# url:www.123echo "-- \$* 演示 ---"for i in "$*"; do echo $idoneecho "-- \$@ 演示 ---"for i in "$@"; do echo $idone 执行脚本，输出结果如下所示： 12345678$ chmod +x test.sh $ ./test.sh 1 2 3-- $* 演示 ---1 2 3-- $@ 演示 ---123 Shell 数组Shell 数组数组中可以存放多个值。Bash Shell 只支持一维数组（不支持多维数组），初始化时不需要定义数组大小（与 PHP 类似）。 与大部分编程语言类似，数组元素的下标由0开始。 Shell 数组用括号来表示，元素用”空格”符号分割开，语法格式如下： 1array_name=(value1 ... valuen) 实例12345#!/bin/bash# author:W3Cschool教程# url:www.123my_array=(A B "C" D) 我们也可以使用下标来定义数组: 123array_name[0]=value0array_name[1]=value1array_name[2]=value2 读取数组读取数组元素值的一般格式是： 1$&#123;array_name[index]&#125; 实例12345678910#!/bin/bash# author:W3Cschool教程# url:www.123my_array=(A B "C" D)echo "第一个元素为: $&#123;my_array[0]&#125;"echo "第二个元素为: $&#123;my_array[1]&#125;"echo "第三个元素为: $&#123;my_array[2]&#125;"echo "第四个元素为: $&#123;my_array[3]&#125;" 执行脚本，输出结果如下所示： 123456$ chmod +x test.sh $ ./test.sh第一个元素为: A第二个元素为: B第三个元素为: C第四个元素为: D 获取数组中的所有元素使用@ 或 * 可以获取数组中的所有元素，例如： 1234567891011#!/bin/bash# author:W3Cschool教程# url:www.123my_array[0]=Amy_array[1]=Bmy_array[2]=Cmy_array[3]=Decho "数组的元素为: $&#123;my_array[*]&#125;"echo "数组的元素为: $&#123;my_array[@]&#125;" 执行脚本，输出结果如下所示： 1234$ chmod +x test.sh $ ./test.sh数组的元素为: A B C D数组的元素为: A B C D 获取数组的长度获取数组长度的方法与获取字符串长度的方法相同，例如： 1234567891011#!/bin/bash# author:W3Cschool教程# url:www.123my_array[0]=Amy_array[1]=Bmy_array[2]=Cmy_array[3]=Decho "数组元素个数为: $&#123;#my_array[*]&#125;"echo "数组元素个数为: $&#123;#my_array[@]&#125;" 执行脚本，输出结果如下所示： 1234$ chmod +x test.sh $ ./test.sh数组元素个数为: 4数组元素个数为: 4 Shell 运算符Shell 基本运算符Shell 和其他编程语言一样，支持多种运算符，包括： 算数运算符 关系运算符 布尔运算符 字符串运算符 文件测试运算符 expr 是一款表达式计算工具，使用它能完成表达式的求值操作。 例如，两个数相加(注意使用的是反引号 ` 而不是单引号 ‘)： 1234#!/bin/bashval=`expr 2 + 2`echo "两数之和为 : $val" 运行实例 » 执行脚本，输出结果如下所示： 1两数之和为 : 4 两点注意： 表达式和运算符之间要有空格，例如 2+2 是不对的，必须写成 2 + 2，这与我们熟悉的大多数编程语言不一样。 完整的表达式要被 包含，注意这个字符不是常用的单引号，在 Esc 键下边。 算术运算符下表列出了常用的算术运算符，假定变量 a 为 10，变量 b 为 20： 运算符 说明 举例 + 加法 expr $a + $b 结果为 30。 - 减法 expr $a - $b 结果为 -10。 * 乘法 expr $a \* $b 结果为 200。 / 除法 expr $b / $a 结果为 2。 % 取余 expr $b % $a 结果为 0。 = 赋值 a=$b 将把变量 b 的值赋给 a。 == 相等。用于比较两个数字，相同则返回 true。 [ $a == $b ] 返回 false。 != 不相等。用于比较两个数字，不相同则返回 true。 [ $a != $b ] 返回 true。 注意：条件表达式要放在方括号之间，并且要有空格，例如: [$a==$b] 是错误的，必须写成 [ $a == $b ]。 实例算术运算符实例如下： 123456789101112131415161718192021222324252627282930#!/bin/bash# author:W3Cschool教程# url:www.123a=10b=20val=`expr $a + $b`echo "a + b : $val"val=`expr $a - $b`echo "a - b : $val"val=`expr $a \* $b`echo "a * b : $val"val=`expr $b / $a`echo "b / a : $val"val=`expr $b % $a`echo "b % a : $val"if [ $a == $b ]then echo "a 等于 b"fiif [ $a != $b ]then echo "a 不等于 b"fi 执行脚本，输出结果如下所示： 123456a + b : 30a - b : -10a * b : 200b / a : 2b % a : 0a 不等于 b 注意：乘号()前边必须加反斜杠()才能实现乘法运算；if…then…fi 是条件语句，后续将会讲解。在 MAC 中 shell 的 expr 语法是：$((表达式))，此处表达式中的 ““ 不需要转义符号 “\” 。 关系运算符关系运算符只支持数字，不支持字符串，除非字符串的值是数字。 下表列出了常用的关系运算符，假定变量 a 为 10，变量 b 为 20： 运算符 说明 举例 -eq 检测两个数是否相等，相等返回 true。 [ $a -eq $b ] 返回 false。 -ne 检测两个数是否相等，不相等返回 true。 [ $a -ne $b ] 返回 true。 -gt 检测左边的数是否大于右边的，如果是，则返回 true。 [ $a -gt $b ] 返回 false。 -lt 检测左边的数是否小于右边的，如果是，则返回 true。 [ $a -lt $b ] 返回 true。 -ge 检测左边的数是否大于等于右边的，如果是，则返回 true。 [ $a -ge $b ] 返回 false。 -le 检测左边的数是否小于等于右边的，如果是，则返回 true。 [ $a -le $b ] 返回 true。 实例关系运算符实例如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243#!/bin/bash# author:W3Cschool教程# url:www.123a=10b=20if [ $a -eq $b ]then echo "$a -eq $b : a 等于 b"else echo "$a -eq $b: a 不等于 b"fiif [ $a -ne $b ]then echo "$a -ne $b: a 不等于 b"else echo "$a -ne $b : a 等于 b"fiif [ $a -gt $b ]then echo "$a -gt $b: a 大于 b"else echo "$a -gt $b: a 不大于 b"fiif [ $a -lt $b ]then echo "$a -lt $b: a 小于 b"else echo "$a -lt $b: a 不小于 b"fiif [ $a -ge $b ]then echo "$a -ge $b: a 大于或等于 b"else echo "$a -ge $b: a 小于 b"fiif [ $a -le $b ]then echo "$a -le $b: a 小于或等于 b"else echo "$a -le $b: a 大于 b"fi 执行脚本，输出结果如下所示： 12345610 -eq 20: a 不等于 b10 -ne 20: a 不等于 b10 -gt 20: a 不大于 b10 -lt 20: a 小于 b10 -ge 20: a 小于 b10 -le 20: a 小于或等于 b 布尔运算符下表列出了常用的布尔运算符，假定变量 a 为 10，变量 b 为 20： 运算符 说明 举例 ! 非运算，表达式为 true 则返回 false，否则返回 true。 [ ! false ] 返回 true。 -o 或运算，有一个表达式为 true 则返回 true。 [ $a -lt 20 -o $b -gt 100 ] 返回 true。 -a 与运算，两个表达式都为 true 才返回 true。 [ $a -lt 20 -a $b -gt 100 ] 返回 false。 实例布尔运算符实例如下： 12345678910111213141516171819202122232425262728293031#!/bin/bash# author:W3Cschool教程# url:www.123a=10b=20if [ $a != $b ]then echo "$a != $b : a 不等于 b"else echo "$a != $b: a 等于 b"fiif [ $a -lt 100 -a $b -gt 15 ]then echo "$a -lt 100 -a $b -gt 15 : 返回 true"else echo "$a -lt 100 -a $b -gt 15 : 返回 false"fiif [ $a -lt 100 -o $b -gt 100 ]then echo "$a -lt 100 -o $b -gt 100 : 返回 true"else echo "$a -lt 100 -o $b -gt 100 : 返回 false"fiif [ $a -lt 5 -o $b -gt 100 ]then echo "$a -lt 5 -o $b -gt 100 : 返回 true"else echo "$a -lt 5 -o $b -gt 100 : 返回 false"fi 执行脚本，输出结果如下所示： 123410 != 20 : a 不等于 b10 -lt 100 -a 20 -gt 15 : 返回 true10 -lt 100 -o 20 -gt 100 : 返回 true10 -lt 5 -o 20 -gt 100 : 返回 false 逻辑运算符以下介绍 Shell 的逻辑运算符，假定变量 a 为 10，变量 b 为 20: 运算符 说明 举例 &amp;&amp; 逻辑的 AND [[ $a -lt 100 &amp;&amp; $b -gt 100 ]] 返回 false \ \ 逻辑的 OR [[ $a -lt 100 \ \ $b -gt 100 ]] 返回 true 实例逻辑运算符实例如下： 1234567891011121314151617181920#!/bin/bash# author:W3Cschool教程# url:www.123a=10b=20if [[ $a -lt 100 &amp;&amp; $b -gt 100 ]]then echo "返回 true"else echo "返回 false"fiif [[ $a -lt 100 || $b -gt 100 ]]then echo "返回 true"else echo "返回 false"fi 执行脚本，输出结果如下所示： 12返回 false返回 true 字符串运算符下表列出了常用的字符串运算符，假定变量 a 为 “abc”，变量 b 为 “efg”： 运算符 说明 举例 = 检测两个字符串是否相等，相等返回 true。 [ $a = $b ] 返回 false。 != 检测两个字符串是否相等，不相等返回 true。 [ $a != $b ] 返回 true。 -z 检测字符串长度是否为0，为0返回 true。 [ -z $a ] 返回 false。 -n 检测字符串长度是否为0，不为0返回 true。 [ -n $a ] 返回 true。 str 检测字符串是否为空，不为空返回 true。 [ $a ] 返回 true。 实例字符串运算符实例如下： 12345678910111213141516171819202122232425262728293031323334353637#!/bin/bash# author:W3Cschool教程# url:www.123a="abc"b="efg"if [ $a = $b ]then echo "$a = $b : a 等于 b"else echo "$a = $b: a 不等于 b"fiif [ $a != $b ]then echo "$a != $b : a 不等于 b"else echo "$a != $b: a 等于 b"fiif [ -z $a ]then echo "-z $a : 字符串长度为 0"else echo "-z $a : 字符串长度不为 0"fiif [ -n $a ]then echo "-n $a : 字符串长度不为 0"else echo "-n $a : 字符串长度为 0"fiif [ $a ]then echo "$a : 字符串不为空"else echo "$a : 字符串为空"fi 执行脚本，输出结果如下所示： 12345abc = efg: a 不等于 babc != efg : a 不等于 b-z abc : 字符串长度不为 0-n abc : 字符串长度不为 0abc : 字符串不为空 文件测试运算符文件测试运算符用于检测 Unix 文件的各种属性。 属性检测描述如下： 操作符 说明 举例 -b file 检测文件是否是块设备文件，如果是，则返回 true。 [ -b $file ] 返回 false。 -c file 检测文件是否是字符设备文件，如果是，则返回 true。 [ -c $file ] 返回 false。 -d file 检测文件是否是目录，如果是，则返回 true。 [ -d $file ] 返回 false。 -f file 检测文件是否是普通文件（既不是目录，也不是设备文件），如果是，则返回 true。 [ -f $file ] 返回 true。 -g file 检测文件是否设置了 SGID 位，如果是，则返回 true。 [ -g $file ] 返回 false。 -k file 检测文件是否设置了粘着位(Sticky Bit)，如果是，则返回 true。 [ -k $file ] 返回 false。 -p file 检测文件是否是有名管道，如果是，则返回 true。 [ -p $file ] 返回 false。 -u file 检测文件是否设置了 SUID 位，如果是，则返回 true。 [ -u $file ] 返回 false。 -r file 检测文件是否可读，如果是，则返回 true。 [ -r $file ] 返回 true。 -w file 检测文件是否可写，如果是，则返回 true。 [ -w $file ] 返回 true。 -x file 检测文件是否可执行，如果是，则返回 true。 [ -x $file ] 返回 true。 -s file 检测文件是否为空（文件大小是否大于0），不为空返回 true。 [ -s $file ] 返回 true。 -e file 检测文件（包括目录）是否存在，如果是，则返回 true。 [ -e $file ] 返回 true。 实例变量 file 表示文件”/var/www/w3cschool/test.sh”，它的大小为100字节，具有 rwx 权限。下面的代码，将检测该文件的各种属性： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#!/bin/bash# author:W3Cschool教程# url:www.123file="/var/www/w3cschool/test.sh"if [ -r $file ]then echo "文件可读"else echo "文件不可读"fiif [ -w $file ]then echo "文件可写"else echo "文件不可写"fiif [ -x $file ]then echo "文件可执行"else echo "文件不可执行"fiif [ -f $file ]then echo "文件为普通文件"else echo "文件为特殊文件"fiif [ -d $file ]then echo "文件是个目录"else echo "文件不是个目录"fiif [ -s $file ]then echo "文件不为空"else echo "文件为空"fiif [ -e $file ]then echo "文件存在"else echo "文件不存在"fi 执行脚本，输出结果如下所示： 1234567文件可读文件可写文件可执行文件为普通文件文件不是个目录文件不为空文件存在 Shell echo命令Shell echo命令Shell 的 echo 指令与 PHP 的 echo 指令类似，都是用于字符串的输出。命令格式： 1echo string 您可以使用echo实现更复杂的输出格式控制。 1.显示普通字符串:1echo "It is a test" 这里的双引号完全可以省略，以下命令与上面实例效果一致： 1echo It is a test 2.显示转义字符1echo "\"It is a test\"" 结果将是: 1"It is a test" 同样，双引号也可以省略 3.显示变量read 命令从标准输入中读取一行,并把输入行的每个字段的值指定给 shell 变量 123#!/bin/shread name echo "$name It is a test" 以上代码保存为 test.sh，name 接收标准输入的变量，结果将是: 123[root@www ~]# sh test.shOK #标准输入OK It is a test #输出 4.显示换行12echo -e &quot;OK!\n&quot; # -e 开启转义echo &quot;It it a test&quot; 输出结果： 123OK!It it a test 5.显示不换行123#!/bin/shecho -e "OK! \c" # -e 开启转义 \c 不换行echo "It is a test" 输出结果： 1OK! It is a test 6.显示结果定向至文件1echo "It is a test" &gt; myfile 7.原样输出字符串，不进行转义或取变量(用单引号)1echo '$name\"' 输出结果： 1$name\" 8.显示命令执行结果1echo `date` 注意：这里使用的是反引号12345结果将显示当前日期```shell Sat Dec 9 14:41:48 CST 2017 Shell printf 命令Shell printf 命令上一章节我们学习了 Shell 的 echo 命令，本章节我们来学习 Shell 的另一个输出命令 printf。 printf 命令模仿 C 程序库（library）里的 printf() 程序。 标准所定义，因此使用printf的脚本比使用echo移植性好。 printf 使用引用文本或空格分隔的参数，外面可以在printf中使用格式化字符串，还可以制定字符串的宽度、左右对齐方式等。默认printf不会像 echo 自动添加换行符，我们可以手动添加 \n。 printf 命令的语法： 1printf format-string [arguments...] 参数说明： format-string: 为格式控制字符串 arguments: 为参数列表。 实例如下： 12345$ echo &quot;Hello, Shell&quot;Hello, Shell$ printf &quot;Hello, Shell\n&quot;Hello, Shell$ 接下来,我来用一个脚本来体现printf的强大功能： 12345678#!/bin/bash# author:W3Cschool教程# url:www.123 printf &quot;%-10s %-8s %-4s\n&quot; 姓名 性别 体重kg printf &quot;%-10s %-8s %-4.2f\n&quot; 郭靖 男 66.1234 printf &quot;%-10s %-8s %-4.2f\n&quot; 杨过 男 48.6543 printf &quot;%-10s %-8s %-4.2f\n&quot; 郭芙 女 47.9876 执行脚本，输出结果如下所示： &quot;%-10s %-8s %-4.2f\n&quot; 郭芙 女 47.9876 1234姓名 性别 体重kg郭靖 男 66.12杨过 男 48.65郭芙 女 47.99 %s %c %d %f都是格式替代符 %-10s 指一个宽度为10个字符（-表示左对齐，没有则表示右对齐），任何字符都会被显示在10个字符宽的字符内，如果不足则自动以空格填充，超过也会将内容全部显示出来。 %-4.2f 指格式化为小数，其中.2指保留2位小数。 更多实例： 12345678910111213141516171819202122#!/bin/bash# author:W3Cschool教程# url:www.123 # format-string为双引号printf "%d %s\n" 1 "abc"# 单引号与双引号效果一样 printf '%d %s\n' 1 "abc" # 没有引号也可以输出printf %s abcdef# 格式只指定了一个参数，但多出的参数仍然会按照该格式输出，format-string 被重用printf %s abc defprintf "%s\n" abc defprintf "%s %s %s\n" a b c d e f g h i j# 如果没有 arguments，那么 %s 用NULL代替，%d 用 0 代替printf "%s and %d \n" 执行脚本，输出结果如下所示： 1234567891 abc1 abcabcdefabcdefabcdefa b cd e fg h ij and 0 printf的转义序列 序列 说明 \a 警告字符，通常为ASCII的BEL字符 \b 后退 \c 抑制（不显示）输出结果中任何结尾的换行字符（只在%b格式指示符控制下的参数字符串中有效），而且，任何留在参数里的字符、任何接下来的参数以及任何留在格式字符串中的字符，都被忽略 \f 换页（formfeed） \n 换行 \r 回车（Carriage return） \t 水平制表符 \v 垂直制表符 \ 一个字面上的反斜杠字符 \ddd 表示1到3位数八进制值的字符。仅在格式字符串中有效 \0ddd 表示1到3位的八进制值字符 实例123456789$ printf "a string, no processing:&lt;%s&gt;\n" "A\nB"a string, no processing:&lt;A\nB&gt;$ printf "a string, no processing:&lt;%b&gt;\n" "A\nB"a string, no processing:&lt;AB&gt;$ printf "www.123 \a"www.123 $ #不换行 Shell test命令Shell中的 test 命令用于检查某个条件是否成立，它可以进行数值、字符和文件三个方面的测试。 数值测试 参数 说明 -eq 等于则为真 -ne 不等于则为真 -gt 大于则为真 -ge 大于等于则为真 -lt 小于则为真 -le 小于等于则为真 实例演示： 12345678num1=100num2=100if test $[num1] -eq $[num2]then echo '两个数相等！'else echo '两个数不相等！'fi 输出结果： 1两个数相等！ 字符串测试 参数 说明 = 等于则为真 != 不相等则为真 -z 字符串 字符串长度为零则为真 -n 字符串 字符串长度不为零则为真 实例演示： 12345678num1="W3Cschool"num2="W3Cschool"if test num1=num2then echo '两个字符串相等!'else echo '两个字符串不相等!'fi 输出结果： 1两个字符串相等! 文件测试 参数 说明 -e 文件名 如果文件存在则为真 -r 文件名 如果文件存在且可读则为真 -w 文件名 如果文件存在且可写则为真 -x 文件名 如果文件存在且可执行则为真 -s 文件名 如果文件存在且至少有一个字符则为真 -d 文件名 如果文件存在且为目录则为真 -f 文件名 如果文件存在且为普通文件则为真 -c 文件名 如果文件存在且为字符型特殊文件则为真 -b 文件名 如果文件存在且为块特殊文件则为真 实例演示： 1234567cd /binif test -e ./bashthen echo '文件已存在!'else echo '文件不存在!'fi 输出结果： 1文件已存在! 另外，Shell还提供了与( -a )、或( -o )、非( ! )三个逻辑操作符用于将测试条件连接起来，其优先级为：”!”最高，”-a”次之，”-o”最低。例如： 1234567cd /binif test -e ./notFile -o -e ./bashthen echo '有一个文件存在!'else echo '两个文件都不存在'fi 输出结果： 1有一个文件存在! Shell 流程控制Shell 流程控制和Java、PHP等语言不一样，sh的流程控制不可为空，如(以下为PHP流程控制写法)： 123456&lt;?php if (isset($_GET["q"])) &#123; search(q); &#125; else &#123; // 不做任何事情 &#125; 在sh/bash里可不能这么写，如果else分支没有语句执行，就不要写这个else，就像这样 if elseifif 语句语法格式： 1234567if conditionthen command1 command2 ... commandN fi 写成一行（适用于终端命令提示符）： 1if [ $(ps -ef | grep -c "ssh") -gt 1 ]; then echo "true"; fi 末尾的fi就是if倒过来拼写，后面还会遇到类似的。 if elseif else 语法格式： 123456789if conditionthen command1 command2 ... commandNelse commandfi if else-if elseif else-if else 语法格式： 12345678if condition1then command1elif condition2 command2else commandNfi if else语句经常与test命令结合使用，如下所示： 12345678num1=$[2*3]num2=$[1+5]if test $[num1] -eq $[num2]then echo '两个数字相等!'else echo '两个数字不相等!'fi 输出结果： 1两个数字相等! for 循环与其他编程语言类似，Shell支持for循环。 for循环一般格式为： 1234567for var in item1 item2 ... itemNdo command1 command2 ... commandNdone 写成一行： 1for var in item1 item2 ... itemN; do command1; command2… done; 当变量值在列表里，for循环即执行一次所有命令，使用变量名获取列表中的当前取值。命令可为任何有效的shell命令和语句。in列表可以包含替换、字符串和文件名。 in列表是可选的，如果不用它，for循环使用命令行的位置参数。 例如，顺序输出当前列表中的数字： 1234for loop in 1 2 3 4 5do echo "The value is: $loop"done 输出结果： 12345The value is: 1The value is: 2The value is: 3The value is: 4The value is: 5 顺序输出字符串中的字符： 1234for str in 'This is a string'do echo $strdone 输出结果： 1This is a string while 语句while循环用于不断执行一系列命令，也用于从输入文件中读取数据；命令通常为测试条件。其格式为： 1234while conditiondo commanddone 以下是一个基本的while循环，测试条件是：如果int小于等于5，那么条件返回真。int从0开始，每次循环处理时，int加1。运行上述脚本，返回数字1到5，然后终止。 123#!/bin/shint=1while(( $int&lt;=5 )) do echo $int let &quot;int++&quot; done 运行脚本，输出： 1234512345 while循环可用于读取键盘信息。下面的例子中，输入信息被设置为变量FILM，按结束循环。 123456echo '按下 &lt;CTRL-D&gt; 退出'echo -n '输入你最喜欢的电影名: 'while read FILMdo echo "是的！$FILM 是一部好电影"done 运行脚本，输出类似下面： 123按下 &lt;CTRL-D&gt; 退出输入你最喜欢的电影名: W3Cschool在线教程是的！W3Cschool在线教程 是一部好电影 无限循环无限循环语法格式： 1234while :do commanddone 或者 1234while truedo commanddone 或者 1for (( ; ; )) until 循环until循环执行一系列命令直至条件为真时停止。 until循环与while循环在处理方式上刚好相反。 一般while循环优于until循环，但在某些时候—也只是极少数情况下，until循环更加有用。 until 语法格式: 1234until conditiondo commanddone 条件可为任意测试条件，测试发生在循环末尾，因此循环至少执行一次—请注意这一点。 caseShell case语句为多选择语句。可以用case语句匹配一个值与一个模式，如果匹配成功，执行相匹配的命令。case语句格式如下： 1234567891011121314case 值 in模式1) command1 command2 ... commandN ;;模式2） command1 command2 ... commandN ;;esac case工作方式如上所示。取值后面必须为单词in，每一模式必须以右括号结束。取值可以为变量或常数。匹配发现取值符合某一模式后，其间所有命令开始执行直至 ;;。 取值将检测匹配的每一个模式。一旦模式匹配，则执行完匹配模式相应命令后不再继续其他模式。如果无一匹配模式，使用星号 * 捕获该值，再执行后面的命令。 下面的脚本提示输入1到4，与每一种模式进行匹配： 123456789101112131415echo '输入 1 到 4 之间的数字:'echo '你输入的数字为:'read aNumcase $aNum in 1) echo '你选择了 1' ;; 2) echo '你选择了 2' ;; 3) echo '你选择了 3' ;; 4) echo '你选择了 4' ;; *) echo '你没有输入 1 到 4 之间的数字' ;;esac 输入不同的内容，会有不同的结果，例如： 1234输入 1 到 4 之间的数字:你输入的数字为:3你选择了 3 跳出循环在循环过程中，有时候需要在未达到循环结束条件时强制跳出循环，Shell使用两个命令来实现该功能：break和continue。 break命令break命令允许跳出所有循环（终止执行后面的所有循环）。 下面的例子中，脚本进入死循环直至用户输入数字大于5。要跳出这个循环，返回到shell提示符下，需要使用break命令。 12345678910111213#!/bin/bashwhile :do echo -n "输入 1 到 5 之间的数字:" read aNum case $aNum in 1|2|3|4|5) echo "你输入的数字为 $aNum!" ;; *) echo "你输入的数字不是 1 到 5 之间的! 游戏结束" break ;; esacdone 执行以上代码，输出结果为： 1234输入 1 到 5 之间的数字:3你输入的数字为 3!输入 1 到 5 之间的数字:7你输入的数字不是 1 到 5 之间的! 游戏结束 continuecontinue命令与break命令类似，只有一点差别，它不会跳出所有循环，仅仅跳出当前循环。 对上面的例子进行修改： 1234567891011121314#!/bin/bashwhile :do echo -n "输入 1 到 5 之间的数字: " read aNum case $aNum in 1|2|3|4|5) echo "你输入的数字为 $aNum!" ;; *) echo "你输入的数字不是 1 到 5 之间的!" continue echo "游戏结束" ;; esacdone 运行代码发现，当输入大于5的数字时，该例中的循环不会结束，语句 echo “Game is over!” 永远不会被执行。 esaccase的语法和C family语言差别很大，它需要一个esac（就是case反过来）作为结束标记，每个case分支用右圆括号，用两个分号表示break。 Shell 流程控制Shell 流程控制和Java、PHP等语言不一样，sh的流程控制不可为空，如(以下为PHP流程控制写法)： 123456&lt;?php if (isset($_GET["q"])) &#123; search(q); &#125; else &#123; // 不做任何事情 &#125; 在sh/bash里可不能这么写，如果else分支没有语句执行，就不要写这个else，就像这样 if elseifif 语句语法格式： 1234567if conditionthen command1 command2 ... commandN fi 写成一行（适用于终端命令提示符）： 1if [ $(ps -ef | grep -c "ssh") -gt 1 ]; then echo "true"; fi 末尾的fi就是if倒过来拼写，后面还会遇到类似的。 if elseif else 语法格式： 123456789if conditionthen command1 command2 ... commandNelse commandfi if else-if elseif else-if else 语法格式： 12345678if condition1then command1elif condition2 command2else commandNfi if else语句经常与test命令结合使用，如下所示： 12345678num1=$[2*3]num2=$[1+5]if test $[num1] -eq $[num2]then echo '两个数字相等!'else echo '两个数字不相等!'fi 输出结果： 1两个数字相等! for 循环与其他编程语言类似，Shell支持for循环。 for循环一般格式为： 1234567for var in item1 item2 ... itemNdo command1 command2 ... commandNdone 写成一行： 1for var in item1 item2 ... itemN; do command1; command2… done; 当变量值在列表里，for循环即执行一次所有命令，使用变量名获取列表中的当前取值。命令可为任何有效的shell命令和语句。in列表可以包含替换、字符串和文件名。 in列表是可选的，如果不用它，for循环使用命令行的位置参数。 例如，顺序输出当前列表中的数字： 1234for loop in 1 2 3 4 5do echo "The value is: $loop"done 输出结果： 12345The value is: 1The value is: 2The value is: 3The value is: 4The value is: 5 顺序输出字符串中的字符： 1234for str in 'This is a string'do echo $strdone 输出结果： 1This is a string while 语句while循环用于不断执行一系列命令，也用于从输入文件中读取数据；命令通常为测试条件。其格式为： 1234while conditiondo commanddone 以下是一个基本的while循环，测试条件是：如果int小于等于5，那么条件返回真。int从0开始，每次循环处理时，int加1。运行上述脚本，返回数字1到5，然后终止。 123#!/bin/shint=1while(( $int&lt;=5 )) do echo $int let "int++" done 运行脚本，输出： 1234512345 while循环可用于读取键盘信息。下面的例子中，输入信息被设置为变量FILM，按结束循环。 123456echo '按下 &lt;CTRL-D&gt; 退出'echo -n '输入你最喜欢的电影名: 'while read FILMdo echo "是的！$FILM 是一部好电影"done 运行脚本，输出类似下面： 123按下 &lt;CTRL-D&gt; 退出输入你最喜欢的电影名: W3Cschool在线教程是的！W3Cschool在线教程 是一部好电影 无限循环无限循环语法格式： 1234while :do commanddone 或者 1234while truedo commanddone 或者 1for (( ; ; )) until 循环until循环执行一系列命令直至条件为真时停止。 until循环与while循环在处理方式上刚好相反。 一般while循环优于until循环，但在某些时候—也只是极少数情况下，until循环更加有用。 until 语法格式: 1234until conditiondo commanddone 条件可为任意测试条件，测试发生在循环末尾，因此循环至少执行一次—请注意这一点。 caseShell case语句为多选择语句。可以用case语句匹配一个值与一个模式，如果匹配成功，执行相匹配的命令。case语句格式如下： 1234567891011121314case 值 in模式1) command1 command2 ... commandN ;;模式2） command1 command2 ... commandN ;;esac case工作方式如上所示。取值后面必须为单词in，每一模式必须以右括号结束。取值可以为变量或常数。匹配发现取值符合某一模式后，其间所有命令开始执行直至 ;;。 取值将检测匹配的每一个模式。一旦模式匹配，则执行完匹配模式相应命令后不再继续其他模式。如果无一匹配模式，使用星号 * 捕获该值，再执行后面的命令。 下面的脚本提示输入1到4，与每一种模式进行匹配： 123456789101112131415echo '输入 1 到 4 之间的数字:'echo '你输入的数字为:'read aNumcase $aNum in 1) echo '你选择了 1' ;; 2) echo '你选择了 2' ;; 3) echo '你选择了 3' ;; 4) echo '你选择了 4' ;; *) echo '你没有输入 1 到 4 之间的数字' ;;esac 输入不同的内容，会有不同的结果，例如： 1234输入 1 到 4 之间的数字:你输入的数字为:3你选择了 3 跳出循环在循环过程中，有时候需要在未达到循环结束条件时强制跳出循环，Shell使用两个命令来实现该功能：break和continue。 break命令break命令允许跳出所有循环（终止执行后面的所有循环）。 下面的例子中，脚本进入死循环直至用户输入数字大于5。要跳出这个循环，返回到shell提示符下，需要使用break命令。 12345678910111213#!/bin/bashwhile :do echo -n "输入 1 到 5 之间的数字:" read aNum case $aNum in 1|2|3|4|5) echo "你输入的数字为 $aNum!" ;; *) echo "你输入的数字不是 1 到 5 之间的! 游戏结束" break ;; esacdone 执行以上代码，输出结果为： 1234输入 1 到 5 之间的数字:3你输入的数字为 3!输入 1 到 5 之间的数字:7你输入的数字不是 1 到 5 之间的! 游戏结束 continuecontinue命令与break命令类似，只有一点差别，它不会跳出所有循环，仅仅跳出当前循环。 对上面的例子进行修改： 1234567891011121314#!/bin/bashwhile :do echo -n "输入 1 到 5 之间的数字: " read aNum case $aNum in 1|2|3|4|5) echo "你输入的数字为 $aNum!" ;; *) echo "你输入的数字不是 1 到 5 之间的!" continue echo "游戏结束" ;; esacdone 运行代码发现，当输入大于5的数字时，该例中的循环不会结束，语句 echo “Game is over!” 永远不会被执行。 esaccase的语法和C family语言差别很大，它需要一个esac（就是case反过来）作为结束标记，每个case分支用右圆括号，用两个分号表示break。 Shell 函数Shell 函数linux shell 可以用户定义函数，然后在shell脚本中可以随便调用。 shell中函数的定义格式如下： 123456789[ function ] funname [()]&#123; action; [return int;]&#125; 说明： 1、可以带function fun() 定义，也可以直接fun() 定义,不带任何参数。 2、参数返回，可以显示加：return 返回，如果不加，将以最后一条命令运行结果，作为返回值。 return后跟数值n(0-255 下面的例子定义了一个函数并进行调用： 1234567#!/bin/bashdemoFun()&#123; echo &quot;这是我的第一个 shell 函数!&quot;&#125;echo &quot;-----函数开始执行-----&quot;demoFunecho &quot;-----函数执行完毕-----&quot; 输出结果： 123-----函数开始执行-----这是我的第一个 shell 函数!-----函数执行完毕----- 下面定义一个带有return语句的函数： 123456789101112#!/bin/bashfunWithReturn()&#123; echo &quot;这个函数会对输入的两个数字进行相加运算...&quot; echo &quot;输入第一个数字: &quot; read aNum echo &quot;输入第二个数字: &quot; read anotherNum echo &quot;两个数字分别为 $aNum 和 $anotherNum !&quot; return $(($aNum+$anotherNum))&#125;funWithReturnecho &quot;输入的两个数字之和为 $? !&quot; 输出类似下面： 1234567这个函数会对输入的两个数字进行相加运算...输入第一个数字: 1输入第二个数字: 2两个数字分别为 1 和 2 !输入的两个数字之和为 3 ! 函数返回值在调用该函数后通过 $? 来获得。 注意：所有函数在使用前必须定义。这意味着必须将函数放在脚本开始部分，直至shell解释器首次发现它时，才可以使用。调用函数仅使用其函数名即可。 函数参数在Shell中，调用函数时可以向其传递参数。在函数体内部，通过 $n 的形式来获取参数的值，例如，$1表示第一个参数，$2表示第二个参数… 带参数的函数示例： 1234567891011#!/bin/bashfunWithParam()&#123; echo &quot;第一个参数为 $1 !&quot; echo &quot;第二个参数为 $2 !&quot; echo &quot;第十个参数为 $10 !&quot; echo &quot;第十个参数为 $&#123;10&#125; !&quot; echo &quot;第十一个参数为 $&#123;11&#125; !&quot; echo &quot;参数总数有 $# 个!&quot; echo &quot;作为一个字符串输出所有参数 $* !&quot;&#125;funWithParam 1 2 3 4 5 6 7 8 9 34 73 输出结果： 1234567第一个参数为 1 !第二个参数为 2 !第十个参数为 10 !第十个参数为 34 !第十一个参数为 73 !参数总数有 11 个!作为一个字符串输出所有参数 1 2 3 4 5 6 7 8 9 34 73 ! 注意，$10 不能获取第十个参数，获取第十个参数需要${10}。当n&gt;=10时，需要使用${n}来获取参数。 另外，还有几个特殊字符用来处理参数： 参数处理 说明 $# 传递到脚本的参数个数 $* 以一个单字符串显示所有向脚本传递的参数 $$ 脚本运行的当前进程ID号 $! 后台运行的最后一个进程的ID号 $@ 与$*相同，但是使用时加引号，并在引号中返回每个参数。 $- 显示Shell使用的当前选项，与set命令功能相同。 $? Shell 输入/输出重定向Shell 输入/输出重定向大多数 UNIX 系统命令从你的终端接受输入并将所产生的输出发送回到您的终端。一个命令通常从一个叫标准输入的地方读取输入，默认情况下，这恰好是你的终端。同样，一个命令通常将其输出写入到标准输出，默认情况下，这也是你的终端。 重定向命令列表如下： 命令 说明 command &gt; file 将输出重定向到 file。 command &lt; file 将输入重定向到 file。 command &gt;&gt; file 将输出以追加的方式重定向到 file。 n &gt; file 将文件描述符为 n 的文件重定向到 file。 n &gt;&gt; file 将文件描述符为 n 的文件以追加的方式重定向到 file。 n &gt;&amp; m 将输出文件 m 和 n 合并。 n &lt;&amp; m 将输入文件 m 和 n 合并。 &lt;&lt; tag 将开始标记 tag 和结束标记 tag 之间的内容作为输入。 需要注意的是文件描述符 0 通常是标准输入（STDIN），1 是标准输出（STDOUT），2 是标准错误输出（STDERR）。 输出重定向重定向一般通过在命令间插入特定的符号来实现。特别的，这些符号的语法如下所示: 1command1 &gt; file1 上面这个命令执行command1然后将输出的内容存入file1。 注意任何file1内的已经存在的内容将被新内容替代。如果要将新内容添加在文件末尾，请使用&gt;&gt;操作符。 实例执行下面的 who 命令，它将命令的完整的输出重定向在用户文件中(users): 1$ who &gt; users 执行后，并没有在终端输出信息，这是因为输出已被从默认的标准输出设备（终端）重定向到指定的文件。 你可以使用 cat 命令查看文件内容： 1234$ cat users_mbsetupuser console Oct 31 17:35 laolan console Oct 31 17:35 laolan ttys000 Dec 1 11:33 输出重定向会覆盖文件内容，请看下面的例子： 1234$ echo &quot;W3Cschool教程：www.123&quot; &gt; users$ cat usersW3Cschool教程：www.123$ 如果不希望文件内容被覆盖，可以使用 &gt;&gt; 追加到文件末尾，例如： 12345$ echo &quot;W3Cschool教程：www.123&quot; &gt;&gt; users$ cat usersW3Cschool教程：www.123W3Cschool教程：www.123$ 输入重定向和输出重定向一样，Unix 命令也可以从文件获取输入，语法为： 1command1 &lt; file1 这样，本来需要从键盘获取输入的命令会转移到文件读取内容。 注意：输出重定向是大于号(&gt;)，输入重定向是小于号(&lt;)。 实例接着以上实例，我们需要统计 users 文件的行数,执行以下命令： 12$ wc -l users 2 users 也可以将输入重定向到 users 文件： 12$ wc -l &lt; users 2 注意：上面两个例子的结果不同：第一个例子，会输出文件名；第二个不会，因为它仅仅知道从标准输入读取内容。 1command1 &lt; infile &gt; outfile 同时替换输入和输出，执行command1，从文件infile读取内容，然后将输出写入到outfile中。 重定向深入讲解一般情况下，每个 Unix/Linux 命令运行时都会打开三个文件： 标准输入文件(stdin)：stdin的文件描述符为0，Unix程序默认从stdin读取数据。 标准输出文件(stdout)：stdout 的文件描述符为1，Unix程序默认向stdout输出数据。 标准错误文件(stderr)：stderr的文件描述符为2，Unix程序会向stderr流中写入错误信息。 默认情况下，command &gt; file 将 stdout 重定向到 file，command &lt; file 将stdin 重定向到 file。 如果希望 stderr 重定向到 file，可以这样写： 1$ command 2 &gt; file 如果希望 stderr 追加到 file 文件末尾，可以这样写： 1$ command 2 &gt;&gt; file 2 表示标准错误文件(stderr)。 如果希望将 stdout 和 stderr 合并后重定向到 file，可以这样写： 12345$ command &gt; file 2&gt;&amp;1或者$ command &gt;&gt; file 2&gt;&amp;1 如果希望对 stdin 和 stdout 都重定向，可以这样写： 1$ command &lt; file1 &gt;file2 command 命令将 stdin 重定向到 file1，将 stdout 重定向到 file2。 Here DocumentHere Document 是 Shell 中的一种特殊的重定向方式，用来将输入重定向到一个交互式 Shell 脚本或程序。 它的基本的形式如下： 123command &lt;&lt; delimiter documentdelimiter 它的作用是将两个 delimiter 之间的内容(document) 作为输入传递给 command。 注意： 结尾的delimiter 一定要顶格写，前面不能有任何字符，后面也不能有任何字符，包括空格和 tab 缩进。 开始的delimiter前后的空格会被忽略掉。 实例在命令行中通过 wc -l 命令计算 Here Document 的行数： 1234567$ wc -l &lt;&lt; EOF 欢迎来到 W3Cschool教程 www.123EOF3 # 输出结果为 3 行$ 我们也可以将 Here Document 用在脚本中，例如： 123456789#!/bin/bash# author:W3Cschool教程# url:www.123cat &lt;&lt; EOF欢迎来到W3Cschool教程www.123EOF 执行以上脚本，输出结果： 123欢迎来到W3Cschool教程www.123 /dev/null 文件如果希望执行某个命令，但又不希望在屏幕上显示输出结果，那么可以将输出重定向到 /dev/null： 1$ command &gt; /dev/null /dev/null 是一个特殊的文件，写入到它的内容都会被丢弃；如果尝试从该文件读取内容，那么什么也读不到。但是 /dev/null 文件非常有用，将命令的输出重定向到它，会起到”禁止输出”的效果。 如果希望屏蔽 stdout 和 stderr，可以这样写： 1$ command &gt; /dev/null 2&gt;&amp;1 注意：0 是标准输入（STDIN），1 是标准输出（STDOUT），2 是标准错误输出（STDERR）。 Shell 文件包含Shell 文件包含和其他语言一样，Shell 也可以包含外部脚本。这样可以很方便的封装一些公用的代码作为一个独立的文件。 Shell 文件包含的语法格式如下： 12345. filename # 注意点号(.)和文件名中间有一空格或source filename 实例创建两个 shell 脚本文件。 test1.sh 代码如下： 12345#!/bin/bash# author:W3Cschool教程# url:www.123url=&quot;http://www.123&quot; test2.sh 代码如下： 1234567891011#!/bin/bash# author:W3Cschool教程# url:www.123#使用 . 号来引用test1.sh 文件. ./test1.sh# 或者使用以下包含文件代码# source ./test1.shecho &quot;W3Cschool教程官网地址：$url&quot; 接下来，我们为 test2.sh 添加可执行权限并执行： 123$ chmod +x test2.sh $ ./test2.sh W3Cschool教程官网地址：http://www.123]]></content>
      <categories>
        <category>-shell</category>
      </categories>
      <tags>
        <tag>-linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux教程]]></title>
    <url>%2F2017%2F02%2F14%2Flinux%2F</url>
    <content type="text"><![CDATA[Linux 安装本章节我们将为大家介绍Linux的安装。 本章节以 centos6.4 为例。 centos6.4 下载地址： 网易镜像：http://mirrors.163.com/centos/6/isos/ 搜狐镜像：http://mirrors.sohu.com/centos/6/isos/ 注：建议安装64位Linux系统。 接下来你需要将下载的Linux系统刻录成光盘或U盘。 注：你也可以在Window上安装VMware虚拟机来安装Linux系统。 Linux 安装步骤1、首先，使用光驱或U盘或你下载的Linux ISO文件进行安装。 界面说明： Install or upgrade an existing system 安装或升级现有的系统 install system with basic video driver 安装过程中采用基本的显卡驱动 Rescue installed system 进入系统修复模式 Boot from local drive 退出安装从硬盘启动 Memory test 内存检测 注：用联想E49安装时选择第一项安装时会出现屏幕显示异常的问题，后改用第二项安装时就没有出现问题 2、介质直接”skip”就可以了 3、出现引导界面，点击”next” 4、选中”English（English）”否则会有部分乱码问题 5、键盘布局选择”U.S.English” 6、选择”Basic Storage Devies”点击”Next” 7、询问是否忽略所有数据，新电脑安装系统选择”Yes,discard any data” 8、Hostname填写格式”英文名.姓” 9、网络设置安装图示顺序点击就可以了 10、时区可以在地图上点击，选择”shanghai”并取消System clock uses UTC前面的对勾 11、设置root的密码 12、硬盘分区，一定要按照图示点选 13、调整分区，必须要有/home这个分区，如果没有这个分区，安装部分软件会出现不能安装的问题 14、询问是否格式化分区 15、将更改写入到硬盘 16、引导程序安装位置 17、最重要的一步，也是本教程最关键的一步，也是其他教程没有提及的一步，按图示顺序点击 18、取消以下内容的所有选项 Applications Base System Servers 并对Desktops进行如下设置 即取消如下选项： Desktop Debugging and Performance Tools Desktop Platform Remote Desktop Clients Input Methods中仅保留ibus-pinyin-1.3.8-1.el6.x86_64,其他的全部取消 19、选中Languages，并选中右侧的Chinese Support然后点击红色区域 20、调整完成后如下图所示 21、至此，一个最精简的桌面环境就设置完成了， 22、安装完成，重启 23、重启之后，的License Information 24、Create User Username：填写您的英文名（不带.姓） Full Name：填写您的英文名.姓（首字母大写） 25、”Date and Time” 选中 “Synchronize data and time over the network” Finsh之后系统将重启 26、第一次登录，登录前不要做任何更改，这个很重要！！！登录之后紧接着退出 第二次登录，选择语言，在红色区域选择下拉小三角，选other，选中”汉语（中国）” 27、登录之后，请一定按照如下顺序点击！ 至此，CentOS安装完成，如有其他问题，请随时与我联系！！ Linux 系统启动过程Linux 系统启动过程linux启动时我们会看到许多启动信息。 Linux系统的启动过程并不是大家想象中的那么复杂，其过程可以分为5个阶段： 内核的引导。 运行init。 系统初始化。 建立终端 。 用户登录系统。 内核引导当计算机打开电源后，首先是BIOS开机自检，按照BIOS中设置的启动设备（通常是硬盘）来启动。 操作系统接管硬件以后，首先读入 /boot 目录下的内核文件。 运行initinit 进程是系统所有进程的起点，你可以把它比拟成系统所有进程的老祖宗，没有这个进程，系统中任何进程都不会启动。 init 程序首先是需要读取配置文件 /etc/inittab。 运行级别许多程序需要开机启动。它们在Windows叫做”服务”（service），在Linux就叫做”守护进程”（daemon）。 init进程的一大任务，就是去运行这些开机启动的程序。 但是，不同的场合需要启动不同的程序，比如用作服务器时，需要启动Apache，用作桌面就不需要。 Linux允许为不同的场合，分配不同的开机启动程序，这就叫做”运行级别”（runlevel）。也就是说，启动时根据”运行级别”，确定要运行哪些程序。 Linux系统有7个运行级别(runlevel)： 运行级别0：系统停机状态，系统默认运行级别不能设为0，否则不能正常启动 运行级别1：单用户工作状态，root权限，用于系统维护，禁止远程登陆 运行级别2：多用户状态(没有NFS) 运行级别3：完全的多用户状态(有NFS)，登陆后进入控制台命令行模式 运行级别4：系统未使用，保留 运行级别5：X11控制台，登陆后进入图形GUI模式 运行级别6：系统正常关闭并重启，默认运行级别不能设为6，否则不能正常启动 系统初始化在init的配置文件中有这么一行： si::sysinit:/etc/rc.d/rc.sysinit 它调用执行了/etc/rc.d/rc.sysinit，而rc.sysinit是一个bash shell的脚本，它主要是完成一些系统初始化的工作，rc.sysinit是每一个运行级别都要首先运行的重要脚本。 它主要完成的工作有：激活交换分区，检查磁盘，加载硬件模块以及其它一些需要优先执行任务。 1l5:5:wait:/etc/rc.d/rc 5 这一行表示以5为参数运行/etc/rc.d/rc，/etc/rc.d/rc是一个Shell脚本，它接受5作为参数，去执行/etc/rc.d/rc5.d/目录下的所有的rc启动脚本，/etc/rc.d/rc5.d/目录中的这些启动脚本实际上都是一些连接文件，而不是真正的rc启动脚本，真正的rc启动脚本实际上都是放在/etc/rc.d/init.d/目录下。 而这些rc启动脚本有着类似的用法，它们一般能接受start、stop、restart、status等参数。 /etc/rc.d/rc5.d/中的rc启动脚本通常是K或S开头的连接文件，对于以以S开头的启动脚本，将以start参数来运行。 而如果发现存在相应的脚本也存在K打头的连接，而且已经处于运行态了(以/var/lock/subsys/下的文件作为标志)，则将首先以stop为参数停止这些已经启动了的守护进程，然后再重新运行。 这样做是为了保证是当init改变运行级别时，所有相关的守护进程都将重启。 至于在每个运行级中将运行哪些守护进程，用户可以通过chkconfig或setup中的”System Services”来自行设定。 建立终端rc执行完毕后，返回init。这时基本系统环境已经设置好了，各种守护进程也已经启动了。 init接下来会打开6个终端，以便用户登录系统。在inittab中的以下6行就是定义了6个终端： 1234561:2345:respawn:/sbin/mingetty tty12:2345:respawn:/sbin/mingetty tty23:2345:respawn:/sbin/mingetty tty34:2345:respawn:/sbin/mingetty tty45:2345:respawn:/sbin/mingetty tty56:2345:respawn:/sbin/mingetty tty6 从上面可以看出在2、3、4、5的运行级别中都将以respawn方式运行mingetty程序，mingetty程序能打开终端、设置模式。 同时它会显示一个文本登录界面，这个界面就是我们经常看到的登录界面，在这个登录界面中会提示用户输入用户名，而用户输入的用户将作为参数传给login程序来验证用户的身份。 用户登录系统一般来说，用户的登录方式有三种： （1）命令行登录 （2）ssh登录 （3）图形界面登录 对于运行级别为5的图形方式用户来说，他们的登录是通过一个图形化的登录界面。登录成功后可以直接进入KDE、Gnome等窗口管理器。 而本文主要讲的还是文本方式登录的情况：当我们看到mingetty的登录界面时，我们就可以输入用户名和密码来登录系统了。 Linux的账号验证程序是login，login会接收mingetty传来的用户名作为用户名参数。 然后login会对用户名进行分析：如果用户名不是root，且存在/etc/nologin文件，login将输出nologin文件的内容，然后退出。 这通常用来系统维护时防止非root用户登录。只有/etc/securetty中登记了的终端才允许root用户登录，如果不存在这个文件，则root可以在任何终端上登录。 /etc/usertty文件用于对用户作出附加访问限制，如果不存在这个文件，则没有其他限制。 &lt;p在分析完用户名后，login将搜索/etc/passwd以及/etc/shadow来验证密码以及设置账户的其它信息，比如：主目录是什么、使用何种shell。如果没有指定主目录，将默认为根目录；如果没有指定shell，将默认为/bin/bash。 图形模式与文字模式的切换方式Linux预设提供了六个命令窗口终端机让我们来登录。 默认我们登录的就是第一个窗口，也就是tty1，这个六个窗口分别为tty1,tty2 … tty6，你可以按下Ctrl + Alt + F1 ~ F6 来切换它们。 如果你安装了图形界面，默认情况下是进入图形界面的，此时你就可以按Ctrl + Alt + F1 ~ F6来进入其中一个命令窗口界面。 当你进入命令窗口界面后再返回图形界面只要按下Ctrl + Alt + F7 就回来了。 如果你用的vmware 虚拟机，命令窗口切换的快捷键为 Alt + Space + F1~F6. 如果你在图形界面下请按Alt + Shift + Ctrl + F1~F6 切换至命令窗口。 Linux 关机在linux领域内大多用在服务器上，很少遇到关机的操作。毕竟服务器上跑一个服务是永无止境的，除非特殊情况下，不得已才会关机。 正确的关机流程为：sync &gt; shutdown &gt; reboot &gt; halt 关机指令为：shutdown ，你可以man shutdown 来看一下帮助文档。 例如你可以运行如下命令关机： 12345678910111213141516171819sync 将数据由内存同步到硬盘中。shutdown 关机指令，你可以man shutdown 来看一下帮助文档。例如你可以运行如下命令关机：shutdown –h 10 ‘This server will shutdown after 10 mins’ 这个命令告诉大家，计算机将在10分钟后关机，并且会显示在登陆用户的当前屏幕中。Shutdown –h now 立马关机Shutdown –h 20:25 系统会在今天20:25关机Shutdown –h +10 十分钟后关机Shutdown –r now 系统立马重启Shutdown –r +10 系统十分钟后重启reboot 就是重启，等同于 shutdown –r nowhalt 关闭系统，等同于shutdown –h now 和 poweroff 最后总结一下，不管是重启系统还是关闭系统，首先要运行sync命令，把内存中的数据写到磁盘中。 关机的命令有 shutdown –h now halt poweroff 和 init 0 , 重启系统的命令有 shutdown –r now ， reboot 和 init Linux 系统目录结构Linux 系统目录结构登录系统后，在当前命令窗口下输入命令： 1ls / 你会看到如下图所示: 树状目录结构： 以下是对这些目录的解释： /bin：bin是Binary的缩写, 这个目录存放着最经常使用的命令。 /boot：这里存放的是启动Linux时使用的一些核心文件，包括一些连接文件以及镜像文件。 /dev ：dev是Device(设备)的缩写, 该目录下存放的是Linux的外部设备，在Linux中访问设备的方式和访问文件的方式是相同的。 /etc：这个目录用来存放所有的系统管理所需要的配置文件和子目录。 /home：用户的主目录，在Linux中，每个用户都有一个自己的目录，一般该目录名是以用户的账号命名的。 /lib：这个目录里存放着系统最基本的动态连接共享库，其作用类似于Windows里的DLL文件。几乎所有的应用程序都需要用到这些共享库。 /lost+found：这个目录一般情况下是空的，当系统非法关机后，这里就存放了一些文件。 /media linux系统会自动识别一些设备，例如U盘、光驱等等，当识别后，linux会把识别的设备挂载到这个目录下。 /mnt：系统提供该目录是为了让用户临时挂载别的文件系统的，我们可以将光驱挂载在/mnt/上，然后进入该目录就可以查看光驱里的内容了。 /opt： 这是给主机额外安装软件所摆放的目录。比如你安装一个ORACLE数据库则就可以放到这个目录下。默认是空的。 /proc：这个目录是一个虚拟的目录，它是系统内存的映射，我们可以通过直接访问这个目录来获取系统信息。这个目录的内容不在硬盘上而是在内存里，我们也可以直接修改里面的某些文件，比如可以通过下面的命令来屏蔽主机的ping命令，使别人无法ping你的机器： 1echo 1 &gt; /proc/sys/net/ipv4/icmp_echo_ignore_all /root：该目录为系统管理员，也称作超级权限者的用户主目录。 /sbin：s就是Super User的意思，这里存放的是系统管理员使用的系统管理程序。 /selinux： 这个目录是Redhat/CentOS所特有的目录，Selinux是一个安全机制，类似于windows的防火墙，但是这套机制比较复杂，这个目录就是存放selinux相关的文件的。 /srv： 该目录存放一些服务启动之后需要提取的数据。 /sys： 这是linux2.6内核的一个很大的变化。该目录下安装了2.6内核中新出现的一个文件系统 sysfs 。 sysfs文件系统集成了下面3种文件系统的信息：针对进程信息的proc文件系统、针对设备的devfs文件系统以及针对伪终端的devpts文件系统。 该文件系统是内核设备树的一个直观反映。 当一个内核对象被创建的时候，对应的文件和目录也在内核对象子系统中被创建。 /tmp：这个目录是用来存放一些临时文件的。 /usr： 这是一个非常重要的目录，用户的很多应用程序和文件都放在这个目录下，类似与windows下的program files目录。 /usr/bin：系统用户使用的应用程序。 /usr/sbin：超级用户使用的比较高级的管理程序和系统守护程序。 /usr/src：内核源代码默认的放置目录。 /var：这个目录中存放着在不断扩充着的东西，我们习惯将那些经常被修改的目录放在这个目录下。包括各种日志文件。 在linux系统中，有几个目录是比较重要的，平时需要注意不要误删除或者随意更改内部文件。 /etc： 上边也提到了，这个是系统中的配置文件，如果你更改了该目录下的某个文件可能会导致系统不能启动。 /bin, /sbin, /usr/bin, /usr/sbin: 这是系统预设的执行文件的放置目录，比如 ls 就是在/bin/ls 目录下的。 值得提出的是，/bin, /usr/bin 是给系统用户使用的指令（除root外的通用户），而/sbin, /usr/sbin 则是给root使用的指令。 /var： 这是一个非常重要的目录，系统上跑了很多程序，那么每个程序都会有相应的日志产生，而这些日志就被记录到这个目录下，具体在/var/log 目录下，另外mail的预设放置也是在这里。 Linux 忘记密码解决方法Linux 忘记密码解决方法很多朋友经常会忘记Linux系统的root密码，linux系统忘记root密码的情况该怎么办呢？重新安装系统吗？当然不用！进入单用户模式更改一下root密码即可。 步骤如下： 重启linux系统 3 秒之内要按一下回车，出现如下界面 然后输入e 在 第二行最后边输入 single，有一个空格。具体方法为按向下尖头移动到第二行，按”e”进入编辑模式 在后边加上single 回车 最后按”b”启动，启动后就进入了单用户模式了 此时已经进入到单用户模式了，你可以更改root密码了。更密码的命令为 passwd 【使用系统安装光盘的救援模式】 救援模式即rescue ，这个模式主要是应用于，系统无法进入的情况。如，grub损坏或者某一个配置文件修改出错。如何使用rescue模式呢？ 光盘启动，按F5 进入rescue模式 输入linux rescue 回车 选择语言，笔者建议你选择英语 选择us 键盘 这里问你是否启动网络，有时候可能会联网调试。我们选no 这里告诉我们，接下来会把系统挂载在/mnt/sysimage 中。 其中有三个选项: Continue 就是挂载后继续下一步。 Read-Only 挂载成只读，这样更安全，有时文件系统损坏时，只读模式会防止文件系统近一步损坏。 Skip就是不挂载，进入一个命令窗口模式。 这里我们选择Continue。 至此，系统已经挂载到了/mnt/sysimage中。接下来回车，输入chroot /mnt/sysimage 进入管理员环境。 提示： 其实也可以到rescue模式下更改root的密码的。这个rescue模式和windows PE系统很相近。 当运行了chroot /mnt/sysimage/ 后，再ls 看到目录结构和原来系统中的目录结构是一样的。 没错！现在的环境和原来系统的环境是一模一样的。你可以输入exit 或者按Ctrl + D退出这个环境。然后你再ls 看一下 这个目录其实就是rescue模式下的目录结构，而我们的系统文件全部在 /mnt/sysimage目录下。 Linux 远程登录Linux一般作为服务器使用，而服务器一般放在机房，你不可能在机房操作你的Linux服务器。这时我们就需要远程登录到Linux服务器来管理维护系统。 CentOS系统默认安装了openssh 如果没有安装可以使用命令进行安装： yum install openssh-server -y Linux系统中是通过ssh服务实现的远程登录功能，默认ssh服务端口号为 22。 Window系统上 Linux 远程登录客户端有SecureCRT, Putty, SSH Secure Shell，Xshell等，本文以Putty为例来登录远程服务器。 putty下载地址：http://www.putty.org/ 如果你下载了putty，请双击putty.exe 然后弹出如下的窗口。 在Host Name( or IP address) 下面的框中输入你要登录的远程服务器IP(可以通过ifconfig命令查看服务器ip)，然后回车。 此时，提示我们输入要登录的用户名。 输入root 然后回车，再输入密码，就能登录到远程的linux系统了。 使用密钥认证机制远程登录linuxSSH 为 Secure Shell 的缩写，由 IETF 的网络工作小组（Network Working Group）所制定。 SSH 为建立在应用层和传输层基础上的安全协议。 首先使用工具 PUTTYGEN.EXE 生成密钥对。打开工具PUTTYGEN.EXE后如下图所示： 该工具可以生成三种格式的key ：SSH-1(RSA) SSH-2(RSA) SSH-2(DSA) ，我们采用默认的格式即SSH-2(RSA)。Number of bits in a generated key 这个是指生成的key的大小，这个数值越大，生成的key就越复杂，安全性就越高。这里我们写2048. 然后单击Generate 开始生成密钥对： 注意的是，在这个过程中鼠标要来回的动，否则这个进度条是不会动的。 到这里，密钥对已经生成了。你可以给你的密钥输入一个密码，（在Key Passphrase那里）也可以留空。然后点 Save public key 保存公钥，点 Save private Key 保存私钥。笔者建议你放到一个比较安全的地方，一来防止别人偷窥，二来防止误删除。接下来就该到远程linux主机上设置了。 1）创建目录 /root/.ssh 并设置权限 [root@localhost ~]# mkdir /root/.ssh mkdir 命令用来创建目录，以后会详细介绍，暂时只了解即可。 [root@localhost ~]# chmod 700 /root/.ssh chmod 命令是用来修改文件属性权限的，以后会详细介绍。 2）创建文件 / root/.ssh/authorized_keys [root@localhost ~]# vim /root/.ssh/authorized_keys vim 命令是编辑一个文本文件的命令，同样在后续章节详细介绍。 3）打开刚才生成的public key 文件，建议使用写字板打开，这样看着舒服一些，复制从AAAA开头至 “—- END SSH2 PUBLIC KEY —-“ 该行上的所有内容，粘贴到/root/.ssh/authorized_keys 文件中，要保证所有字符在一行。（可以先把复制的内容拷贝至记事本，然后编辑成一行载粘贴到该文件中）。 在这里要简单介绍一下，如何粘贴，用vim打开那个文件后，该文件不存在，所以vim会自动创建。按一下字母”i”然后同时按shift + Insert 进行粘贴（或者单击鼠标右键即可），前提是已经复制到剪切板中了。粘贴好后，然后把光标移动到该行最前面输入ssh-rsa ，然后按空格。再按ESC，然后输入冒号wq 即 :wq 就保存了。格式如下图： 4）再设置putty选项，点窗口左侧的SSh –&gt; Auth ，单击窗口右侧的Browse… 选择刚刚生成的私钥， 再点Open ，此时输入root，就不用输入密码就能登录了。 如果在前面你设置了Key Passphrase ，那么此时就会提示你输入密码的。为了更加安全建议大家要设置一个Key Passphrase。 Linux 文件基本属性Linux 文件基本属性Linux系统是一种典型的多用户系统，不同的用户处于不同的地位，拥有不同的权限。为了保护系统的安全性，Linux系统对不同的用户访问同一文件（包括目录文件）的权限做了不同的规定。 在Linux中我们可以使用 ll 或者 ls –l 命令来显示一个文件的属性以及文件所属的用户和组，如： 12345[root@www /]# ls -ltotal 64dr-xr-xr-x 2 root root 4096 Dec 14 2012 bindr-xr-xr-x 4 root root 4096 Apr 19 2012 boot…… 实例中，bin文件的第一个属性用”d”表示。”d”在Linux中代表该文件是一个目录文件。 在Linux中第一个字符代表这个文件是目录、文件或链接文件等等。 当为[ d ]则是目录 当为[ - ]则是文件； 若是[ l ]则表示为链接文档(link file)； 若是[ b ]则表示为装置文件里面的可供储存的接口设备(可随机存取装置)； 若是[ c ]则表示为装置文件里面的串行端口设备，例如键盘、鼠标(一次性读取装置)。 接下来的字符中，以三个为一组，且均为『rwx』 的三个参数的组合。其中，[ r ]代表可读(read)、[ w ]代表可写(write)、[ x ]代表可执行(execute)。 要注意的是，这三个权限的位置不会改变，如果没有权限，就会出现减号[ - ]而已。 每个文件的属性由左边第一部分的10个字符来确定（如下图）。 从左至右用0-9这些数字来表示。 第0位确定文件类型，第1-3位确定属主（该文件的所有者）拥有该文件的权限。 第4-6位确定属组（所有者的同组用户）拥有该文件的权限，第7-9位确定其他用户拥有该文件的权限。 其中，第1、4、7位表示读权限，如果用”r”字符表示，则有读权限，如果用”-“字符表示，则没有读权限； 第2、5、8位表示写权限，如果用”w”字符表示，则有写权限，如果用”-“字符表示没有写权限；第3、6、9位表示可执行权限，如果用”x”字符表示，则有执行权限，如果用”-“字符表示，则没有执行权限。 Linux文件属主和属组12345[root@www /]# ls -ltotal 64dr-xr-xr-x 2 root root 4096 Dec 14 2012 bindr-xr-xr-x 4 root root 4096 Apr 19 2012 boot…… 对于文件来说，它都有一个特定的所有者，也就是对该文件具有所有权的用户。 同时，在Linux系统中，用户是按组分类的，一个用户属于一个或多个组。 文件所有者以外的用户又可以分为文件所有者的同组用户和其他用户。 因此，Linux系统按文件所有者、文件所有者同组用户和其他用户来规定了不同的文件访问权限。 在以上实例中，bin文件是一个目录文件，属主和属组都为root，属主有可读、可写、可执行的权限；与属主同组的其他用户有可读和可执行的权限；其他用户也有可读和可执行的权限。 更改文件属性1、chgrp：更改文件属组语法： 1chgrp [-R] 属组名文件名 参数选项 -R：递归更改文件属组，就是在更改某个目录文件的属组时，如果加上-R的参数，那么该目录下的所有文件的属组都会更改。 2、chown：更改文件属主，也可以同时更改文件属组语法： 12chown [–R] 属主名 文件名chown [-R] 属主名：属组名 文件名 进入 /root 目录（~）将install.log的拥有者改为bin这个账号： 1234[root@www ~] cd ~[root@www ~]# chown bin install.log[root@www ~]# ls -l-rw-r--r-- 1 bin users 68495 Jun 25 08:53 install.log 将install.log的拥有者与群组改回为root： 123[root@www ~]# chown root:root install.log[root@www ~]# ls -l-rw-r--r-- 1 root root 68495 Jun 25 08:53 install.log 3、chmod：更改文件9个属性Linux文件属性有两种设置方法，一种是数字，一种是符号。 Linux文件的基本权限就有九个，分别是owner/group/others三种身份各有自己的read/write/execute权限。 先复习一下刚刚上面提到的数据：文件的权限字符为：『-rwxrwxrwx』， 这九个权限是三个三个一组的！其中，我们可以使用数字来代表各个权限，各权限的分数对照表如下： r:4 w:2 x:1 每种身份(owner/group/others)各自的三个权限(r/w/x)分数是需要累加的，例如当权限为： [-rwxrwx—] 分数则是： owner = rwx = 4+2+1 = 7 group = rwx = 4+2+1 = 7 others= — = 0+0+0 = 0 所以等一下我们设定权限的变更时，该文件的权限数字就是770啦！变更权限的指令chmod的语法是这样的： 1chmod [-R] xyz 文件或目录 选项与参数： xyz : 就是刚刚提到的数字类型的权限属性，为 rwx 属性数值的相加。 -R : 进行递归(recursive)的持续变更，亦即连同此目录下的所有文件都会变更 举例来说，如果要将.bashrc这个文件所有的权限都设定启用，那么命令如下： 12345[root@www ~]# ls -al .bashrc-rw-r--r-- 1 root root 395 Jul 4 11:45 .bashrc[root@www ~]# chmod 777 .bashrc[root@www ~]# ls -al .bashrc-rwxrwxrwx 1 root root 395 Jul 4 11:45 .bashrc 那如果要将权限变成 -rwxr-xr– 呢？那么权限的分数就成为 [4+2+1][4+0+1][4+0+0]=754。 符号类型改变文件权限还有一个改变权限的方法呦！从之前的介绍中我们可以发现，基本上就九个权限分别是(1)user (2)group (3)others三种身份啦！ 那么我们就可以藉由u, g, o来代表三种身份的权限！ 此外， a 则代表 all 亦即全部的身份！那么读写的权限就可以写成r, w, x！也就是可以使用底下的方式来看： chmod u g o a +(加入) -(除去) =(设定) r w x 文件或目录 如果我们需要将文件权限设置为 -rwxr-xr– ，可以使用 chmod u=rwx,g=rx,o=r 文件名 来设定: 12345[root@www ~]# ls -al .bashrc-rwxr-xr-x 1 root root 395 Jul 4 11:45 .bashrc[root@www ~]# chmod a+w .bashrc[root@www ~]# ls -al .bashrc-rwxrwxrwx 1 root root 395 Jul 4 11:45 .bashrc 而如果是要将权限去掉而不改变其他已存在的权限呢？例如要拿掉全部人的可执行权限，则： 123[root@www ~]# chmod a-x .bashrc[root@www ~]# ls -al .bashrc-rw-rw-rw- 1 root root 395 Jul 4 11:45 .bashrc Linux 文件与目录管理Linux 文件与目录管理我们知道Linux的目录结构为树状结构，最顶级的目录为根目录 /。 其他目录通过挂载可以将它们添加到树中，通过解除挂载可以移除它们。 在开始本教程前我们需要先知道什么是绝对路径与相对路径。 绝对路径：路径的写法，由根目录 / 写起，例如： /usr/share/doc 这个目录。 相对路径：路径的写法，不是由 / 写起，例如由 /usr/share/doc 要到 /usr/share/man 底下时，可以写成： cd ../man 这就是相对路径的写法啦！ 处理目录的常用命令接下来我们就来看几个常见的处理目录的命令吧： ls: 列出目录 cd：切换目录 pwd：显示目前的目录 mkdir：创建一个新的目录 rmdir：删除一个空的目录 cp: 复制文件或目录 rm: 移除文件或目录 mv: 移动文件与目录、文件重命名 你可以使用 man [命令] 来查看各个命令的使用文档，如 ：man cp。 ls (列出目录)在Linux系统当中， ls 命令可能是最常被运行的。 语法： 123[root@www ~]# ls [-aAdfFhilnrRSt] 目录名称[root@www ~]# ls [--color=&#123;never,auto,always&#125;] 目录名称[root@www ~]# ls [--full-time] 目录名称 选项与参数： -a ：全部的文件，连同隐藏档( 开头为 . 的文件) 一起列出来(常用) -d ：仅列出目录本身，而不是列出目录内的文件数据(常用) -l ：长数据串列出，包含文件的属性与权限等等数据；(常用) 将家目录下的所有文件列出来(含属性与隐藏档) 1[root@www ~]# ls -al ~ cd (切换目录)cd是Change Directory的缩写，这是用来变换工作目录的命令。 语法： 123456789101112131415 cd [相对路径或绝对路径]#使用 mkdir 命令创建w3cschool.cn目录[root@www ~]# mkdir w3cschool.cn#使用绝对路径切换到w3cschool.cn目录[root@www ~]# cd /root/w3cschool.cn/#使用相对路径切换到w3cschool.cn目录[root@www ~]# cd ./w3cschool.cn/# 表示回到自己的家目录，亦即是 /root 这个目录[root@www w3cschool.cn]# cd ~# 表示去到目前的上一级目录，亦即是 /root 的上一级目录的意思；[root@www ~]# cd .. 接下来大家多操作几次应该就可以很好的理解 cd 命令的。 pwd (显示目前所在的目录)pwd是Print Working Directory的缩写，也就是显示目前所在目录的命令。 123456789101112131415161718[root@www ~]# pwd [-P]选项与参数：-P ：显示出确实的路径，而非使用连结 (link) 路径。范例：单纯显示出目前的工作目录：[root@www ~]# pwd/root &lt;== 显示出目录啦～ 范例：显示出实际的工作目录，而非连结档本身的目录名而已 [root@www ~]# cd /var/mail &lt;==注意，/var/mail是一个连结档 [root@www mail]# pwd /var/mail &lt;==列出目前的工作目录 [root@www mail]# pwd -P /var/spool/mail &lt;==怎么回事？有没有加 -P 差很多～ [root@www mail]# ls -ld /var/mail lrwxrwxrwx 1 root root 10 Sep 4 17:54 /var/mail -&gt; spool/mail# 看到这里应该知道为啥了吧？因为 /var/mail 是连结档，连结到 /var/spool/mail # 所以，加上 pwd -P 的选项后，会不以连结档的数据显示，而是显示正确的完整路径啊！ mkdir (创建新目录)如果想要创建新的目录的话，那么就使用mkdir (make directory)吧。 语法： 1mkdir [-mp] 目录名称 选项与参数： -m ：配置文件的权限喔！直接配置，不需要看默认权限 (umask) 的脸色～ -p ：帮助你直接将所需要的目录(包含上一级目录)递回创建起来！ 范例：请到/tmp底下尝试创建数个新目录看看： 123456[root@www ~]# cd /tmp[root@www tmp]# mkdir test &lt;==创建一名为 test 的新目录 [root@www tmp]# mkdir test1/test2/test3/test4 mkdir: cannot create directory `test1/test2/test3/test4&apos;: No such file or directory &lt;== 没办法直接创建此目录啊！ [root@www tmp]# mkdir -p test1/test2/test3/test4 加了这个 -p 的选项，可以自行帮你创建多层目录！ 范例：创建权限为rwx–x–x的目录 12345[root@www tmp]# mkdir -m 711 test2[root@www tmp]# ls -ldrwxr-xr-x 3 root root 4096 Jul 18 12:50 testdrwxr-xr-x 3 root root 4096 Jul 18 12:53 test1drwx--x--x 2 root root 4096 Jul 18 12:54 test2 上面的权限部分，如果没有加上 -m 来强制配置属性，系统会使用默认属性。 如果我们使用 -m ，如上例我们给予 -m 711 来给予新的目录 drwx–x–x 的权限。 rmdir (删除空的目录)语法： 1rmdir [-p] 目录名称 选项与参数： -p ：连同上一级『空的』目录也一起删除 删除 w3cschool.cn 目录 1[root@www tmp]# rmdir w3cschool.cn/ 范例：将於mkdir范例中创建的目录(/tmp底下)删除掉！ 12345678910[root@www tmp]# ls -l &lt;==看看有多少目录存在？ drwxr-xr-x 3 root root 4096 Jul 18 12:50 test drwxr-xr-x 3 root root 4096 Jul 18 12:53 test1 drwx--x--x 2 root root 4096 Jul 18 12:54 test2 [root@www tmp]# rmdir test &lt;==可直接删除掉，没问题 [root@www tmp]# rmdir test1 &lt;==因为尚有内容，所以无法删除！ rmdir: `test1&apos;: Directory not empty [root@www tmp]# rmdir -p test1/test2/test3/test4 [root@www tmp]# ls -l &lt;==您看看，底下的输出中test与test1不见了！ drwx--x--x 2 root root 4096 Jul 18 12:54 test2 利用 -p 这个选项，立刻就可以将 test1/test2/test3/test4 一次删除。 不过要注意的是，这个 rmdir 仅能删除空的目录，你可以使用 rm 命令来删除非空目录。 cp (复制文件或目录)cp 即拷贝文件和目录。 语法: 12[root@www ~]# cp [-adfilprsu] 来源档(source) 目标档(destination)[root@www ~]# cp [options] source1 source2 source3 .... directory 选项与参数： -a ：相当於 -pdr 的意思，至於 pdr 请参考下列说明；(常用) -d ：若来源档为连结档的属性(link file)，则复制连结档属性而非文件本身； -f ：为强制(force)的意思，若目标文件已经存在且无法开启，则移除后再尝试一次； -i ：若目标档(destination)已经存在时，在覆盖时会先询问动作的进行(常用) -l ：进行硬式连结(hard link)的连结档创建，而非复制文件本身； -p ：连同文件的属性一起复制过去，而非使用默认属性(备份常用)； -r ：递回持续复制，用於目录的复制行为；(常用) -s ：复制成为符号连结档 (symbolic link)，亦即『捷径』文件； -u ：若 destination 比 source 旧才升级 destination ！ 用root身份，将家目录下的 .bashrc 复制到 /tmp 下，并更名为 bashr 123[root@www ~]# cp ~/.bashrc /tmp/bashrc[root@www ~]# cp -i ~/.bashrc /tmp/bashrccp: overwrite `/tmp/bashrc&apos;? n &lt;==n不覆盖，y为覆盖 rm (移除文件或目录)语法： 1rm [-fir] 文件或目录 选项与参数： -f ：就是 force 的意思，忽略不存在的文件，不会出现警告信息； -i ：互动模式，在删除前会询问使用者是否动作 -r ：递回删除啊！最常用在目录的删除了！这是非常危险的选项！！！ 将刚刚在 cp 的范例中创建的 bashrc 删除掉！ 12[root@www tmp]# rm -i bashrcrm: remove regular file `bashrc&apos;? y 如果加上 -i 的选项就会主动询问喔，避免你删除到错误的档名！ mv (移动文件与目录，或修改名称)语法： 12[root@www ~]# mv [-fiu] source destination[root@www ~]# mv [options] source1 source2 source3 .... directory 选项与参数： -f ：force 强制的意思，如果目标文件已经存在，不会询问而直接覆盖； -i ：若目标文件 (destination) 已经存在时，就会询问是否覆盖！ -u ：若目标文件已经存在，且 source 比较新，才会升级 (update) 复制一文件，创建一目录，将文件移动到目录中 1234[root@www ~]# cd /tmp[root@www tmp]# cp ~/.bashrc bashrc[root@www tmp]# mkdir mvtest[root@www tmp]# mv bashrc mvtest 将某个文件移动到某个目录去，就是这样做！ 将刚刚的目录名称更名为 mvtest2 1[root@www tmp]# mv mvtest mvtest2 Linux 文件内容查看Linux系统中使用以下命令来查看文件的内容： cat 由第一行开始显示文件内容 tac 从最后一行开始显示，可以看出 tac 是 cat 的倒着写！ nl 显示的时候，顺道输出行号！ more 一页一页的显示文件内容 less 与 more 类似，但是比 more 更好的是，他可以往前翻页！ head 只看头几行 tail 只看尾巴几行 你可以使用 man [命令]来查看各个命令的使用文档，如 ：man cp。 cat由第一行开始显示文件内容 语法： 1cat [-AbEnTv] 选项与参数： -A ：相当於 -vET 的整合选项，可列出一些特殊字符而不是空白而已； -b ：列出行号，仅针对非空白行做行号显示，空白行不标行号！ -E ：将结尾的断行字节 $ 显示出来； -n ：列印出行号，连同空白行也会有行号，与 -b 的选项不同； -T ：将 [tab] 按键以 ^I 显示出来； -v ：列出一些看不出来的特殊字符 检看 /etc/issue 这个文件的内容： 123[root@www ~]# cat /etc/issueCentOS release 6.4 (Final)Kernel \r on an \m tactac与cat命令刚好相反，文件内容从最后一行开始显示，可以看出 tac 是 cat 的倒着写！如： 1234[root@www ~]# tac /etc/issueKernel \r on an \mCentOS release 6.4 (Final) nl显示行号 语法： 1nl [-bnw] 文件 选项与参数： -b ：指定行号指定的方式，主要有两种：-b a ：表示不论是否为空行，也同样列出行号(类似 cat -n)；-b t ：如果有空行，空的那一行不要列出行号(默认值)； -n ：列出行号表示的方法，主要有三种：-n ln ：行号在萤幕的最左方显示；-n rn ：行号在自己栏位的最右方显示，且不加 0 ；-n rz ：行号在自己栏位的最右方显示，且加 0 ； -w ：行号栏位的占用的位数。 范例一：用 nl 列出 /etc/issue 的内容 123[root@www ~]# nl /etc/issue 1 CentOS release 6.4 (Final) 2 Kernel \r on an \m more一页一页翻动 12345678[root@www ~]# more /etc/man.config## Generated automatically from man.conf.in by the# configure script.## man.conf from man-1.6d....(中间省略)....--More--(28%) &lt;== 重点在这一行喔！你的光标也会在这里等待你的命令 在 more 这个程序的运行过程中，你有几个按键可以按的： 空白键 (space)：代表向下翻一页； Enter ：代表向下翻『一行』； /字串 ：代表在这个显示的内容当中，向下搜寻『字串』这个关键字； :f ：立刻显示出档名以及目前显示的行数； q ：代表立刻离开 more ，不再显示该文件内容。 b 或 [ctrl]-b ：代表往回翻页，不过这动作只对文件有用，对管线无用。 less一页一页翻动，以下实例输出/etc/man.config文件的内容： 12345678[root@www ~]# less /etc/man.config## Generated automatically from man.conf.in by the# configure script.## man.conf from man-1.6d....(中间省略)....: &lt;== 这里可以等待你输入命令！ less运行时可以输入的命令有： 空白键 ：向下翻动一页； [pagedown]：向下翻动一页； [pageup] ：向上翻动一页； /字串 ：向下搜寻『字串』的功能； ?字串 ：向上搜寻『字串』的功能； n ：重复前一个搜寻 (与 / 或 ? 有关！) N ：反向的重复前一个搜寻 (与 / 或 ? 有关！) q ：离开 less 这个程序； head取出文件前面几行 语法： 1head [-n number] 文件 选项与参数： -n ：后面接数字，代表显示几行的意思 1[root@www ~]# head /etc/man.config 默认的情况中，显示前面 10 行！若要显示前 20 行，就得要这样： 1[root@www ~]# head -n 20 /etc/man.config tail取出文件后面几行 语法： 1tail [-n number] 文件 选项与参数： -n ：后面接数字，代表显示几行的意思 -f ：表示持续侦测后面所接的档名，要等到按下[ctrl]-c才会结束tail的侦测 123[root@www ~]# tail /etc/man.config# 默认的情况中，显示最后的十行！若要显示最后的 20 行，就得要这样：[root@www ~]# tail -n 20 /etc/man.config Linux 用户和用户组管理Linux 用户和用户组管理Linux系统是一个多用户多任务的分时操作系统，任何一个要使用系统资源的用户，都必须首先向系统管理员申请一个账号，然后以这个账号的身份进入系统。 用户的账号一方面可以帮助系统管理员对使用系统的用户进行跟踪，并控制他们对系统资源的访问；另一方面也可以帮助用户组织文件，并为用户提供安全性保护。 每个用户账号都拥有一个惟一的用户名和各自的口令。 用户在登录时键入正确的用户名和口令后，就能够进入系统和自己的主目录。 实现用户账号的管理，要完成的工作主要有如下几个方面： 用户账号的添加、删除与修改。 用户口令的管理。 用户组的管理。 一、Linux系统用户账号的管理用户账号的管理工作主要涉及到用户账号的添加、修改和删除。 添加用户账号就是在系统中创建一个新账号，然后为新账号分配用户号、用户组、主目录和登录Shell等资源。刚添加的账号是被锁定的，无法使用。 1、添加新的用户账号使用useradd命令，其语法如下：1useradd 选项 用户名 参数说明： 选项: -c comment 指定一段注释性描述。 -d 目录 指定用户主目录，如果此目录不存在，则同时使用-m选项，可以创建主目录。 -g 用户组 指定用户所属的用户组。 -G 用户组，用户组 指定用户所属的附加组。 -s Shell文件 指定用户的登录Shell。 -u 用户号 指定用户的用户号，如果同时有-o选项，则可以重复使用其他用户的标识号。 用户名: 指定新账号的登录名。 实例11# useradd –d /usr/sam -m sam 此命令创建了一个用户sam，其中-d和-m选项用来为登录名sam产生一个主目录/usr/sam（/usr为默认的用户主目录所在的父目录）。 实例21# useradd -s /bin/sh -g group –G adm,root gem 此命令新建了一个用户gem，该用户的登录Shell是 /bin/sh，它属于group用户组，同时又属于adm和root用户组，其中group用户组是其主组。 这里可能新建组：#groupadd group及groupadd adm 增加用户账号就是在/etc/passwd文件中为新用户增加一条记录，同时更新其他系统文件如/etc/shadow, /etc/group等。 Linux提供了集成的系统管理工具userconf，它可以用来对用户账号进行统一管理。 3、删除帐号如果一个用户的账号不再使用，可以从系统中删除。删除用户账号就是要将/etc/passwd等系统文件中的该用户记录删除，必要时还删除用户的主目录。 删除一个已有的用户账号使用userdel命令，其格式如下： 1userdel 选项 用户名 常用的选项是-r，它的作用是把用户的主目录一起删除。 例如： 1# userdel sam 此命令删除用户sam在系统文件中（主要是/etc/passwd, /etc/shadow, /etc/group等）的记录，同时删除用户的主目录。 4、修改帐号修改用户账号就是根据实际情况更改用户的有关属性，如用户号、主目录、用户组、登录Shell等。 修改已有用户的信息使用usermod命令，其格式如下： 1usermod 选项 用户名 常用的选项包括-c, -d, -m, -g, -G, -s, -u以及-o等，这些选项的意义与useradd命令中的选项一样，可以为用户指定新的资源值。 另外，有些系统可以使用选项：-l 新用户名 这个选项指定一个新的账号，即将原来的用户名改为新的用户名。 例如： 1# usermod -s /bin/ksh -d /home/z –g developer sam 此命令将用户sam的登录Shell修改为ksh，主目录改为/home/z，用户组改为developer。 5、用户口令的管理用户管理的一项重要内容是用户口令的管理。用户账号刚创建时没有口令，但是被系统锁定，无法使用，必须为其指定口令后才可以使用，即使是指定空口令。 指定和修改用户口令的Shell命令是passwd。超级用户可以为自己和其他用户指定口令，普通用户只能用它修改自己的口令。命令的格式为： 1passwd 选项 用户名 可使用的选项： -l 锁定口令，即禁用账号。 -u 口令解锁。 -d 使账号无口令。 -f 强迫用户下次登录时修改口令。 如果默认用户名，则修改当前用户的口令。 例如，假设当前用户是sam，则下面的命令修改该用户自己的口令： 1234$ passwd Old password:****** New password:******* Re-enter new password:******* 如果是超级用户，可以用下列形式指定任何用户的口令： 123# passwd sam New password:******* Re-enter new password:******* 普通用户修改自己的口令时，passwd命令会先询问原口令，验证后再要求用户输入两遍新口令，如果两次输入的口令一致，则将这个口令指定给用户；而超级用户为用户指定口令时，就不需要知道原口令。 为了系统安全起见，用户应该选择比较复杂的口令，例如最好使用8位长的口令，口令中包含有大写、小写字母和数字，并且应该与姓名、生日等不相同。 为用户指定空口令时，执行下列形式的命令： 1# passwd -d sam 此命令将用户sam的口令删除，这样用户sam下一次登录时，系统就不再询问口令。 passwd命令还可以用-l(lock)选项锁定某一用户，使其不能登录，例如： 1# passwd -l sam 二、Linux系统用户组的管理每个用户都有一个用户组，系统可以对一个用户组中的所有用户进行集中管理。不同Linux 系统对用户组的规定有所不同，如Linux下的用户属于与它同名的用户组，这个用户组在创建用户时同时创建。 用户组的管理涉及用户组的添加、删除和修改。组的增加、删除和修改实际上就是对/etc/group文件的更新。 1、增加一个新的用户组使用groupadd命令。其格式如下：1groupadd 选项 用户组 可以使用的选项有： -g GID 指定新用户组的组标识号（GID）。 -o 一般与-g选项同时使用，表示新用户组的GID可以与系统已有用户组的GID相同。 实例1：1# groupadd group1 此命令向系统中增加了一个新组group1，新组的组标识号是在当前已有的最大组标识号的基础上加1。 实例2：1# groupadd -g 101 group2 此命令向系统中增加了一个新组group2，同时指定新组的组标识号是101。 2、如果要删除一个已有的用户组，使用groupdel命令，其格式如下：1groupdel 用户组 例如：1# groupdel group1 此命令从系统中删除组group1。 3、修改用户组的属性使用groupmod命令。其语法如下：1groupmod 选项 用户组 常用的选项有： -g GID 为用户组指定新的组标识号。 -o 与-g选项同时使用，用户组的新GID可以与系统已有用户组的GID相同。 -n新用户组 将用户组的名字改为新名字 实例1：1# groupmod -g 102 group2 此命令将组group2的组标识号修改为102。 实例2：1# groupmod –g 10000 -n group3 group2 此命令将组group2的标识号改为10000，组名修改为group3。 4、如果一个用户同时属于多个用户组，那么用户可以在用户组之间切换，以便具有其他用户组的权限。用户可以在登录后，使用命令newgrp切换到其他用户组，这个命令的参数就是目的用户组。例如： 1$ newgrp root 这条命令将当前用户切换到root用户组，前提条件是root用户组确实是该用户的主组或附加组。类似于用户账号的管理，用户组的管理也可以通过集成的系统管理工具来完成。 三、与用户账号有关的系统文件完成用户管理的工作有许多种方法，但是每一种方法实际上都是对有关的系统文件进行修改。 与用户和用户组相关的信息都存放在一些系统文件中，这些文件包括/etc/passwd, /etc/shadow, /etc/group等。 下面分别介绍这些文件的内容。 1、/etc/passwd文件是用户管理工作涉及的最重要的一个文件。Linux系统中的每个用户都在/etc/passwd文件中有一个对应的记录行，它记录了这个用户的一些基本属性。 这个文件对所有用户都是可读的。它的内容类似下面的例子： 12345678910111213＃ cat /etc/passwdroot:x:0:0:Superuser:/:daemon:x:1:1:System daemons:/etc:bin:x:2:2:Owner of system commands:/bin:sys:x:3:3:Owner of system files:/usr/sys:adm:x:4:4:System accounting:/usr/adm:uucp:x:5:5:UUCP administrator:/usr/lib/uucp:auth:x:7:21:Authentication administrator:/tcb/files/auth:cron:x:9:16:Cron daemon:/usr/spool/cron:listen:x:37:4:Network daemon:/usr/net/nls:lp:x:71:18:Printer administrator:/usr/spool/lp:sam:x:200:50:Sam san:/usr/sam:/bin/sh 从上面的例子我们可以看到，/etc/passwd中一行记录对应着一个用户，每行记录又被冒号(:)分隔为7个字段，其格式和具体含义如下： 1用户名:口令:用户标识号:组标识号:注释性描述:主目录:登录Shell 1）”用户名”是代表用户账号的字符串。通常长度不超过8个字符，并且由大小写字母和/或数字组成。登录名中不能有冒号(:)，因为冒号在这里是分隔符。 为了兼容起见，登录名中最好不要包含点字符(.)，并且不使用连字符(-)和加号(+)打头。 2）“口令”一些系统中，存放着加密后的用户口令字。虽然这个字段存放的只是用户口令的加密串，不是明文，但是由于/etc/passwd文件对所有用户都可读，所以这仍是一个安全隐患。因此，现在许多Linux 系统（如SVR4）都使用了shadow技术，把真正的加密后的用户口令字存放到/etc/shadow文件中，而在/etc/passwd文件的口令字段中只存放一个特殊的字符，例如“x”或者“*”。 3）“用户标识号”是一个整数，系统内部用它来标识用户。一般情况下它与用户名是一一对应的。如果几个用户名对应的用户标识号是一样的，系统内部将把它们视为同一个用户，但是它们可以有不同的口令、不同的主目录以及不同的登录Shell等。 通常用户标识号的取值范围是0～65 535。0是超级用户root的标识号，1～99由系统保留，作为管理账号，普通用户的标识号从100开始。在Linux系统中，这个界限是500。 4）“组标识号”字段记录的是用户所属的用户组。它对应着/etc/group文件中的一条记录。 5)“注释性描述”字段记录着用户的一些个人情况。例如用户的真实姓名、电话、地址等，这个字段并没有什么实际的用途。在不同的Linux 系统中，这个字段的格式并没有统一。在许多Linux系统中，这个字段存放的是一段任意的注释性描述文字，用做finger命令的输出。 6)“主目录”，也就是用户的起始工作目录。它是用户在登录到系统之后所处的目录。在大多数系统中，各用户的主目录都被组织在同一个特定的目录下，而用户主目录的名称就是该用户的登录名。各用户对自己的主目录有读、写、执行（搜索）权限，其他用户对此目录的访问权限则根据具体情况设置。 7)用户登录后，要启动一个进程，负责将用户的操作传给内核，这个进程是用户登录到系统后运行的命令解释器或某个特定的程序，即Shell。Shell是用户与Linux系统之间的接口。Linux的Shell有许多种，每种都有不同的特点。常用的有sh(Bourne Shell), csh(C Shell), ksh(Korn Shell), tcsh(TENEX/TOPS-20 type C Shell), bash(Bourne Again Shell)等。 系统管理员可以根据系统情况和用户习惯为用户指定某个Shell。如果不指定Shell，那么系统使用sh为默认的登录Shell，即这个字段的值为/bin/sh。 用户的登录Shell也可以指定为某个特定的程序（此程序不是一个命令解释器）。 利用这一特点，我们可以限制用户只能运行指定的应用程序，在该应用程序运行结束后，用户就自动退出了系统。有些Linux 系统要求只有那些在系统中登记了的程序才能出现在这个字段中。 8)系统中有一类用户称为伪用户（psuedo users）。这些用户在/etc/passwd文件中也占有一条记录，但是不能登录，因为它们的登录Shell为空。它们的存在主要是方便系统管理，满足相应的系统进程对文件属主的要求。 常见的伪用户如下所示： 1234567伪 用 户 含 义 bin 拥有可执行的用户命令文件 sys 拥有系统文件 adm 拥有帐户文件 uucp UUCP使用 lp lp或lpd子系统使用 nobody NFS使用 拥有帐户文件1、除了上面列出的伪用户外，还有许多标准的伪用户，例如：audit, cron, mail, usenet等，它们也都各自为相关的进程和文件所需要。由于/etc/passwd文件是所有用户都可读的，如果用户的密码太简单或规律比较明显的话，一台普通的计算机就能够很容易地将它破解，因此对安全性要求较高的Linux系统都把加密后的口令字分离出来，单独存放在一个文件中，这个文件是/etc/shadow文件。 有超级用户才拥有该文件读权限，这就保证了用户密码的安全性。 2、/etc/shadow中的记录行与/etc/passwd中的一一对应，它由pwconv命令根据/etc/passwd中的数据自动产生它的文件格式与/etc/passwd类似，由若干个字段组成，字段之间用”:”隔开。这些字段是： 1登录名:加密口令:最后一次修改时间:最小时间间隔:最大时间间隔:警告时间:不活动时间:失效时间:标志 “登录名”是与/etc/passwd文件中的登录名相一致的用户账号 “口令”字段存放的是加密后的用户口令字，长度为13个字符。如果为空，则对应用户没有口令，登录时不需要口令；如果含有不属于集合 { ./0-9A-Za-z }中的字符，则对应的用户不能登录。 “最后一次修改时间”表示的是从某个时刻起，到用户最后一次修改口令时的天数。时间起点对不同的系统可能不一样。例如在SCO Linux 中，这个时间起点是1970年1月1日。 “最小时间间隔”指的是两次修改口令之间所需的最小天数。 “最大时间间隔”指的是口令保持有效的最大天数。 “警告时间”字段表示的是从系统开始警告用户到用户密码正式失效之间的天数。 “不活动时间”表示的是用户没有登录活动但账号仍能保持有效的最大天数。 “失效时间”字段给出的是一个绝对的天数，如果使用了这个字段，那么就给出相应账号的生存期。期满后，该账号就不再是一个合法的账号，也就不能再用来登录了。 下面是/etc/shadow的一个例子： 1234567891011121314＃ cat /etc/shadowroot:Dnakfw28zf38w:8764:0:168:7:::daemon:*::0:0::::bin:*::0:0::::sys:*::0:0::::adm:*::0:0::::uucp:*::0:0::::nuucp:*::0:0::::auth:*::0:0::::cron:*::0:0::::listen:*::0:0::::lp:*::0:0::::sam:EkdiSECLWPdSa:9740:0:0:::: 3、用户组的所有信息都存放在/etc/group文件中。将用户分组是Linux 系统中对用户进行管理及控制访问权限的一种手段。 每个用户都属于某个用户组；一个组中可以有多个用户，一个用户也可以属于不同的组。 当一个用户同时是多个组中的成员时，在/etc/passwd文件中记录的是用户所属的主组，也就是登录时所属的默认组，而其他组称为附加组。 用户要访问属于附加组的文件时，必须首先使用newgrp命令使自己成为所要访问的组中的成员。 用户组的所有信息都存放在/etc/group文件中。此文件的格式也类似于/etc/passwd文件，由冒号(:)隔开若干个字段，这些字段有： 1组名:口令:组标识号:组内用户列表 “组名”是用户组的名称，由字母或数字构成。与/etc/passwd中的登录名一样，组名不应重复。 “口令”字段存放的是用户组加密后的口令字。一般Linux 系统的用户组都没有口令，即这个字段一般为空，或者是*。 “组标识号”与用户标识号类似，也是一个整数，被系统内部用来标识组。 “组内用户列表”是属于这个组的所有用户的列表/b]，不同用户之间用逗号(,)分隔。这个用户组可能是用户的主组，也可能是附加组。 /etc/group文件的一个例子如下： 1234567root::0:rootbin::2:root,binsys::3:root,uucpadm::4:root,admdaemon::5:root,daemonlp::7:root,lpusers::20:root,sam 四、批量添加用户添加和删除用户对每位Linux系统管理员都是轻而易举的事，比较棘手的是如果要添加几十个、上百个甚至上千个用户时，我们不太可能还使用useradd一个一个地添加，必然要找一种简便的创建大量用户的方法。Linux系统提供了创建大量用户的工具，可以让您立即创建大量用户，方法如下： （1）先编辑一个文本用户文件。每一列按照/etc/passwd密码文件的格式书写，要注意每个用户的用户名、UID、宿主目录都不可以相同，其中密码栏可以留做空白或输入x号。一个范例文件user.txt内容如下： 123456user001::600:100:user:/home/user001:/bin/bashuser002::601:100:user:/home/user002:/bin/bashuser003::602:100:user:/home/user003:/bin/bashuser004::603:100:user:/home/user004:/bin/bashuser005::604:100:user:/home/user005:/bin/bashuser006::605:100:user:/home/user006:/bin/bash （2）以root身份执行命令 /usr/sbin/newusers，从刚创建的用户文件user.txt中导入数据，创建用户：1# newusers &lt; user.txt 然后可以执行命令 vipw 或 vi /etc/passwd 检查 /etc/passwd 文件是否已经出现这些用户的数据，并且用户的宿主目录是否已经创建。 （3）执行命令/usr/sbin/pwunconv。将 /etc/shadow 产生的 shadow 密码解码，然后回写到 /etc/passwd 中，并将/etc/shadow的shadow密码栏删掉。这是为了方便下一步的密码转换工作，即先取消 shadow password 功能。 1# pwunconv （4）编辑每个用户的密码对照文件。范例文件 passwd.txt 内容如下： 123456user001:密码user002:密码user003:密码user004:密码user005:密码user006:密码 （5）以root身份执行命令 /usr/sbin/chpasswd。创建用户密码，chpasswd 会将经过 /usr/bin/passwd 命令编码过的密码写入 /etc/passwd 的密码栏。 1# chpasswd &lt; passwd.txt （6）确定密码经编码写入/etc/passwd的密码栏后。执行命令 /usr/sbin/pwconv 将密码编码为 shadow password，并将结果写入 /etc/shadow。 1# pwconv 这样就完成了大量用户的创建了，之后您可以到/home下检查这些用户宿主目录的权限设置是否都正确，并登录验证用户密码是否正确。 Linux vi/vim所有的 Unix Like 系统都会内建 vi 文书编辑器，其他的文书编辑器则不一定会存在。但是目前我们使用比较多的是 vim 编辑器。 vim 具有程序编辑的能力，可以主动的以字体颜色辨别语法的正确性，方便程序设计。 什么是 vim？Vim是从 vi 发展出来的一个文本编辑器。代码补完、编译及错误跳转等方便编程的功能特别丰富，在程序员中被广泛使用。 简单的来说， vi 是老式的字处理器，不过功能已经很齐全了，但是还是有可以进步的地方。 vim 则可以说是程序开发者的一项很好用的工具。 连 vim 的官方网站 (http://www.vim.org) 自己也说 vim 是一个程序开发工具而不是文字处理软件。 vim 键盘图： vi/vim 的使用基本上 vi/vim 共分为三种模式，分别是命令模式（Command mode），输入模式（Insert mode）和底线命令模式（Last line mode）。 这三种模式的作用分别是： 命令模式： 用户刚刚启动 vi/vim，便进入了命令模式。 此状态下敲击键盘动作会被Vim识别为命令，而非输入字符。比如我们此时按下i，并不会输入一个字符，i被当作了一个命令。 以下是常用的几个命令： i 切换到输入模式，以输入字符。 x 删除当前光标所在处的字符。 : 切换到底线命令模式，以在最底一行输入命令。 若想要编辑文本：启动Vim，进入了命令模式，按下i，切换到输入模式。 命令模式只有一些最基本的命令，因此仍要依靠底线命令模式输入更多命令。 输入模式 在命令模式下按下i就进入了输入模式。 在输入模式中，可以使用以下按键： 字符按键以及Shift组合，输入字符 ENTER，回车键，换行 BACK SPACE，退格键，删除光标前一个字符 DEL，删除键，删除光标后一个字符 方向键，在文本中移动光标 HOME/END，移动光标到行首/行尾 Page Up/Page Down，上/下翻页 Insert，切换光标为输入/替换模式，光标将变成竖线/下划线 ESC，退出输入模式，切换到命令模式 底线命令模式 在命令模式下按下:（英文冒号）就进入了底线命令模式。 底线命令模式可以输入单个或多个字符的命令，可用的命令非常多。 在底线命令模式中，基本的命令有（已经省略了冒号）： q 退出程序 w 保存文件 按ESC键可随时退出底线命令模式。 简单的说，我们可以将这三个模式想成底下的图标来表示： vi/vim 使用实例使用 vi/vim 进入命令模式如果你想要使用 vi 来建立一个名为 test.txt 的文件时，你可以这样做： 1[root@www ~]# vi test.txt 直接输入 vi 文件名 就能够进入 vi 的命令模式了。请注意，记得 vi 后面一定要加文件名，不管该文件存在与否！ 按下 i 进入输入模式，开始编辑文字在命令模式之中，只要按下 i, o, a 等字符就可以进入输入模式了！ 在输入模式当中，你可以发现在左下角状态栏中会出现 –INSERT- 的字样，那就是可以输入任意字符的提示。 这个时候，键盘上除了 [Esc] 这个按键之外，其他的按键都可以视作为一般的输入按钮了，所以你可以进行任何的编辑。 按下 [ESC] 按钮回到命令模式好了，假设我已经按照上面的样式给他编辑完毕了，那么应该要如何退出呢？是的！没错！就是给他按下 [Esc] 这个按钮即可！马上你就会发现画面左下角的 – INSERT – 不见了！ 在命令模式中按下 :wq 储存后离开 viOK，我们要存档了，存盘并离开的指令很简单，输入『:wq』即可保存离开！ OK! 这样我们就成功创建了一个 test.txt 的文件。是不是很简单。 vi/vim 按键说明除了上面简易范例的 i, [Esc], :wq 之外，其实 vim 还有非常多的按键可以使用。 第一部份：命令模式可用的按钮说明，光标移动、复制贴上、搜寻取代等 移动光标的方法 h 或 向左箭头键(←) 光标向左移动一个字符 j 或 向下箭头键(↓) 光标向下移动一个字符 k 或 向上箭头键(↑) 光标向上移动一个字符 l 或 向右箭头键(→) 光标向右移动一个字符 如果你将右手放在键盘上的话，你会发现 hjkl 是排列在一起的，因此可以使用这四个按钮来移动光标。 如果想要进行多次移动的话，例如向下移动 30 行，可以使用 “30j” 或 “30↓” 的组合按键， 亦即加上想要进行的次数(数字)后，按下动作即可！ [Ctrl] + [f] 屏幕『向下』移动一页，相当于 [Page Down]按键 (常用) [Ctrl] + [b] 屏幕『向上』移动一页，相当于 [Page Up] 按键 (常用) [Ctrl] + [d] 屏幕『向下』移动半页 [Ctrl] + [u] 屏幕『向上』移动半页 + 光标移动到非空格符的下一列 - 光标移动到非空格符的上一列 n 那个 n 表示『数字』，例如 20 。按下数字后再按空格键，光标会向右移动这一行的 n 个字符。例如 20 则光标会向后面移动 20 个字符距离。 0 或功能键[Home] 这是数字『 0 』：移动到这一行的最前面字符处 (常用) $ 或功能键[End] 移动到这一行的最后面字符处(常用) H 光标移动到这个屏幕的最上方那一行的第一个字符 M 光标移动到这个屏幕的中央那一行的第一个字符 L 光标移动到这个屏幕的最下方那一行的第一个字符 G 移动到这个档案的最后一行(常用) nG n 为数字。移动到这个档案的第 n 行。例如 20G 则会移动到这个档案的第 20 行(可配合 :set nu) gg 移动到这个档案的第一行，相当于 1G 啊！ (常用) n n 为数字。光标向下移动 n 行(常用) 搜寻与取代 /word 向光标之下寻找一个名称为 word 的字符串。例如要在档案内搜寻 vbird 这个字符串，就输入 /vbird 即可！ (常用) ?word 向光标之上寻找一个字符串名称为 word 的字符串。 n 这个 n 是英文按键。代表重复前一个搜寻的动作。举例来说， 如果刚刚我们执行 /vbird 去向下搜寻 vbird 这个字符串，则按下 n 后，会向下继续搜寻下一个名称为 vbird 的字符串。如果是执行 ?vbird 的话，那么按下 n 则会向上继续搜寻名称为 vbird 的字符串！ N 这个 N 是英文按键。与 n 刚好相反，为『反向』进行前一个搜寻动作。 例如 /vbird 后，按下 N 则表示『向上』搜寻 vbird 。 使用 /word 配合 n 及 N 是非常有帮助的！可以让你重复的找到一些你搜寻的关键词！ :n1,n2s/word1/word2/g n1 与 n2 为数字。在第 n1 与 n2 行之间寻找 word1 这个字符串，并将该字符串取代为 word2 ！举例来说，在 100 到 200 行之间搜寻 vbird 并取代为 VBIRD 则： 『:100,200s/vbird/VBIRD/g』。(常用) :1,$s/word1/word2/g 从第一行到最后一行寻找 word1 字符串，并将该字符串取代为 word2 ！(常用) :1,$s/word1/word2/gc 从第一行到最后一行寻找 word1 字符串，并将该字符串取代为 word2 ！且在取代前显示提示字符给用户确认 (confirm) 是否需要取代！(常用) 删除、复制与贴上 x, X 在一行字当中，x 为向后删除一个字符 (相当于 [del] 按键)， X 为向前删除一个字符(相当于 [backspace] 亦即是退格键) (常用) nx n 为数字，连续向后删除 n 个字符。举例来说，我要连续删除 10 个字符， 『10x』。 dd 删除游标所在的那一整行(常用) ndd n 为数字。删除光标所在的向下 n 行，例如 20dd 则是删除 20 行 (常用) d1G 删除光标所在到第一行的所有数据 dG 删除光标所在到最后一行的所有数据 d$ 删除游标所在处，到该行的最后一个字符 d0 那个是数字的 0 ，删除游标所在处，到该行的最前面一个字符 yy 复制游标所在的那一行(常用) nyy n 为数字。复制光标所在的向下 n 列，例如 20yy 则是复制 20 列(常用) y1G 复制游标所在列到第一列的所有数据 yG 复制游标所在列到最后一列的所有数据 y0 复制光标所在的那个字符到该行行首的所有数据 y$ 复制光标所在的那个字符到该行行尾的所有数据 p, P p 为将已复制的数据在光标下一行贴上，P 则为贴在游标上一行！ 举例来说，我目前光标在第 20 行，且已经复制了 10 行数据。则按下 p 后， 那 10 行数据会贴在原本的 20 行之后，亦即由 21 行开始贴。但如果是按下 P 呢？ 那么原本的第 20 行会被推到变成 30 行。 (常用) J 将光标所在列与下一列的数据结合成同一列 c 重复删除多个数据，例如向下删除 10 行，[ 10cj ] u 复原前一个动作。(常用) [Ctrl]+r 重做上一个动作。(常用) 这个 u 与 [Ctrl]+r 是很常用的指令！一个是复原，另一个则是重做一次～ 利用这两个功能按键，你的编辑，嘿嘿！很快乐的啦！ . 不要怀疑！这就是小数点！意思是重复前一个动作的意思。 如果你想要重复删除、重复贴上等等动作，按下小数点『.』就好了！ (常用) 第二部份：命令模式切换到输入模式的可用的按钮说明 进入输入或取代的编辑模式 i, I 进入输入模式(Insert mode)： i 为『从目前光标所在处输入』， I 为『在目前所在行的第一个非空格符处开始输入』。 (常用) a, A 进入输入模式(Insert mode)： a 为『从目前光标所在的下一个字符处开始输入』， A 为『从光标所在行的最后一个字符处开始输入』。(常用) o, O 进入输入模式(Insert mode)： 这是英文字母 o 的大小写。o 为『在目前光标所在的下一行处输入新的一行』； O 为在目前光标所在处的上一行输入新的一行！(常用) r, R 进入取代模式(Replace mode)： r 只会取代光标所在的那一个字符一次；R会一直取代光标所在的文字，直到按下 ESC 为止；(常用) 上面这些按键中，在 vi 画面的左下角处会出现『–INSERT–』或『–REPLACE–』的字样。 由名称就知道该动作了吧！！特别注意的是，我们上面也提过了，你想要在档案里面输入字符时， 一定要在左下角处看到 INSERT 或 REPLACE 才能输入喔！ [Esc] 退出编辑模式，回到命令模式中(常用) 第三部份：命令模式切换到底线命令模式的可用的按钮说明 底线命令模式的储存、离开等指令 :w 将编辑的数据写入硬盘档案中(常用) :w! 若文件属性为『只读』时，强制写入该档案。不过，到底能不能写入， 还是跟你对该档案的档案权限有关啊！ :q 离开 vi (常用) :q! 若曾修改过档案，又不想储存，使用 ! 为强制离开不储存档案。 注意一下啊，那个惊叹号 (!) 在 vi 当中，常常具有『强制』的意思～ :wq 储存后离开，若为 :wq! 则为强制储存后离开 (常用) ZZ 这是大写的 Z 喔！若档案没有更动，则不储存离开，若档案已经被更动过，则储存后离开！ :w [filename] 将编辑的数据储存成另一个档案（类似另存新档） :r [filename] 在编辑的数据中，读入另一个档案的数据。亦即将 『filename』 这个档案内容加到游标所在行后面 :n1,n2 w [filename] 将 n1 到 n2 的内容储存成 filename 这个档案。 :! command 暂时离开 vi 到指令列模式下执行 command 的显示结果！例如 『:! ls /home』即可在 vi 当中察看 /home 底下以 ls 输出的档案信息！ vim 环境的变更 :set nu 显示行号，设定之后，会在每一行的前缀显示该行的行号 :set nonu 与 set nu 相反，为取消行号！ 特别注意，在 vi/vim 中，数字是很有意义的！数字通常代表重复做几次的意思！ 也有可能是代表去到第几个什么什么的意思。 举例来说，要删除 50 行，则是用 『50dd』 对吧！ 数字加在动作之前，如我要向下移动 20 行呢？那就是『20j』或者是『20↓』即可。 linux yum 命令linux yum 命令yum（ Yellow dog Updater, Modified）是一个在Fedora和RedHat以及SUSE中的Shell前端软件包管理器。 基於RPM包管理，能够从指定的服务器自动下载RPM包并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软体包，无须繁琐地一次次下载、安装。 yum提供了查找、安装、删除某一个、一组甚至全部软件包的命令，而且命令简洁而又好记。 yum 语法1yum [options] [command] [package ...] options：可选，选项包括-h（帮助），-y（当安装过程提示选择全部为”yes”），-q（不显示安装的过程）等等。 command：要进行的操作。 package操作的对象。 yum常用命令 1.列出所有可更新的软件清单命令：yum check-update 2.更新所有软件命令：yum update 3.仅安装指定的软件命令：yum install &lt;package_name&gt; 4.仅更新指定的软件命令：yum update &lt;package_name&gt; 5.列出所有可安裝的软件清单命令：yum list 6.删除软件包命令：yum remove &lt;package_name&gt; 7.查找软件包 命令：yum search 8.清除缓存命令: yum clean packages: 清除缓存目录下的软件包 yum clean headers: 清除缓存目录下的 headers yum clean oldheaders: 清除缓存目录下旧的 headers yum clean, yum clean all (= yum clean packages; yum clean oldheaders) :清除缓存目录下的软件包及旧的headers 实例 1安装 pam-devel 123456789101112[root@www ~]# yum install pam-develSetting up Install ProcessParsing package install argumentsResolving Dependencies &lt;==先检查软件的属性相依问题 --&gt; Running transaction check---&gt; Package pam-devel.i386 0:0.99.6.2-4.el5 set to be updated--&gt; Processing Dependency: pam = 0.99.6.2-4.el5 for package: pam-devel--&gt; Running transaction check---&gt; Package pam.i386 0:0.99.6.2-4.el5 set to be updatedfilelists.xml.gz 100% |=========================| 1.6 MB 00:05filelists.xml.gz 100% |=========================| 138 kB 00:00-&gt; Finished Dependency Resolution……(省略) 实例 2移除 pam-devel 12345678910111213141516171819[root@www ~]# yum remove pam-develSetting up Remove ProcessResolving Dependencies &lt;==同样的，先解决属性相依的问题 --&gt; Running transaction check---&gt; Package pam-devel.i386 0:0.99.6.2-4.el5 set to be erased--&gt; Finished Dependency ResolutionDependencies Resolved============================================================================= Package Arch Version Repository Size=============================================================================Removing: pam-devel i386 0.99.6.2-4.el5 installed 495 kTransaction Summary=============================================================================Install 0 Package(s)Update 0 Package(s)Remove 1 Package(s) &lt;==还好，并没有属性相依的问题，单纯移除一个软件 Is this ok [y/N]: y Downloading Packages: Running rpm_check_debug Running Transaction Test Finished Transaction Test Transaction Test Succeeded Running Transaction Erasing : pam-devel ######################### [1/1] Removed: pam-devel.i386 0:0.99.6.2-4.el5 Complete! 实例 3利用 yum 的功能，找出以 pam 为开头的软件名称有哪些？ 123456789[root@www ~]# yum list pam*Installed Packagespam.i386 0.99.6.2-3.27.el5 installedpam_ccreds.i386 3-5 installedpam_krb5.i386 2.2.14-1 installedpam_passwdqc.i386 1.0.2-1.2.2 installedpam_pkcs11.i386 0.5.3-23 installedpam_smb.i386 1.1.7-7.2.1 installedAvailable Packages &lt;==底下则是『可升级』的或『未安装』的 pam.i386 0.99.6.2-4.el5 base pam-devel.i386 0.99.6.2-4.el5 base pam_krb5.i386 2.2.14-10 base 国内 yum 源网易（163）yum源是国内最好的yum源之一 ，无论是速度还是软件版本，都非常的不错。 将yum源设置为163 yum，可以提升软件包安装和更新的速度，同时避免一些常见软件版本无法找到。 安装步骤首先备份/etc/yum.repos.d/CentOS-Base.repo 1mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup 下载对应版本repo文件, 放入/etc/yum.repos.d/(操作前请做好相应备份) CentOS5 ：http://mirrors.163.com/.help/CentOS5-Base-163.repo CentOS6 ：http://mirrors.163.com/.help/CentOS6-Base-163.repo 运行以下命令生成缓存 12yum clean allyum makecache 除了网易之外，国内还有其他不错的yum源，比如中科大和搜狐。 中科大的yum源，安装方法查看：https://lug.ustc.edu.cn/wiki/mirrors/help/centos sohu的yum源安装方法查看: http://mirrors.sohu.com/help/centos.html]]></content>
      <categories>
        <category>-linux</category>
      </categories>
      <tags>
        <tag>-linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[md语法]]></title>
    <url>%2F2015%2F02%2F15%2Fmd%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[Markdown 编辑器语法指南基本技巧代码如果你只想高亮语句中的某个函数名或关键字，可以使用 function_name() 实现 通常编辑器根据代码片段适配合适的高亮方法，但你也可以用 包裹一段代码，并指定一种语言12 ​1234$(document).ready(function () &#123; alert('hello world');&#125;);​ 1234支持的语言：`1c, abnf, accesslog, actionscript, ada, apache, applescript, arduino, armasm, asciidoc, aspectj, autohotkey, autoit, avrasm, awk, axapta, bash, basic, bnf, brainfuck, cal, capnproto, ceylon, clean, clojure, clojure-repl, cmake, coffeescript, coq, cos, cpp, crmsh, crystal, cs, csp, css, d, dart, delphi, diff, django, dns, dockerfile, dos, dsconfig, dts, dust, ebnf, elixir, elm, erb, erlang, erlang-repl, excel, fix, flix, fortran, fsharp, gams, gauss, gcode, gherkin, glsl, go, golo, gradle, groovy, haml, handlebars, haskell, haxe, hsp, htmlbars, http, hy, inform7, ini, irpf90, java, javascript, json, julia, kotlin, lasso, ldif, leaf, less, lisp, livecodeserver, livescript, llvm, lsl, lua, makefile, markdown, mathematica, matlab, maxima, mel, mercury, mipsasm, mizar, mojolicious, monkey, moonscript, n1ql, nginx, nimrod, nix, nsis, objectivec, ocaml, openscad, oxygene, parser3, perl, pf, php, pony, powershell, processing, profile, prolog, protobuf, puppet, purebasic, python, q, qml, r, rib, roboconf, rsl, ruby, ruleslanguage, rust, scala, scheme, scilab, scss, smali, smalltalk, sml, sqf, sql, stan, stata, step21, stylus, subunit, swift, taggerscript, tap, tcl, tex, thrift, tp, twig, typescript, vala, vbnet, vbscript, vbscript-html, verilog, vhdl, vim, x86asm, xl, xml, xquery, yaml, zephir`也可以使用 4 空格缩进，再贴上代码，实现相同的的效果 def g(x): yield from range(x, 0, -1) yield from range(x) 12如你不需要代码高亮，可以用下面的方法禁用： ​1​ 1234### 标题文章内容较多时，可以用标题分段： 标题1标题2大标题小标题12### 粗斜体 斜体文本 斜体文本粗体文本 粗体文本粗斜体文本 粗斜体文本1234### 链接常用链接方法 文字链接 链接名称网址链接 http://链接网址12高级链接技巧 这个链接用 1 作为网址变量 Google.这个链接用 yahoo 作为网址变量 Yahoo!.然后在文档的结尾为变量赋值（网址） 1234### 列表普通无序列表 列表文本前使用 [减号+空格] 列表文本前使用 [加号+空格] 列表文本前使用 [星号+空格]12普通有序列表 列表前使用 [数字+空格] 我们会自动帮你添加数字 不用担心数字不对，显示的时候我们会自动把这行的 7 纠正为 3 12列表嵌套 列出所有元素： 无序列表元素 A 元素 A 的有序子列表 前面加四个空格 列表里的多段换行： 前面必须加四个空格， 这样换行，整体的格式不会乱 列表里引用： 前面空一行仍然需要在 &gt; 前面加四个空格 列表里代码段： 1前面四个空格，之后按代码语法 ``` 书写 或者直接空八个，引入代码块 1234### 引用普通引用 引用文本前使用 [大于号+空格]折行可以不加，新起一行都要加上哦12引用里嵌套引用 最外层引用 多一个 &gt; 嵌套一层引用 可以嵌套很多层12引用里嵌套列表 这是引用里嵌套的一个列表 还可以有子列表 子列表需要从 - 之后延后四个空格开始12引用里嵌套代码块 同样的，在前面加四个空格形成代码块 12&gt; 或者使用 ``` 形成代码块&gt; 1234### 图片跟链接的方法区别在于前面加了个感叹号 `!`，这样是不是觉得好记多了呢？ 12当然，你也可以像网址那样对图片网址使用变量 这个链接用 1 作为网址变量 Google.然后在文档的结尾位变量赋值（网址） 1234### 换行如果另起一行，只需在当前行结尾加 2 个空格 在当前行的结尾加 2 个空格这行就会新起一行123456如果是要起一个新段落，只需要空出一行即可。### 分隔符如果你有写分割线的习惯，可以新起一行输入三个减号`-`。当前后都有段落时，请空出一行： 前面的段落 后面的段落12345678## 高级技巧### 行内 HTML 元素目前只支持部分段内 HTML 元素效果，包括 `&lt;kdb&gt; &lt;b&gt; &lt;i&gt; &lt;em&gt; &lt;sup&gt; &lt;sub&gt; &lt;br&gt;` ，如键位显示 使用 Ctrl+Alt+Del 重启电脑12代码块 使用 元素同样可以形成代码块12粗斜体 Markdown 在此处同样适用，如 加粗 1234### 符号转义如果你的描述中需要用到 markdown 的符号，比如 `_` `#` `*` 等，但又不想它被转义，这时候可以在这些符号前加反斜杠，如 `\_` `\#``\*` 进行避免。 _不想这里的文本变斜体_**不想这里的文本被加粗**1234### 扩展支持 **jsfiddle、gist、runjs、优酷视频**，直接填写 url，在其之后会自动添加预览点击会展开相关内容。 http://{url_of_the_fiddle}/embedded/[{tabs}/[{style}]]/https://gist.github.com/{gist_id}http://runjs.cn/detail/{id}http://v.youku.com/v_show/id_{video_id}.html1234### 公式当你需要在编辑器中插入数学公式时，可以使用两个美元符 $$ 包裹 TeX 或 LaTeX 格式的数学公式来实现。提交后，问答和文章页会根据需要加载 Mathjax 对数学公式进行渲染。如： $$ x = {-b \pm \sqrt{b^2-4ac} \over 2a}. $$ $$x \href{why-equal.html}{=} y^2 + 1$$12同时也支持 HTML 属性，如： $$ (x+1)^2 = \class{hidden}{(x+1)(x+1)} $$ $$(x+1)^2 = \cssId{step1}{\style{visibility:hidden}{(x+1)(x+1)}}`]]></content>
      <categories>
        <category>-MD语法</category>
      </categories>
      <tags>
        <tag>-md语法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[maven教程]]></title>
    <url>%2F2014%2F05%2F20%2Fmaven%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[Maven 功能Maven 能够帮助开发者完成以下工作： 构建 文档生成 报告 依赖 SCMs 发布 分发 邮件列表 约定配置Maven 提倡使用一个共同的标准目录结构，Maven 使用约定优于配置的原则，大家尽可能的遵守这样的目录结构。如下所示： 目录 目的 ${basedir} 存放pom.xml和所有的子目录 ${basedir}/src/main/java 项目的java源代码 ${basedir}/src/main/resources 项目的资源，比如说property文件，springmvc.xml ${basedir}/src/test/java 项目的测试类，比如说Junit代码 ${basedir}/src/test/resources 测试用的资源 ${basedir}/src/main/webapp/WEB-INF web应用文件目录，web项目的信息，比如存放web.xml、本地图片、jsp视图页面 ${basedir}/target 打包输出目录 ${basedir}/target/classes 编译输出目录 ${basedir}/target/test-classes 测试编译输出目录 Test.java Maven只会自动运行符合该命名规则的测试类 ~/.m2/repository Maven默认的本地仓库目录位置 Maven 特点 项目设置遵循统一的规则。 任意工程中共享。 依赖管理包括自动更新。 一个庞大且不断增长的库。 可扩展，能够轻松编写 Java 或脚本语言的插件。 只需很少或不需要额外配置即可即时访问新功能。 基于模型的构建 − Maven能够将任意数量的项目构建到预定义的输出类型中，如 JAR，WAR 或基于项目元数据的分发，而不需要在大多数情况下执行任何脚本。 项目信息的一致性站点 − 使用与构建过程相同的元数据，Maven 能够生成一个网站或PDF，包括您要添加的任何文档，并添加到关于项目开发状态的标准报告中。 发布管理和发布单独的输出 − Maven 将不需要额外的配置，就可以与源代码管理系统（如 Subversion 或 Git）集成，并可以基于某个标签管理项目的发布。它也可以将其发布到分发位置供其他项目使用。Maven 能够发布单独的输出，如 JAR，包含其他依赖和文档的归档，或者作为源代码发布。 向后兼容性 − 您可以很轻松的从旧版本 Maven 的多个模块移植到 Maven 3 中。 子项目使用父项目依赖时，正常情况子项目应该继承父项目依赖，无需使用版本号， 并行构建 − 编译的速度能普遍提高20 - 50 %。 更好的错误报告 − Maven 改进了错误报告，它为您提供了 Maven wiki 页面的链接，您可以点击链接查看错误的完整描述。 Maven 环境配置Maven 是一个基于 Java 的工具，所以要做的第一件事情就是安装 JDK。 如果你还未安装 JDK，可以参考我们的 Java 开发环境配置。 系统要求 项目 要求 JDK Maven 3.3 要求 JDK 1.7 或以上 Maven 3.2 要求 JDK 1.6 或以上 Maven 3.0/3.1 要求 JDK 1.5 或以上 内存 没有最低要求 磁盘 Maven 自身安装需要大约 10 MB 空间。除此之外，额外的磁盘空间将用于你的本地 Maven 仓库。你本地仓库的大小取决于使用情况，但预期至少 500 MB 操作系统 没有最低要求 检查 Java 安装 操作系统 任务 命令 Windows 打开命令控制台 c:\&gt; java -version Linux 打开命令终端 # java -version Mac 打开终端 $ java -version Maven 下载Maven 下载地址：http://maven.apache.org/download.cgi 不同平台下载对应的包： 系统 包名 Windows apache-maven-3.3.9-bin.zip Linux apache-maven-3.3.9-bin.tar.gz Mac apache-maven-3.3.9-bin.tar.gz 下载包后解压到对应目录： 系统 存储位置 (可根据自己情况配置) Windows E:\Maven\apache-maven-3.3.9 Linux /usr/local/apache-maven-3.3.9 Mac /usr/local/apache-maven-3.3.9 设置 Maven 环境变量添加环境变量 MAVEN_HOME： 系统 配置 Windows 右键 “计算机”，选择 “属性”，之后点击 “高级系统设置”，点击”环境变量”，来设置环境变量，有以下系统变量需要配置：新建系统变量 MAVEN_HOME，变量值：E:\Maven\apache-maven-3.3.9编辑系统变量 Path，添加变量值：;%MAVEN_HOME%\bin注意：注意多个值之间需要有分号隔开，然后点击确定。 Linux 下载解压：# wget http://mirrors.hust.edu.cn/apache/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz # tar -xvf apache-maven-3.3.9-bin.tar.gz # sudo mv -f apache-maven-3.3.9 /usr/local/编辑 /etc/profile 文件 sudo vim /etc/profile，在文件末尾添加如下代码：export MAVEN_HOME=/usr/local/apache-maven-3.3.9 export PATH=${PATH}:${MAVEN_HOME}/bin保存文件，并运行如下命令使环境变量生效：# source /etc/profile在控制台输入如下命令，如果能看到 Maven 相关版本信息，则说明 Maven 已经安装成功：# mvn -v Mac 下载解压：$ curl -O http://mirrors.hust.edu.cn/apache/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz $ tar -xvf apache-maven-3.3.9-bin.tar.gz $ sudo mv -f apache-maven-3.3.9 /usr/local/编辑 /etc/profile 文件 sudo vim /etc/profile，在文件末尾添加如下代码：export MAVEN_HOME=/usr/local/apache-maven-3.3.9 export PATH=${PATH}:${MAVEN_HOME}/bin保存文件，并运行如下命令使环境变量生效：$ source /etc/profile在控制台输入如下命令，如果能看到 Maven 相关版本信息，则说明 Maven 已经安装成功：$ mvn -v Apache Maven 3.3.9 (bb52d8502b132ec0a5a3f4c09453c07478323dc5; 2015-11-11T00:41:47+08:00) Maven home: /usr/local/apache-maven-3.3.9 Java version: 1.8.0_31, vendor: Oracle Corporation Java home: /Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre Default locale: zh_CN, platform encoding: ISO8859-1 OS name: &quot;mac os x&quot;, version: &quot;10.13.4&quot;, arch: &quot;x86_64&quot;, family: &quot;mac&quot; Maven POMPOM( Project Object Model，项目对象模型 ) 是 Maven 工程的基本工作单元，是一个XML文件，包含了项目的基本信息，用于描述项目如何构建，声明项目依赖，等等。 执行任务或目标时，Maven 会在当前目录中查找 POM。它读取 POM，获取所需的配置信息，然后执行目标。 POM 中可以指定以下配置： 项目依赖 插件 执行目标 项目构建 profile 项目版本 项目开发者列表 相关邮件列表信息 在创建 POM 之前，我们首先需要描述项目组 (groupId), 项目的唯一ID。 12345678910111213141516&lt;project xmlns = &quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi = &quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation = &quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;!-- 模型版本 --&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;!-- 公司或者组织的唯一标志，并且配置时生成的路径也是由此生成， 如com.companyname.project-group，maven会将该项目打成的jar包放本地路径：/com/companyname/project-group --&gt; &lt;groupId&gt;com.companyname.project-group&lt;/groupId&gt; &lt;!-- 项目的唯一ID，一个groupId下面可能多个项目，就是靠artifactId来区分的 --&gt; &lt;artifactId&gt;project&lt;/artifactId&gt; &lt;!-- 版本号 --&gt; &lt;version&gt;1.0&lt;/version&gt;&lt;/project&gt; 所有 POM 文件都需要 project 元素和三个必需字段：groupId，artifactId，version。 节点 描述 project 工程的根标签。 modelVersion 模型版本需要设置为 4.0。 groupId 这是工程组的标识。它在一个组织或者项目中通常是唯一的。例如，一个银行组织 com.companyname.project-group 拥有所有的和银行相关的项目。 artifactId 这是工程的标识。它通常是工程的名称。例如，消费者银行。groupId 和 artifactId 一起定义了 artifact 在仓库中的位置。 version 这是工程的版本号。在 artifact 的仓库中，它用来区分不同的版本。例如：com.company.bank:consumer-banking:1.0 com.company.bank:consumer-banking:1.1 父（Super）POM父（Super）POM是 Maven 默认的 POM。所有的 POM 都继承自一个父 POM（无论是否显式定义了这个父 POM）。父 POM 包含了一些可以被继承的默认设置。因此，当 Maven 发现需要下载 POM 中的 依赖时，它会到 Super POM 中配置的默认仓库 http://repo1.maven.org/maven2 去下载。 Maven 使用 effective pom（Super pom 加上工程自己的配置）来执行相关的目标，它帮助开发者在 pom.xml 中做尽可能少的配置，当然这些配置可以被重写。 使用以下命令来查看 Super POM 默认配置： 1mvn help:effective-pom 接下来我们创建目录 MVN/project，在该目录下创建 pom.xml，内容如下： 1234567891011121314151617&lt;project xmlns = &quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi = &quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation = &quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;!-- 模型版本 --&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;!-- 公司或者组织的唯一标志，并且配置时生成的路径也是由此生成， 如com.companyname.project-group，maven会将该项目打成的jar包放本地路径：/com/companyname/project-group --&gt; &lt;groupId&gt;com.companyname.project-group&lt;/groupId&gt; &lt;!-- 项目的唯一ID，一个groupId下面可能多个项目，就是靠artifactId来区分的 --&gt; &lt;artifactId&gt;project&lt;/artifactId&gt; &lt;!-- 版本号 --&gt; &lt;version&gt;1.0&lt;/version&gt;&lt;/project&gt; 在命令控制台，进入 MVN/project 目录，执行以下命令： 1C:\MVN\project&gt;mvn help:effective-pom Maven 将会开始处理并显示 effective-pom。 12345678910[INFO] Scanning for projects...Downloading: https://repo.maven.apache.org/maven2/org/apache/maven/plugins/maven-clean-plugin/2.5/maven-clean-plugin-2.5.pom...[INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 01:36 min[INFO] Finished at: 2018-09-05T11:31:28+08:00[INFO] Final Memory: 15M/149M[INFO] ------------------------------------------------------------------------ Effective POM 的结果就像在控制台中显示的一样，经过继承、插值之后，使配置生效。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!-- ================================================================= --&gt;&lt;!-- --&gt;&lt;!-- Generated by Maven Help Plugin on 2012-07-05T11:41:51 --&gt;&lt;!-- See: http://maven.apache.org/plugins/maven-help-plugin/ --&gt;&lt;!-- --&gt;&lt;!-- ================================================================= --&gt;&lt;!-- ================================================================= --&gt;&lt;!-- --&gt;&lt;!-- Effective POM for project --&gt;&lt;!-- 'com.companyname.project-group:project-name:jar:1.0' --&gt;&lt;!-- --&gt;&lt;!-- ================================================================= --&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.companyname.project-group&lt;/groupId&gt; &lt;artifactId&gt;project&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;build&gt; &lt;sourceDirectory&gt;C:\MVN\project\src\main\java&lt;/sourceDirectory&gt; &lt;scriptSourceDirectory&gt;src/main/scripts&lt;/scriptSourceDirectory&gt; &lt;testSourceDirectory&gt;C:\MVN\project\src\test\java&lt;/testSourceDirectory&gt; &lt;outputDirectory&gt;C:\MVN\project\target\classes&lt;/outputDirectory&gt; &lt;testOutputDirectory&gt;C:\MVN\project\target\test-classes&lt;/testOutputDirectory&gt; &lt;resources&gt; &lt;resource&gt; &lt;mergeId&gt;resource-0&lt;/mergeId&gt; &lt;directory&gt;C:\MVN\project\src\main\resources&lt;/directory&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;testResources&gt; &lt;testResource&gt; &lt;mergeId&gt;resource-1&lt;/mergeId&gt; &lt;directory&gt;C:\MVN\project\src\test\resources&lt;/directory&gt; &lt;/testResource&gt; &lt;/testResources&gt; &lt;directory&gt;C:\MVN\project\target&lt;/directory&gt; &lt;finalName&gt;project-1.0&lt;/finalName&gt; &lt;pluginManagement&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-antrun-plugin&lt;/artifactId&gt; &lt;version&gt;1.3&lt;/version&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;version&gt;2.2-beta-2&lt;/version&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-clean-plugin&lt;/artifactId&gt; &lt;version&gt;2.2&lt;/version&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;2.0.2&lt;/version&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt; &lt;version&gt;2.0&lt;/version&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-deploy-plugin&lt;/artifactId&gt; &lt;version&gt;2.4&lt;/version&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-ear-plugin&lt;/artifactId&gt; &lt;version&gt;2.3.1&lt;/version&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-ejb-plugin&lt;/artifactId&gt; &lt;version&gt;2.1&lt;/version&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-install-plugin&lt;/artifactId&gt; &lt;version&gt;2.2&lt;/version&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt; &lt;version&gt;2.2&lt;/version&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-javadoc-plugin&lt;/artifactId&gt; &lt;version&gt;2.5&lt;/version&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-plugin-plugin&lt;/artifactId&gt; &lt;version&gt;2.4.3&lt;/version&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-rar-plugin&lt;/artifactId&gt; &lt;version&gt;2.2&lt;/version&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-release-plugin&lt;/artifactId&gt; &lt;version&gt;2.0-beta-8&lt;/version&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-resources-plugin&lt;/artifactId&gt; &lt;version&gt;2.3&lt;/version&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-site-plugin&lt;/artifactId&gt; &lt;version&gt;2.0-beta-7&lt;/version&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-source-plugin&lt;/artifactId&gt; &lt;version&gt;2.0.4&lt;/version&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt; &lt;version&gt;2.4.3&lt;/version&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-war-plugin&lt;/artifactId&gt; &lt;version&gt;2.1-alpha-2&lt;/version&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/pluginManagement&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-help-plugin&lt;/artifactId&gt; &lt;version&gt;2.1.1&lt;/version&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;repositories&gt; &lt;repository&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;id&gt;central&lt;/id&gt; &lt;name&gt;Maven Repository Switchboard&lt;/name&gt; &lt;url&gt;http://repo1.maven.org/maven2&lt;/url&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;pluginRepositories&gt; &lt;pluginRepository&gt; &lt;releases&gt; &lt;updatePolicy&gt;never&lt;/updatePolicy&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;id&gt;central&lt;/id&gt; &lt;name&gt;Maven Plugin Repository&lt;/name&gt; &lt;url&gt;http://repo1.maven.org/maven2&lt;/url&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt; &lt;reporting&gt; &lt;outputDirectory&gt;C:\MVN\project\target/site&lt;/outputDirectory&gt; &lt;/reporting&gt;&lt;/project&gt;x 在上面的 pom.xml 中，你可以看到 Maven 在执行目标时需要用到的默认工程源码目录结构、输出目录、需要的插件、仓库和报表目录。 Maven 的 pom.xml 文件也不需要手工编写。 Maven 提供了大量的原型插件来创建工程，包括工程结构和 pom.xml。 POM 标签大全详解123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0http://maven.apache.org/maven-v4_0_0.xsd"&gt; &lt;!--父项目的坐标。如果项目中没有规定某个元素的值，那么父项目中的对应值即为项目的默认值。 坐标包括group ID，artifact ID和 version。 --&gt; &lt;parent&gt; &lt;!--被继承的父项目的构件标识符 --&gt; &lt;artifactId /&gt; &lt;!--被继承的父项目的全球唯一标识符 --&gt; &lt;groupId /&gt; &lt;!--被继承的父项目的版本 --&gt; &lt;version /&gt; &lt;!-- 父项目的pom.xml文件的相对路径。相对路径允许你选择一个不同的路径。默认值是../pom.xml。Maven首先在构建当前项目的地方寻找父项 目的pom，其次在文件系统的这个位置（relativePath位置），然后在本地仓库，最后在远程仓库寻找父项目的pom。 --&gt; &lt;relativePath /&gt; &lt;/parent&gt; &lt;!--声明项目描述符遵循哪一个POM模型版本。模型本身的版本很少改变，虽然如此，但它仍然是必不可少的，这是为了当Maven引入了新的特性或者其他模型变更的时候，确保稳定性。 --&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;!--项目的全球唯一标识符，通常使用全限定的包名区分该项目和其他项目。并且构建时生成的路径也是由此生成， 如com.mycompany.app生成的相对路径为：/com/mycompany/app --&gt; &lt;groupId&gt;asia.banseon&lt;/groupId&gt; &lt;!-- 构件的标识符，它和group ID一起唯一标识一个构件。换句话说，你不能有两个不同的项目拥有同样的artifact ID和groupID；在某个 特定的group ID下，artifact ID也必须是唯一的。构件是项目产生的或使用的一个东西，Maven为项目产生的构件包括：JARs，源 码，二进制发布和WARs等。 --&gt; &lt;artifactId&gt;banseon-maven2&lt;/artifactId&gt; &lt;!--项目产生的构件类型，例如jar、war、ear、pom。插件可以创建他们自己的构件类型，所以前面列的不是全部构件类型 --&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;!--项目当前版本，格式为:主版本.次版本.增量版本-限定版本号 --&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;!--项目的名称, Maven产生的文档用 --&gt; &lt;name&gt;banseon-maven&lt;/name&gt; &lt;!--项目主页的URL, Maven产生的文档用 --&gt; &lt;url&gt;http://www.baidu.com/banseon&lt;/url&gt; &lt;!-- 项目的详细描述, Maven 产生的文档用。 当这个元素能够用HTML格式描述时（例如，CDATA中的文本会被解析器忽略，就可以包含HTML标 签）， 不鼓励使用纯文本描述。如果你需要修改产生的web站点的索引页面，你应该修改你自己的索引页文件，而不是调整这里的文档。 --&gt; &lt;description&gt;A maven project to study maven.&lt;/description&gt; &lt;!--描述了这个项目构建环境中的前提条件。 --&gt; &lt;prerequisites&gt; &lt;!--构建该项目或使用该插件所需要的Maven的最低版本 --&gt; &lt;maven /&gt; &lt;/prerequisites&gt; &lt;!--项目的问题管理系统(Bugzilla, Jira, Scarab,或任何你喜欢的问题管理系统)的名称和URL，本例为 jira --&gt; &lt;issueManagement&gt; &lt;!--问题管理系统（例如jira）的名字， --&gt; &lt;system&gt;jira&lt;/system&gt; &lt;!--该项目使用的问题管理系统的URL --&gt; &lt;url&gt;http://jira.baidu.com/banseon&lt;/url&gt; &lt;/issueManagement&gt; &lt;!--项目持续集成信息 --&gt; &lt;ciManagement&gt; &lt;!--持续集成系统的名字，例如continuum --&gt; &lt;system /&gt; &lt;!--该项目使用的持续集成系统的URL（如果持续集成系统有web接口的话）。 --&gt; &lt;url /&gt; &lt;!--构建完成时，需要通知的开发者/用户的配置项。包括被通知者信息和通知条件（错误，失败，成功，警告） --&gt; &lt;notifiers&gt; &lt;!--配置一种方式，当构建中断时，以该方式通知用户/开发者 --&gt; &lt;notifier&gt; &lt;!--传送通知的途径 --&gt; &lt;type /&gt; &lt;!--发生错误时是否通知 --&gt; &lt;sendOnError /&gt; &lt;!--构建失败时是否通知 --&gt; &lt;sendOnFailure /&gt; &lt;!--构建成功时是否通知 --&gt; &lt;sendOnSuccess /&gt; &lt;!--发生警告时是否通知 --&gt; &lt;sendOnWarning /&gt; &lt;!--不赞成使用。通知发送到哪里 --&gt; &lt;address /&gt; &lt;!--扩展配置项 --&gt; &lt;configuration /&gt; &lt;/notifier&gt; &lt;/notifiers&gt; &lt;/ciManagement&gt; &lt;!--项目创建年份，4位数字。当产生版权信息时需要使用这个值。 --&gt; &lt;inceptionYear /&gt; &lt;!--项目相关邮件列表信息 --&gt; &lt;mailingLists&gt; &lt;!--该元素描述了项目相关的所有邮件列表。自动产生的网站引用这些信息。 --&gt; &lt;mailingList&gt; &lt;!--邮件的名称 --&gt; &lt;name&gt;Demo&lt;/name&gt; &lt;!--发送邮件的地址或链接，如果是邮件地址，创建文档时，mailto: 链接会被自动创建 --&gt; &lt;post&gt;banseon@126.com&lt;/post&gt; &lt;!--订阅邮件的地址或链接，如果是邮件地址，创建文档时，mailto: 链接会被自动创建 --&gt; &lt;subscribe&gt;banseon@126.com&lt;/subscribe&gt; &lt;!--取消订阅邮件的地址或链接，如果是邮件地址，创建文档时，mailto: 链接会被自动创建 --&gt; &lt;unsubscribe&gt;banseon@126.com&lt;/unsubscribe&gt; &lt;!--你可以浏览邮件信息的URL --&gt; &lt;archive&gt;http:/hi.baidu.com/banseon/demo/dev/&lt;/archive&gt; &lt;/mailingList&gt; &lt;/mailingLists&gt; &lt;!--项目开发者列表 --&gt; &lt;developers&gt; &lt;!--某个项目开发者的信息 --&gt; &lt;developer&gt; &lt;!--SCM里项目开发者的唯一标识符 --&gt; &lt;id&gt;HELLO WORLD&lt;/id&gt; &lt;!--项目开发者的全名 --&gt; &lt;name&gt;banseon&lt;/name&gt; &lt;!--项目开发者的email --&gt; &lt;email&gt;banseon@126.com&lt;/email&gt; &lt;!--项目开发者的主页的URL --&gt; &lt;url /&gt; &lt;!--项目开发者在项目中扮演的角色，角色元素描述了各种角色 --&gt; &lt;roles&gt; &lt;role&gt;Project Manager&lt;/role&gt; &lt;role&gt;Architect&lt;/role&gt; &lt;/roles&gt; &lt;!--项目开发者所属组织 --&gt; &lt;organization&gt;demo&lt;/organization&gt; &lt;!--项目开发者所属组织的URL --&gt; &lt;organizationUrl&gt;http://hi.baidu.com/banseon&lt;/organizationUrl&gt; &lt;!--项目开发者属性，如即时消息如何处理等 --&gt; &lt;properties&gt; &lt;dept&gt;No&lt;/dept&gt; &lt;/properties&gt; &lt;!--项目开发者所在时区， -11到12范围内的整数。 --&gt; &lt;timezone&gt;-5&lt;/timezone&gt; &lt;/developer&gt; &lt;/developers&gt; &lt;!--项目的其他贡献者列表 --&gt; &lt;contributors&gt; &lt;!--项目的其他贡献者。参见developers/developer元素 --&gt; &lt;contributor&gt; &lt;name /&gt; &lt;email /&gt; &lt;url /&gt; &lt;organization /&gt; &lt;organizationUrl /&gt; &lt;roles /&gt; &lt;timezone /&gt; &lt;properties /&gt; &lt;/contributor&gt; &lt;/contributors&gt; &lt;!--该元素描述了项目所有License列表。 应该只列出该项目的license列表，不要列出依赖项目的 license列表。如果列出多个license，用户可以选择它们中的一个而不是接受所有license。 --&gt; &lt;licenses&gt; &lt;!--描述了项目的license，用于生成项目的web站点的license页面，其他一些报表和validation也会用到该元素。 --&gt; &lt;license&gt; &lt;!--license用于法律上的名称 --&gt; &lt;name&gt;Apache 2&lt;/name&gt; &lt;!--官方的license正文页面的URL --&gt; &lt;url&gt;http://www.baidu.com/banseon/LICENSE-2.0.txt&lt;/url&gt; &lt;!--项目分发的主要方式： repo，可以从Maven库下载 manual， 用户必须手动下载和安装依赖 --&gt; &lt;distribution&gt;repo&lt;/distribution&gt; &lt;!--关于license的补充信息 --&gt; &lt;comments&gt;A business-friendly OSS license&lt;/comments&gt; &lt;/license&gt; &lt;/licenses&gt; &lt;!--SCM(Source Control Management)标签允许你配置你的代码库，供Maven web站点和其它插件使用。 --&gt; &lt;scm&gt; &lt;!--SCM的URL,该URL描述了版本库和如何连接到版本库。欲知详情，请看SCMs提供的URL格式和列表。该连接只读。 --&gt; &lt;connection&gt; scm:svn:http://svn.baidu.com/banseon/maven/banseon/banseon-maven2-trunk(dao-trunk) &lt;/connection&gt; &lt;!--给开发者使用的，类似connection元素。即该连接不仅仅只读 --&gt; &lt;developerConnection&gt; scm:svn:http://svn.baidu.com/banseon/maven/banseon/dao-trunk &lt;/developerConnection&gt; &lt;!--当前代码的标签，在开发阶段默认为HEAD --&gt; &lt;tag /&gt; &lt;!--指向项目的可浏览SCM库（例如ViewVC或者Fisheye）的URL。 --&gt; &lt;url&gt;http://svn.baidu.com/banseon&lt;/url&gt; &lt;/scm&gt; &lt;!--描述项目所属组织的各种属性。Maven产生的文档用 --&gt; &lt;organization&gt; &lt;!--组织的全名 --&gt; &lt;name&gt;demo&lt;/name&gt; &lt;!--组织主页的URL --&gt; &lt;url&gt;http://www.baidu.com/banseon&lt;/url&gt; &lt;/organization&gt; &lt;!--构建项目需要的信息 --&gt; &lt;build&gt; &lt;!--该元素设置了项目源码目录，当构建项目的时候，构建系统会编译目录里的源码。该路径是相对于pom.xml的相对路径。 --&gt; &lt;sourceDirectory /&gt; &lt;!--该元素设置了项目脚本源码目录，该目录和源码目录不同：绝大多数情况下，该目录下的内容 会被拷贝到输出目录(因为脚本是被解释的，而不是被编译的)。 --&gt; &lt;scriptSourceDirectory /&gt; &lt;!--该元素设置了项目单元测试使用的源码目录，当测试项目的时候，构建系统会编译目录里的源码。该路径是相对于pom.xml的相对路径。 --&gt; &lt;testSourceDirectory /&gt; &lt;!--被编译过的应用程序class文件存放的目录。 --&gt; &lt;outputDirectory /&gt; &lt;!--被编译过的测试class文件存放的目录。 --&gt; &lt;testOutputDirectory /&gt; &lt;!--使用来自该项目的一系列构建扩展 --&gt; &lt;extensions&gt; &lt;!--描述使用到的构建扩展。 --&gt; &lt;extension&gt; &lt;!--构建扩展的groupId --&gt; &lt;groupId /&gt; &lt;!--构建扩展的artifactId --&gt; &lt;artifactId /&gt; &lt;!--构建扩展的版本 --&gt; &lt;version /&gt; &lt;/extension&gt; &lt;/extensions&gt; &lt;!--当项目没有规定目标（Maven2 叫做阶段）时的默认值 --&gt; &lt;defaultGoal /&gt; &lt;!--这个元素描述了项目相关的所有资源路径列表，例如和项目相关的属性文件，这些资源被包含在最终的打包文件里。 --&gt; &lt;resources&gt; &lt;!--这个元素描述了项目相关或测试相关的所有资源路径 --&gt; &lt;resource&gt; &lt;!-- 描述了资源的目标路径。该路径相对target/classes目录（例如$&#123;project.build.outputDirectory&#125;）。举个例 子，如果你想资源在特定的包里(org.apache.maven.messages)，你就必须该元素设置为org/apache/maven /messages。然而，如果你只是想把资源放到源码目录结构里，就不需要该配置。 --&gt; &lt;targetPath /&gt; &lt;!--是否使用参数值代替参数名。参数值取自properties元素或者文件里配置的属性，文件在filters元素里列出。 --&gt; &lt;filtering /&gt; &lt;!--描述存放资源的目录，该路径相对POM路径 --&gt; &lt;directory /&gt; &lt;!--包含的模式列表，例如**/*.xml. --&gt; &lt;includes /&gt; &lt;!--排除的模式列表，例如**/*.xml --&gt; &lt;excludes /&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;!--这个元素描述了单元测试相关的所有资源路径，例如和单元测试相关的属性文件。 --&gt; &lt;testResources&gt; &lt;!--这个元素描述了测试相关的所有资源路径，参见build/resources/resource元素的说明 --&gt; &lt;testResource&gt; &lt;targetPath /&gt; &lt;filtering /&gt; &lt;directory /&gt; &lt;includes /&gt; &lt;excludes /&gt; &lt;/testResource&gt; &lt;/testResources&gt; &lt;!--构建产生的所有文件存放的目录 --&gt; &lt;directory /&gt; &lt;!--产生的构件的文件名，默认值是$&#123;artifactId&#125;-$&#123;version&#125;。 --&gt; &lt;finalName /&gt; &lt;!--当filtering开关打开时，使用到的过滤器属性文件列表 --&gt; &lt;filters /&gt; &lt;!--子项目可以引用的默认插件信息。该插件配置项直到被引用时才会被解析或绑定到生命周期。给定插件的任何本地配置都会覆盖这里的配置 --&gt; &lt;pluginManagement&gt; &lt;!--使用的插件列表 。 --&gt; &lt;plugins&gt; &lt;!--plugin元素包含描述插件所需要的信息。 --&gt; &lt;plugin&gt; &lt;!--插件在仓库里的group ID --&gt; &lt;groupId /&gt; &lt;!--插件在仓库里的artifact ID --&gt; &lt;artifactId /&gt; &lt;!--被使用的插件的版本（或版本范围） --&gt; &lt;version /&gt; &lt;!--是否从该插件下载Maven扩展（例如打包和类型处理器），由于性能原因，只有在真需要下载时，该元素才被设置成enabled。 --&gt; &lt;extensions /&gt; &lt;!--在构建生命周期中执行一组目标的配置。每个目标可能有不同的配置。 --&gt; &lt;executions&gt; &lt;!--execution元素包含了插件执行需要的信息 --&gt; &lt;execution&gt; &lt;!--执行目标的标识符，用于标识构建过程中的目标，或者匹配继承过程中需要合并的执行目标 --&gt; &lt;id /&gt; &lt;!--绑定了目标的构建生命周期阶段，如果省略，目标会被绑定到源数据里配置的默认阶段 --&gt; &lt;phase /&gt; &lt;!--配置的执行目标 --&gt; &lt;goals /&gt; &lt;!--配置是否被传播到子POM --&gt; &lt;inherited /&gt; &lt;!--作为DOM对象的配置 --&gt; &lt;configuration /&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;!--项目引入插件所需要的额外依赖 --&gt; &lt;dependencies&gt; &lt;!--参见dependencies/dependency元素 --&gt; &lt;dependency&gt; ...... &lt;/dependency&gt; &lt;/dependencies&gt; &lt;!--任何配置是否被传播到子项目 --&gt; &lt;inherited /&gt; &lt;!--作为DOM对象的配置 --&gt; &lt;configuration /&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/pluginManagement&gt; &lt;!--使用的插件列表 --&gt; &lt;plugins&gt; &lt;!--参见build/pluginManagement/plugins/plugin元素 --&gt; &lt;plugin&gt; &lt;groupId /&gt; &lt;artifactId /&gt; &lt;version /&gt; &lt;extensions /&gt; &lt;executions&gt; &lt;execution&gt; &lt;id /&gt; &lt;phase /&gt; &lt;goals /&gt; &lt;inherited /&gt; &lt;configuration /&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;dependencies&gt; &lt;!--参见dependencies/dependency元素 --&gt; &lt;dependency&gt; ...... &lt;/dependency&gt; &lt;/dependencies&gt; &lt;goals /&gt; &lt;inherited /&gt; &lt;configuration /&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;!--在列的项目构建profile，如果被激活，会修改构建处理 --&gt; &lt;profiles&gt; &lt;!--根据环境参数或命令行参数激活某个构建处理 --&gt; &lt;profile&gt; &lt;!--构建配置的唯一标识符。即用于命令行激活，也用于在继承时合并具有相同标识符的profile。 --&gt; &lt;id /&gt; &lt;!--自动触发profile的条件逻辑。Activation是profile的开启钥匙。profile的力量来自于它 能够在某些特定的环境中自动使用某些特定的值；这些环境通过activation元素指定。activation元素并不是激活profile的唯一方式。 --&gt; &lt;activation&gt; &lt;!--profile默认是否激活的标志 --&gt; &lt;activeByDefault /&gt; &lt;!--当匹配的jdk被检测到，profile被激活。例如，1.4激活JDK1.4，1.4.0_2，而!1.4激活所有版本不是以1.4开头的JDK。 --&gt; &lt;jdk /&gt; &lt;!--当匹配的操作系统属性被检测到，profile被激活。os元素可以定义一些操作系统相关的属性。 --&gt; &lt;os&gt; &lt;!--激活profile的操作系统的名字 --&gt; &lt;name&gt;Windows XP&lt;/name&gt; &lt;!--激活profile的操作系统所属家族(如 'windows') --&gt; &lt;family&gt;Windows&lt;/family&gt; &lt;!--激活profile的操作系统体系结构 --&gt; &lt;arch&gt;x86&lt;/arch&gt; &lt;!--激活profile的操作系统版本 --&gt; &lt;version&gt;5.1.2600&lt;/version&gt; &lt;/os&gt; &lt;!--如果Maven检测到某一个属性（其值可以在POM中通过$&#123;名称&#125;引用），其拥有对应的名称和值，Profile就会被激活。如果值 字段是空的，那么存在属性名称字段就会激活profile，否则按区分大小写方式匹配属性值字段 --&gt; &lt;property&gt; &lt;!--激活profile的属性的名称 --&gt; &lt;name&gt;mavenVersion&lt;/name&gt; &lt;!--激活profile的属性的值 --&gt; &lt;value&gt;2.0.3&lt;/value&gt; &lt;/property&gt; &lt;!--提供一个文件名，通过检测该文件的存在或不存在来激活profile。missing检查文件是否存在，如果不存在则激活 profile。另一方面，exists则会检查文件是否存在，如果存在则激活profile。 --&gt; &lt;file&gt; &lt;!--如果指定的文件存在，则激活profile。 --&gt; &lt;exists&gt;/usr/local/hudson/hudson-home/jobs/maven-guide-zh-to-production/workspace/ &lt;/exists&gt; &lt;!--如果指定的文件不存在，则激活profile。 --&gt; &lt;missing&gt;/usr/local/hudson/hudson-home/jobs/maven-guide-zh-to-production/workspace/ &lt;/missing&gt; &lt;/file&gt; &lt;/activation&gt; &lt;!--构建项目所需要的信息。参见build元素 --&gt; &lt;build&gt; &lt;defaultGoal /&gt; &lt;resources&gt; &lt;resource&gt; &lt;targetPath /&gt; &lt;filtering /&gt; &lt;directory /&gt; &lt;includes /&gt; &lt;excludes /&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;testResources&gt; &lt;testResource&gt; &lt;targetPath /&gt; &lt;filtering /&gt; &lt;directory /&gt; &lt;includes /&gt; &lt;excludes /&gt; &lt;/testResource&gt; &lt;/testResources&gt; &lt;directory /&gt; &lt;finalName /&gt; &lt;filters /&gt; &lt;pluginManagement&gt; &lt;plugins&gt; &lt;!--参见build/pluginManagement/plugins/plugin元素 --&gt; &lt;plugin&gt; &lt;groupId /&gt; &lt;artifactId /&gt; &lt;version /&gt; &lt;extensions /&gt; &lt;executions&gt; &lt;execution&gt; &lt;id /&gt; &lt;phase /&gt; &lt;goals /&gt; &lt;inherited /&gt; &lt;configuration /&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;dependencies&gt; &lt;!--参见dependencies/dependency元素 --&gt; &lt;dependency&gt; ...... &lt;/dependency&gt; &lt;/dependencies&gt; &lt;goals /&gt; &lt;inherited /&gt; &lt;configuration /&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/pluginManagement&gt; &lt;plugins&gt; &lt;!--参见build/pluginManagement/plugins/plugin元素 --&gt; &lt;plugin&gt; &lt;groupId /&gt; &lt;artifactId /&gt; &lt;version /&gt; &lt;extensions /&gt; &lt;executions&gt; &lt;execution&gt; &lt;id /&gt; &lt;phase /&gt; &lt;goals /&gt; &lt;inherited /&gt; &lt;configuration /&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;dependencies&gt; &lt;!--参见dependencies/dependency元素 --&gt; &lt;dependency&gt; ...... &lt;/dependency&gt; &lt;/dependencies&gt; &lt;goals /&gt; &lt;inherited /&gt; &lt;configuration /&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;!--模块（有时称作子项目） 被构建成项目的一部分。列出的每个模块元素是指向该模块的目录的相对路径 --&gt; &lt;modules /&gt; &lt;!--发现依赖和扩展的远程仓库列表。 --&gt; &lt;repositories&gt; &lt;!--参见repositories/repository元素 --&gt; &lt;repository&gt; &lt;releases&gt; &lt;enabled /&gt; &lt;updatePolicy /&gt; &lt;checksumPolicy /&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled /&gt; &lt;updatePolicy /&gt; &lt;checksumPolicy /&gt; &lt;/snapshots&gt; &lt;id /&gt; &lt;name /&gt; &lt;url /&gt; &lt;layout /&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;!--发现插件的远程仓库列表，这些插件用于构建和报表 --&gt; &lt;pluginRepositories&gt; &lt;!--包含需要连接到远程插件仓库的信息.参见repositories/repository元素 --&gt; &lt;pluginRepository&gt; &lt;releases&gt; &lt;enabled /&gt; &lt;updatePolicy /&gt; &lt;checksumPolicy /&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled /&gt; &lt;updatePolicy /&gt; &lt;checksumPolicy /&gt; &lt;/snapshots&gt; &lt;id /&gt; &lt;name /&gt; &lt;url /&gt; &lt;layout /&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt; &lt;!--该元素描述了项目相关的所有依赖。 这些依赖组成了项目构建过程中的一个个环节。它们自动从项目定义的仓库中下载。要获取更多信息，请看项目依赖机制。 --&gt; &lt;dependencies&gt; &lt;!--参见dependencies/dependency元素 --&gt; &lt;dependency&gt; ...... &lt;/dependency&gt; &lt;/dependencies&gt; &lt;!--不赞成使用. 现在Maven忽略该元素. --&gt; &lt;reports /&gt; &lt;!--该元素包括使用报表插件产生报表的规范。当用户执行"mvn site"，这些报表就会运行。 在页面导航栏能看到所有报表的链接。参见reporting元素 --&gt; &lt;reporting&gt; ...... &lt;/reporting&gt; &lt;!--参见dependencyManagement元素 --&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;!--参见dependencies/dependency元素 --&gt; &lt;dependency&gt; ...... &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;!--参见distributionManagement元素 --&gt; &lt;distributionManagement&gt; ...... &lt;/distributionManagement&gt; &lt;!--参见properties元素 --&gt; &lt;properties /&gt; &lt;/profile&gt; &lt;/profiles&gt; &lt;!--模块（有时称作子项目） 被构建成项目的一部分。列出的每个模块元素是指向该模块的目录的相对路径 --&gt; &lt;modules /&gt; &lt;!--发现依赖和扩展的远程仓库列表。 --&gt; &lt;repositories&gt; &lt;!--包含需要连接到远程仓库的信息 --&gt; &lt;repository&gt; &lt;!--如何处理远程仓库里发布版本的下载 --&gt; &lt;releases&gt; &lt;!--true或者false表示该仓库是否为下载某种类型构件（发布版，快照版）开启。 --&gt; &lt;enabled /&gt; &lt;!--该元素指定更新发生的频率。Maven会比较本地POM和远程POM的时间戳。这里的选项是：always（一直），daily（默认，每日），interval：X（这里X是以分钟为单位的时间间隔），或者never（从不）。 --&gt; &lt;updatePolicy /&gt; &lt;!--当Maven验证构件校验文件失败时该怎么做：ignore（忽略），fail（失败），或者warn（警告）。 --&gt; &lt;checksumPolicy /&gt; &lt;/releases&gt; &lt;!-- 如何处理远程仓库里快照版本的下载。有了releases和snapshots这两组配置，POM就可以在每个单独的仓库中，为每种类型的构件采取不同的 策略。例如，可能有人会决定只为开发目的开启对快照版本下载的支持。参见repositories/repository/releases元素 --&gt; &lt;snapshots&gt; &lt;enabled /&gt; &lt;updatePolicy /&gt; &lt;checksumPolicy /&gt; &lt;/snapshots&gt; &lt;!--远程仓库唯一标识符。可以用来匹配在settings.xml文件里配置的远程仓库 --&gt; &lt;id&gt;banseon-repository-proxy&lt;/id&gt; &lt;!--远程仓库名称 --&gt; &lt;name&gt;banseon-repository-proxy&lt;/name&gt; &lt;!--远程仓库URL，按protocol://hostname/path形式 --&gt; &lt;url&gt;http://192.168.1.169:9999/repository/&lt;/url&gt; &lt;!-- 用于定位和排序构件的仓库布局类型-可以是default（默认）或者legacy（遗留）。Maven 2为其仓库提供了一个默认的布局；然 而，Maven 1.x有一种不同的布局。我们可以使用该元素指定布局是default（默认）还是legacy（遗留）。 --&gt; &lt;layout&gt;default&lt;/layout&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;!--发现插件的远程仓库列表，这些插件用于构建和报表 --&gt; &lt;pluginRepositories&gt; &lt;!--包含需要连接到远程插件仓库的信息.参见repositories/repository元素 --&gt; &lt;pluginRepository&gt; ...... &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt; &lt;!--该元素描述了项目相关的所有依赖。 这些依赖组成了项目构建过程中的一个个环节。它们自动从项目定义的仓库中下载。要获取更多信息，请看项目依赖机制。 --&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;!--依赖的group ID --&gt; &lt;groupId&gt;org.apache.maven&lt;/groupId&gt; &lt;!--依赖的artifact ID --&gt; &lt;artifactId&gt;maven-artifact&lt;/artifactId&gt; &lt;!--依赖的版本号。 在Maven 2里, 也可以配置成版本号的范围。 --&gt; &lt;version&gt;3.8.1&lt;/version&gt; &lt;!-- 依赖类型，默认类型是jar。它通常表示依赖的文件的扩展名，但也有例外。一个类型可以被映射成另外一个扩展名或分类器。类型经常和使用的打包方式对应， 尽管这也有例外。一些类型的例子：jar，war，ejb-client和test-jar。如果设置extensions为 true，就可以在 plugin里定义新的类型。所以前面的类型的例子不完整。 --&gt; &lt;type&gt;jar&lt;/type&gt; &lt;!-- 依赖的分类器。分类器可以区分属于同一个POM，但不同构建方式的构件。分类器名被附加到文件名的版本号后面。例如，如果你想要构建两个单独的构件成 JAR，一个使用Java 1.4编译器，另一个使用Java 6编译器，你就可以使用分类器来生成两个单独的JAR构件。 --&gt; &lt;classifier&gt;&lt;/classifier&gt; &lt;!--依赖范围。在项目发布过程中，帮助决定哪些构件被包括进来。欲知详情请参考依赖机制。 - compile ：默认范围，用于编译 - provided：类似于编译，但支持你期待jdk或者容器提供，类似于classpath - runtime: 在执行时需要使用 - test: 用于test任务时使用 - system: 需要外在提供相应的元素。通过systemPath来取得 - systemPath: 仅用于范围为system。提供相应的路径 - optional: 当项目自身被依赖时，标注依赖是否传递。用于连续依赖时使用 --&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;!--仅供system范围使用。注意，不鼓励使用这个元素，并且在新的版本中该元素可能被覆盖掉。该元素为依赖规定了文件系统上的路径。需要绝对路径而不是相对路径。推荐使用属性匹配绝对路径，例如$&#123;java.home&#125;。 --&gt; &lt;systemPath&gt;&lt;/systemPath&gt; &lt;!--当计算传递依赖时， 从依赖构件列表里，列出被排除的依赖构件集。即告诉maven你只依赖指定的项目，不依赖项目的依赖。此元素主要用于解决版本冲突问题 --&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;!--可选依赖，如果你在项目B中把C依赖声明为可选，你就需要在依赖于B的项目（例如项目A）中显式的引用对C的依赖。可选依赖阻断依赖的传递性。 --&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;!--不赞成使用. 现在Maven忽略该元素. --&gt; &lt;reports&gt;&lt;/reports&gt; &lt;!--该元素描述使用报表插件产生报表的规范。当用户执行"mvn site"，这些报表就会运行。 在页面导航栏能看到所有报表的链接。 --&gt; &lt;reporting&gt; &lt;!--true，则，网站不包括默认的报表。这包括"项目信息"菜单中的报表。 --&gt; &lt;excludeDefaults /&gt; &lt;!--所有产生的报表存放到哪里。默认值是$&#123;project.build.directory&#125;/site。 --&gt; &lt;outputDirectory /&gt; &lt;!--使用的报表插件和他们的配置。 --&gt; &lt;plugins&gt; &lt;!--plugin元素包含描述报表插件需要的信息 --&gt; &lt;plugin&gt; &lt;!--报表插件在仓库里的group ID --&gt; &lt;groupId /&gt; &lt;!--报表插件在仓库里的artifact ID --&gt; &lt;artifactId /&gt; &lt;!--被使用的报表插件的版本（或版本范围） --&gt; &lt;version /&gt; &lt;!--任何配置是否被传播到子项目 --&gt; &lt;inherited /&gt; &lt;!--报表插件的配置 --&gt; &lt;configuration /&gt; &lt;!--一组报表的多重规范，每个规范可能有不同的配置。一个规范（报表集）对应一个执行目标 。例如，有1，2，3，4，5，6，7，8，9个报表。1，2，5构成A报表集，对应一个执行目标。2，5，8构成B报表集，对应另一个执行目标 --&gt; &lt;reportSets&gt; &lt;!--表示报表的一个集合，以及产生该集合的配置 --&gt; &lt;reportSet&gt; &lt;!--报表集合的唯一标识符，POM继承时用到 --&gt; &lt;id /&gt; &lt;!--产生报表集合时，被使用的报表的配置 --&gt; &lt;configuration /&gt; &lt;!--配置是否被继承到子POMs --&gt; &lt;inherited /&gt; &lt;!--这个集合里使用到哪些报表 --&gt; &lt;reports /&gt; &lt;/reportSet&gt; &lt;/reportSets&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/reporting&gt; &lt;!-- 继承自该项目的所有子项目的默认依赖信息。这部分的依赖信息不会被立即解析,而是当子项目声明一个依赖（必须描述group ID和 artifact ID信息），如果group ID和artifact ID以外的一些信息没有描述，则通过group ID和artifact ID 匹配到这里的依赖，并使用这里的依赖信息。 --&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;!--参见dependencies/dependency元素 --&gt; &lt;dependency&gt; ...... &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;!--项目分发信息，在执行mvn deploy后表示要发布的位置。有了这些信息就可以把网站部署到远程服务器或者把构件部署到远程仓库。 --&gt; &lt;distributionManagement&gt; &lt;!--部署项目产生的构件到远程仓库需要的信息 --&gt; &lt;repository&gt; &lt;!--是分配给快照一个唯一的版本号（由时间戳和构建流水号）？还是每次都使用相同的版本号？参见repositories/repository元素 --&gt; &lt;uniqueVersion /&gt; &lt;id&gt;banseon-maven2&lt;/id&gt; &lt;name&gt;banseon maven2&lt;/name&gt; &lt;url&gt;file://$&#123;basedir&#125;/target/deploy&lt;/url&gt; &lt;layout /&gt; &lt;/repository&gt; &lt;!--构件的快照部署到哪里？如果没有配置该元素，默认部署到repository元素配置的仓库，参见distributionManagement/repository元素 --&gt; &lt;snapshotRepository&gt; &lt;uniqueVersion /&gt; &lt;id&gt;banseon-maven2&lt;/id&gt; &lt;name&gt;Banseon-maven2 Snapshot Repository&lt;/name&gt; &lt;url&gt;scp://svn.baidu.com/banseon:/usr/local/maven-snapshot&lt;/url&gt; &lt;layout /&gt; &lt;/snapshotRepository&gt; &lt;!--部署项目的网站需要的信息 --&gt; &lt;site&gt; &lt;!--部署位置的唯一标识符，用来匹配站点和settings.xml文件里的配置 --&gt; &lt;id&gt;banseon-site&lt;/id&gt; &lt;!--部署位置的名称 --&gt; &lt;name&gt;business api website&lt;/name&gt; &lt;!--部署位置的URL，按protocol://hostname/path形式 --&gt; &lt;url&gt; scp://svn.baidu.com/banseon:/var/www/localhost/banseon-web &lt;/url&gt; &lt;/site&gt; &lt;!--项目下载页面的URL。如果没有该元素，用户应该参考主页。使用该元素的原因是：帮助定位那些不在仓库里的构件（由于license限制）。 --&gt; &lt;downloadUrl /&gt; &lt;!--如果构件有了新的group ID和artifact ID（构件移到了新的位置），这里列出构件的重定位信息。 --&gt; &lt;relocation&gt; &lt;!--构件新的group ID --&gt; &lt;groupId /&gt; &lt;!--构件新的artifact ID --&gt; &lt;artifactId /&gt; &lt;!--构件新的版本号 --&gt; &lt;version /&gt; &lt;!--显示给用户的，关于移动的额外信息，例如原因。 --&gt; &lt;message /&gt; &lt;/relocation&gt; &lt;!-- 给出该构件在远程仓库的状态。不得在本地项目中设置该元素，因为这是工具自动更新的。有效的值有：none（默认），converted（仓库管理员从 Maven 1 POM转换过来），partner（直接从伙伴Maven 2仓库同步过来），deployed（从Maven 2实例部 署），verified（被核实时正确的和最终的）。 --&gt; &lt;status /&gt; &lt;/distributionManagement&gt; &lt;!--以值替代名称，Properties可以在整个POM中使用，也可以作为触发条件（见settings.xml配置文件里activation元素的说明）。格式是&lt;name&gt;value&lt;/name&gt;。 --&gt; &lt;properties /&gt;&lt;/project&gt; Maven 构建生命周期Maven 构建生命周期定义了一个项目构建跟发布的过程。 一个典型的 Maven 构建（build）生命周期是由以下几个阶段的序列组成的： 阶段 处理 描述 验证 validate 验证项目 验证项目是否正确且所有必须信息是可用的 编译 compile 执行编译 源代码编译在此阶段完成 测试 Test 测试 使用适当的单元测试框架（例如JUnit）运行测试。 包装 package 打包 创建JAR/WAR包如在 pom.xml 中定义提及的包 检查 verify 检查 对集成测试的结果进行检查，以保证质量达标 安装 install 安装 安装打包的项目到本地仓库，以供其他项目使用 部署 deploy 部署 拷贝最终的工程包到远程仓库中，以共享给其他开发人员和工程 为了完成 default 生命周期，这些阶段（包括其他未在上面罗列的生命周期阶段）将被按顺序地执行。 Maven 有以下三个标准的生命周期： clean：项目清理的处理 default(或 build)：项目部署的处理 site：项目站点文档创建的处理 构建阶段由插件目标构成一个插件目标代表一个特定的任务（比构建阶段更为精细），这有助于项目的构建和管理。这些目标可能被绑定到多个阶段或者无绑定。不绑定到任何构建阶段的目标可以在构建生命周期之外通过直接调用执行。这些目标的执行顺序取决于调用目标和构建阶段的顺序。 例如，考虑下面的命令： clean 和 pakage 是构建阶段，dependency:copy-dependencies 是目标 1mvn clean dependency:copy-dependencies package 这里的 clean 阶段将会被首先执行，然后 dependency:copy-dependencies 目标会被执行，最终 package 阶段被执行。 Clean 生命周期当我们执行 mvn post-clean 命令时，Maven 调用 clean 生命周期，它包含以下阶段： pre-clean：执行一些需要在clean之前完成的工作 clean：移除所有上一次构建生成的文件 post-clean：执行一些需要在clean之后立刻完成的工作 mvn clean 中的 clean 就是上面的 clean，在一个生命周期中，运行某个阶段的时候，它之前的所有阶段都会被运行，也就是说，如果执行 mvn clean 将运行以下两个生命周期阶段： 1pre-clean, clean 如果我们运行 mvn post-clean ，则运行以下三个生命周期阶段： 1pre-clean, clean, post-clean 我们可以通过在上面的 clean 生命周期的任何阶段定义目标来修改这部分的操作行为。 在下面的例子中，我们将 maven-antrun-plugin:run 目标添加到 pre-clean、clean 和 post-clean 阶段中。这样我们可以在 clean 生命周期的各个阶段显示文本信息。 我们已经在 C:\MVN\project 目录下创建了一个 pom.xml 文件。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt;&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;&lt;groupId&gt;com.companyname.projectgroup&lt;/groupId&gt;&lt;artifactId&gt;project&lt;/artifactId&gt;&lt;version&gt;1.0&lt;/version&gt;&lt;build&gt;&lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-antrun-plugin&lt;/artifactId&gt; &lt;version&gt;1.1&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;id.pre-clean&lt;/id&gt; &lt;phase&gt;pre-clean&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;run&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;tasks&gt; &lt;echo&gt;pre-clean phase&lt;/echo&gt; &lt;/tasks&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;execution&gt; &lt;id&gt;id.clean&lt;/id&gt; &lt;phase&gt;clean&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;run&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;tasks&gt; &lt;echo&gt;clean phase&lt;/echo&gt; &lt;/tasks&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;execution&gt; &lt;id&gt;id.post-clean&lt;/id&gt; &lt;phase&gt;post-clean&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;run&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;tasks&gt; &lt;echo&gt;post-clean phase&lt;/echo&gt; &lt;/tasks&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt;&lt;/plugins&gt;&lt;/build&gt;&lt;/project&gt; 现在打开命令控制台，跳转到 pom.xml 所在目录，并执行下面的 mvn 命令。 1C:\MVN\project&gt;mvn post-clean Maven 将会开始处理并显示 clean 生命周期的所有阶段。 12345678910111213141516171819202122232425[INFO] Scanning for projects...[INFO] ------------------------------------------------------------------[INFO] Building Unnamed - com.companyname.projectgroup:project:jar:1.0[INFO] task-segment: [post-clean][INFO] ------------------------------------------------------------------[INFO] [antrun:run &#123;execution: id.pre-clean&#125;][INFO] Executing tasks [echo] pre-clean phase[INFO] Executed tasks[INFO] [clean:clean &#123;execution: default-clean&#125;][INFO] [antrun:run &#123;execution: id.clean&#125;][INFO] Executing tasks [echo] clean phase[INFO] Executed tasks[INFO] [antrun:run &#123;execution: id.post-clean&#125;][INFO] Executing tasks [echo] post-clean phase[INFO] Executed tasks[INFO] ------------------------------------------------------------------[INFO] BUILD SUCCESSFUL[INFO] ------------------------------------------------------------------[INFO] Total time: &lt; 1 second[INFO] Finished at: Sat Jul 07 13:38:59 IST 2012[INFO] Final Memory: 4M/44M[INFO] ------------------------------------------------------------------ 你可以尝试修改 mvn clean 命令，来显示 pre-clean 和 clean，而在 post-clean 阶段不执行任何操作。 Default (Build) 生命周期这是 Maven 的主要生命周期，被用于构建应用，包括下面的 23 个阶段： 生命周期阶段 描述 validate 检查工程配置是否正确，完成构建过程的所有必要信息是否能够获取到。 initialize 初始化构建状态，例如设置属性。 generate-sources 生成编译阶段需要包含的任何源码文件。 process-sources 处理源代码，例如，过滤任何值（filter any value）。 generate-resources 生成工程包中需要包含的资源文件。 process-resources 拷贝和处理资源文件到目的目录中，为打包阶段做准备。 compile 编译工程源码。 process-classes 处理编译生成的文件，例如 Java Class 字节码的加强和优化。 generate-test-sources 生成编译阶段需要包含的任何测试源代码。 process-test-sources 处理测试源代码，例如，过滤任何值（filter any values)。 test-compile 编译测试源代码到测试目的目录。 process-test-classes 处理测试代码文件编译后生成的文件。 test 使用适当的单元测试框架（例如JUnit）运行测试。 prepare-package 在真正打包之前，为准备打包执行任何必要的操作。 package 获取编译后的代码，并按照可发布的格式进行打包，例如 JAR、WAR 或者 EAR 文件。 pre-integration-test 在集成测试执行之前，执行所需的操作。例如，设置所需的环境变量。 integration-test 处理和部署必须的工程包到集成测试能够运行的环境中。 post-integration-test 在集成测试被执行后执行必要的操作。例如，清理环境。 verify 运行检查操作来验证工程包是有效的，并满足质量要求。 install 安装工程包到本地仓库中，该仓库可以作为本地其他工程的依赖。 deploy 拷贝最终的工程包到远程仓库中，以共享给其他开发人员和工程。 有一些与 Maven 生命周期相关的重要概念需要说明： 当一个阶段通过 Maven 命令调用时，例如 mvn compile，只有该阶段之前以及包括该阶段在内的所有阶段会被执行。 不同的 maven 目标将根据打包的类型（JAR / WAR / EAR），被绑定到不同的 Maven 生命周期阶段。 在下面的例子中，我们将 maven-antrun-plugin:run 目标添加到 Build 生命周期的一部分阶段中。这样我们可以显示生命周期的文本信息。 我们已经更新了 C:\MVN\project 目录下的 pom.xml 文件。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt;&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;&lt;groupId&gt;com.companyname.projectgroup&lt;/groupId&gt;&lt;artifactId&gt;project&lt;/artifactId&gt;&lt;version&gt;1.0&lt;/version&gt;&lt;build&gt;&lt;plugins&gt;&lt;plugin&gt;&lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;&lt;artifactId&gt;maven-antrun-plugin&lt;/artifactId&gt;&lt;version&gt;1.1&lt;/version&gt;&lt;executions&gt; &lt;execution&gt; &lt;id&gt;id.validate&lt;/id&gt; &lt;phase&gt;validate&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;run&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;tasks&gt; &lt;echo&gt;validate phase&lt;/echo&gt; &lt;/tasks&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;execution&gt; &lt;id&gt;id.compile&lt;/id&gt; &lt;phase&gt;compile&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;run&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;tasks&gt; &lt;echo&gt;compile phase&lt;/echo&gt; &lt;/tasks&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;execution&gt; &lt;id&gt;id.test&lt;/id&gt; &lt;phase&gt;test&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;run&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;tasks&gt; &lt;echo&gt;test phase&lt;/echo&gt; &lt;/tasks&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;execution&gt; &lt;id&gt;id.package&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;run&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;tasks&gt; &lt;echo&gt;package phase&lt;/echo&gt; &lt;/tasks&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;execution&gt; &lt;id&gt;id.deploy&lt;/id&gt; &lt;phase&gt;deploy&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;run&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;tasks&gt; &lt;echo&gt;deploy phase&lt;/echo&gt; &lt;/tasks&gt; &lt;/configuration&gt; &lt;/execution&gt;&lt;/executions&gt;&lt;/plugin&gt;&lt;/plugins&gt;&lt;/build&gt;&lt;/project&gt; 现在打开命令控制台，跳转到 pom.xml 所在目录，并执行以下 mvn 命令。 1C:\MVN\project&gt;mvn compile Maven 将会开始处理并显示直到编译阶段的构建生命周期的各个阶段。 1234567891011121314151617181920212223242526[INFO] Scanning for projects...[INFO] ------------------------------------------------------------------[INFO] Building Unnamed - com.companyname.projectgroup:project:jar:1.0[INFO] task-segment: [compile][INFO] ------------------------------------------------------------------[INFO] [antrun:run &#123;execution: id.validate&#125;][INFO] Executing tasks [echo] validate phase[INFO] Executed tasks[INFO] [resources:resources &#123;execution: default-resources&#125;][WARNING] Using platform encoding (Cp1252 actually) to copy filtered resources,i.e. build is platform dependent![INFO] skip non existing resourceDirectory C:\MVN\project\src\main\resources[INFO] [compiler:compile &#123;execution: default-compile&#125;][INFO] Nothing to compile - all classes are up to date[INFO] [antrun:run &#123;execution: id.compile&#125;][INFO] Executing tasks [echo] compile phase[INFO] Executed tasks[INFO] ------------------------------------------------------------------[INFO] BUILD SUCCESSFUL[INFO] ------------------------------------------------------------------[INFO] Total time: 2 seconds[INFO] Finished at: Sat Jul 07 20:18:25 IST 2012[INFO] Final Memory: 7M/64M[INFO] ------------------------------------------------------------------ 命令行调用在开发环境中，使用下面的命令去构建、安装工程到本地仓库 1mvn install 这个命令在执行 install 阶段前，按顺序执行了 default 生命周期的阶段 （validate，compile，package，等等），我们只需要调用最后一个阶段，如这里是 install。 在构建环境中，使用下面的调用来纯净地构建和部署项目到共享仓库中 1mvn clean deploy 这行命令也可以用于多模块的情况下，即包含多个子项目的项目，Maven 会在每一个子项目执行 clean 命令，然后再执行 deploy 命令。 Site 生命周期Maven Site 插件一般用来创建新的报告文档、部署站点等。 pre-site：执行一些需要在生成站点文档之前完成的工作 site：生成项目的站点文档 post-site： 执行一些需要在生成站点文档之后完成的工作，并且为部署做准备 site-deploy：将生成的站点文档部署到特定的服务器上 这里经常用到的是site阶段和site-deploy阶段，用以生成和发布Maven站点，这可是Maven相当强大的功能，Manager比较喜欢，文档及统计数据自动生成，很好看。 在下面的例子中，我们将 maven-antrun-plugin:run 目标添加到 Site 生命周期的所有阶段中。这样我们可以显示生命周期的所有文本信息。 我们已经更新了 C:\MVN\project 目录下的 pom.xml 文件。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt;&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;&lt;groupId&gt;com.companyname.projectgroup&lt;/groupId&gt;&lt;artifactId&gt;project&lt;/artifactId&gt;&lt;version&gt;1.0&lt;/version&gt;&lt;build&gt;&lt;plugins&gt;&lt;plugin&gt;&lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;&lt;artifactId&gt;maven-antrun-plugin&lt;/artifactId&gt;&lt;version&gt;1.1&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;id.pre-site&lt;/id&gt; &lt;phase&gt;pre-site&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;run&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;tasks&gt; &lt;echo&gt;pre-site phase&lt;/echo&gt; &lt;/tasks&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;execution&gt; &lt;id&gt;id.site&lt;/id&gt; &lt;phase&gt;site&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;run&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;tasks&gt; &lt;echo&gt;site phase&lt;/echo&gt; &lt;/tasks&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;execution&gt; &lt;id&gt;id.post-site&lt;/id&gt; &lt;phase&gt;post-site&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;run&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;tasks&gt; &lt;echo&gt;post-site phase&lt;/echo&gt; &lt;/tasks&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;execution&gt; &lt;id&gt;id.site-deploy&lt;/id&gt; &lt;phase&gt;site-deploy&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;run&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;tasks&gt; &lt;echo&gt;site-deploy phase&lt;/echo&gt; &lt;/tasks&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt;&lt;/plugin&gt;&lt;/plugins&gt;&lt;/build&gt;&lt;/project&gt; 现在打开命令控制台，跳转到 pom.xml 所在目录，并执行以下 mvn 命令。 1C:\MVN\project&gt;mvn site Maven 将会开始处理并显示直到 site 阶段的 site 生命周期的各个阶段。 1234567891011121314151617181920212223242526272829303132[INFO] Scanning for projects...[INFO] ------------------------------------------------------------------[INFO] Building Unnamed - com.companyname.projectgroup:project:jar:1.0[INFO] task-segment: [site][INFO] ------------------------------------------------------------------[INFO] [antrun:run &#123;execution: id.pre-site&#125;][INFO] Executing tasks [echo] pre-site phase[INFO] Executed tasks[INFO] [site:site &#123;execution: default-site&#125;][INFO] Generating &quot;About&quot; report.[INFO] Generating &quot;Issue Tracking&quot; report.[INFO] Generating &quot;Project Team&quot; report.[INFO] Generating &quot;Dependencies&quot; report.[INFO] Generating &quot;Project Plugins&quot; report.[INFO] Generating &quot;Continuous Integration&quot; report.[INFO] Generating &quot;Source Repository&quot; report.[INFO] Generating &quot;Project License&quot; report.[INFO] Generating &quot;Mailing Lists&quot; report.[INFO] Generating &quot;Plugin Management&quot; report.[INFO] Generating &quot;Project Summary&quot; report.[INFO] [antrun:run &#123;execution: id.site&#125;][INFO] Executing tasks [echo] site phase[INFO] Executed tasks[INFO] ------------------------------------------------------------------[INFO] BUILD SUCCESSFUL[INFO] ------------------------------------------------------------------[INFO] Total time: 3 seconds[INFO] Finished at: Sat Jul 07 15:25:10 IST 2012[INFO] Final Memory: 24M/149M[INFO] ------------------------------------------------------------------ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160# Maven 构建配置文件构建配置文件是一系列的配置项的值，可以用来设置或者覆盖 Maven 构建默认值。使用构建配置文件，你可以为不同的环境，比如说生产环境（Producation）和开发（Development）环境，定制构建方式。配置文件在 pom.xml 文件中使用 activeProfiles 或者 profiles 元素指定，并且可以通过各种方式触发。配置文件在构建时修改 POM，并且用来给参数设定不同的目标环境（比如说，开发（Development）、测试（Testing）和生产环境（Producation）中数据库服务器的地址）。------## 构建配置文件的类型构建配置文件大体上有三种类型:| 类型 | 在哪定义 || --------------------- | ------------------------------------------------------------ || 项目级（Per Project） | 定义在项目的POM文件pom.xml中 || 用户级 （Per User） | 定义在Maven的设置xml文件中 (%USER_HOME%/.m2/settings.xml) || 全局（Global） | 定义在 Maven 全局的设置 xml 文件中 (%M2_HOME%/conf/settings.xml) |------## 配置文件激活Maven的构建配置文件可以通过多种方式激活。- 使用命令控制台输入显式激活。- 通过 maven 设置。- 基于环境变量（用户或者系统变量）。- 操作系统设置（比如说，Windows系列）。- 文件的存在或者缺失。### 配置文件激活实例假定项目结构如下：![img](http://www.runoob.com/wp-content/uploads/2018/09/1536129535-6460-structure.jpg)其中在src/main/resources文件夹下有三个用于测试文件：| 文件名 | 描述 || ------------------- | ------------------------------------ || env.properties | 如果未指定配置文件时默认使用的配置。 || env.test.properties | 当测试配置文件使用时的测试配置。 || env.prod.properties | 当生产配置文件使用时的生产配置。 |**注意：**这三个配置文件并不是代表构建配置文件的功能，而是用于本次测试的目的；比如，我指定了构建配置文件为 prod 时，项目就使用 envprod.properties文件。**注意：**下面的例子仍然是使用 AntRun 插件，因为此插件能绑定 Maven 生命周期阶段，并通过 Ant 的标签不用编写一点代码即可输出信息、复制文件等，经此而已。其余的与本次构建配置文件无关。### 1、配置文件激活profile 可以让我们定义一系列的配置信息，然后指定其激活条件。这样我们就可以定义多个 profile，然后每个 profile 对应不同的激活条件和配置信息，从而达到不同环境使用不同配置信息的效果。以下实例，我们将 maven-antrun-plugin:run 目标添加到测试阶段中。这样可以我们在不同的 profile 中输出文本信息。我们将使用 pom.xml 来定义不同的 profile，并在命令控制台中使用 maven 命令激活 profile。pom.xml 文件如下：```xml&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.jsoft.test&lt;/groupId&gt; &lt;artifactId&gt;testproject&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;version&gt;0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;testproject&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;3.8.1&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;profiles&gt; &lt;profile&gt; &lt;id&gt;test&lt;/id&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-antrun-plugin&lt;/artifactId&gt; &lt;version&gt;1.8&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;test&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;run&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;tasks&gt; &lt;echo&gt;Using env.test.properties&lt;/echo&gt; &lt;copy file=&quot;src/main/resources/env.test.properties&quot; tofile=&quot;$&#123;project.build.outputDirectory&#125;/env.properties&quot; overwrite=&quot;true&quot;/&gt; &lt;/tasks&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; &lt;profile&gt; &lt;id&gt;normal&lt;/id&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-antrun-plugin&lt;/artifactId&gt; &lt;version&gt;1.8&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;test&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;run&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;tasks&gt; &lt;echo&gt;Using env.properties&lt;/echo&gt; &lt;copy file=&quot;src/main/resources/env.properties&quot; tofile=&quot;$&#123;project.build.outputDirectory&#125;/env.properties&quot; overwrite=&quot;true&quot;/&gt; &lt;/tasks&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; &lt;profile&gt; &lt;id&gt;prod&lt;/id&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-antrun-plugin&lt;/artifactId&gt; &lt;version&gt;1.8&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;test&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;run&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;tasks&gt; &lt;echo&gt;Using env.prod.properties&lt;/echo&gt; &lt;copy file=&quot;src/main/resources/env.prod.properties&quot; tofile=&quot;$&#123;project.build.outputDirectory&#125;/env.properties&quot; overwrite=&quot;true&quot;/&gt; &lt;/tasks&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; &lt;/profiles&gt;&lt;/project&gt; 注意：**构建配置文件采用的是 ** 节点。 说明：上面新建了三个 ，其中 区分了不同的 执行不同的 AntRun 任务；而 AntRun 的任务可以这么理解，AntRun 监听 test 的 Maven 生命周期阶段，当 Maven 执行 test 时，就除了发 AntRun 的任务，任务里面为输出文本并复制文件到指定的位置；而至于要执行哪个 AntRun 任务，此时构建配置文件起到了传输指定的作用，比如，通过命令行参数输入指定的 。 执行命令： 1mvn test -Ptest 提示：第一个 test 为 Maven 生命周期阶段，第 2 个 test 为构建配置文件指定的 参数，这个参数通过 -P 来传输，当然，它可以是 prod 或者 normal 这些由你定义的。 运行的结果如下： 可以看出成功的触发了AntRun的任务。并且是对应构建配置文件下的 为 test 的任务。 再测试其余两个命令，结果如下： 2、通过Maven设置激活配置文件打开 %USER_HOME%/.m2 目录下的 settings.xml 文件，其中 %USER_HOME% 代表用户主目录。如果 setting.xml 文件不存在就直接拷贝 %M2_HOME%/conf/settings.xml 到 .m2 目录，其中 %M2_HOME% 代表 Maven 的安装目录。 配置 setting.xml 文件，增加 &lt;activeProfiles&gt;属性： 12345&lt;settings xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd"&gt; &lt;activeProfiles&gt; &lt;activeProfile&gt;test&lt;/activeProfile&gt; &lt;/activeProfiles&gt; &lt;/settings&gt; 执行命令： 1mvn test 提示 1：此时不需要使用 -Ptest 来输入参数了，上面的 setting.xml 文件的 已经指定了 test 参数代替了。 提示 2：同样可以使用在 %M2_HOME%/conf/settings.xml 的文件进行配置，效果一致。 执行结果： 3、通过环境变量激活配置文件先把上一步测试的 setting.xml 值全部去掉。 然后在 pom.xml 里面的 为 test 的 节点，加入 节点： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.jsoft.test&lt;/groupId&gt; &lt;artifactId&gt;testproject&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;version&gt;0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;testproject&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;3.8.1&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;profiles&gt; &lt;profile&gt; &lt;id&gt;test&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;env&lt;/name&gt; &lt;value&gt;test&lt;/value&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-antrun-plugin&lt;/artifactId&gt; &lt;version&gt;1.8&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;test&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;run&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;tasks&gt; &lt;echo&gt;Using env.test.properties&lt;/echo&gt; &lt;copy file="src/main/resources/env.test.properties" tofile="$&#123;project.build.outputDirectory&#125;/env.properties" overwrite="true"/&gt; &lt;/tasks&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; &lt;profile&gt; &lt;id&gt;normal&lt;/id&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-antrun-plugin&lt;/artifactId&gt; &lt;version&gt;1.8&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;test&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;run&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;tasks&gt; &lt;echo&gt;Using env.properties&lt;/echo&gt; &lt;copy file="src/main/resources/env.properties" tofile="$&#123;project.build.outputDirectory&#125;/env.properties" overwrite="true"/&gt; &lt;/tasks&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; &lt;profile&gt; &lt;id&gt;prod&lt;/id&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-antrun-plugin&lt;/artifactId&gt; &lt;version&gt;1.8&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;test&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;run&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;tasks&gt; &lt;echo&gt;Using env.prod.properties&lt;/echo&gt; &lt;copy file="src/main/resources/env.prod.properties" tofile="$&#123;project.build.outputDirectory&#125;/env.properties" overwrite="true"/&gt; &lt;/tasks&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; &lt;/profiles&gt;&lt;/project&gt; 执行命令： 1mvn test -Denv=test 提示 1：上面使用 -D 传递环境变量，其中 evn 对应刚才设置的 值，test 对应。 提示 2：在 Windows 10 上测试了系统的环境变量，但是不生效，所以，只能通过 -D 传递。 执行结果： 4、通过操作系统激活配置文件activation 元素包含下面的操作系统信息。当系统为 windows XP 时，test Profile 将会被触发。 1234567891011&lt;profile&gt; &lt;id&gt;test&lt;/id&gt; &lt;activation&gt; &lt;os&gt; &lt;name&gt;Windows XP&lt;/name&gt; &lt;family&gt;Windows&lt;/family&gt; &lt;arch&gt;x86&lt;/arch&gt; &lt;version&gt;5.1.2600&lt;/version&gt; &lt;/os&gt; &lt;/activation&gt;&lt;/profile&gt; 现在打开命令控制台，跳转到 pom.xml 所在目录，并执行下面的 mvn 命令。不要使用 -P 选项指定 Profile 的名称。Maven 将显示被激活的 test Profile 的结果。 1mvn test 5、通过文件的存在或者缺失激活配置文件现在使用 activation 元素包含下面的操作系统信息。当 target/generated-sources/axistools/wsdl2java/com/companyname/group 缺失时，test Profile 将会被触发。 123456789&lt;profile&gt; &lt;id&gt;test&lt;/id&gt; &lt;activation&gt; &lt;file&gt; &lt;missing&gt;target/generated-sources/axistools/wsdl2java/ com/companyname/group&lt;/missing&gt; &lt;/file&gt; &lt;/activation&gt;&lt;/profile&gt; 现在打开命令控制台，跳转到 pom.xml 所在目录，并执行下面的 mvn 命令。不要使用 -P 选项指定 Profile 的名称。Maven 将显示被激活的 test Profile 的结果。 1mvn test Maven 仓库在 Maven 的术语中，仓库是一个位置（place）。 Maven 仓库是项目中依赖的第三方库，这个库所在的位置叫做仓库。 在 Maven 中，任何一个依赖、插件或者项目构建的输出，都可以称之为构件。 Maven 仓库能帮助我们管理构件（主要是JAR），它就是放置所有JAR文件（WAR，ZIP，POM等等）的地方。 Maven 仓库有三种类型： 本地（local） 中央（central） 远程（remote） 本地仓库Maven 的本地仓库，在安装 Maven 后并不会创建，它是在第一次执行 maven 命令的时候才被创建。 运行 Maven 的时候，Maven 所需要的任何构件都是直接从本地仓库获取的。如果本地仓库没有，它会首先尝试从远程仓库下载构件至本地仓库，然后再使用本地仓库的构件。 默认情况下，不管Linux还是 Windows，每个用户在自己的用户目录下都有一个路径名为 .m2/respository/ 的仓库目录。 Maven 本地仓库默认被创建在 %USER_HOME% 目录下。要修改默认位置，在 %M2_HOME%\conf 目录中的 Maven 的 settings.xml 文件中定义另一个路径。 12&lt;settings xmlns="http://maven.apache.org/SETTINGS/1.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd"&gt; &lt;localRepository&gt;C:/MyLocalRepository&lt;/localRepository&gt; &lt;/settings&gt; 当你运行 Maven 命令，Maven 将下载依赖的文件到你指定的路径中。 中央仓库Maven 中央仓库是由 Maven 社区提供的仓库，其中包含了大量常用的库。 中央仓库包含了绝大多数流行的开源Java构件，以及源码、作者信息、SCM、信息、许可证信息等。一般来说，简单的Java项目依赖的构件都可以在这里下载到。 中央仓库的关键概念： 这个仓库由 Maven 社区管理。 不需要配置。 需要通过网络才能访问。 要浏览中央仓库的内容，maven 社区提供了一个 URL：http://search.maven.org/#browse。使用这个仓库，开发人员可以搜索所有可以获取的代码库。 远程仓库如果 Maven 在中央仓库中也找不到依赖的文件，它会停止构建过程并输出错误信息到控制台。为避免这种情况，Maven 提供了远程仓库的概念，它是开发人员自己定制仓库，包含了所需要的代码库或者其他工程中用到的 jar 文件。 举例说明，使用下面的 pom.xml，Maven 将从远程仓库中下载该 pom.xml 中声明的所依赖的（在中央仓库中获取不到的）文件。 1234567891011121314151617181920212223242526&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.companyname.projectgroup&lt;/groupId&gt; &lt;artifactId&gt;project&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.companyname.common-lib&lt;/groupId&gt; &lt;artifactId&gt;common-lib&lt;/artifactId&gt; &lt;version&gt;1.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependencies&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;companyname.lib1&lt;/id&gt; &lt;url&gt;http://download.companyname.org/maven2/lib1&lt;/url&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;companyname.lib2&lt;/id&gt; &lt;url&gt;http://download.companyname.org/maven2/lib2&lt;/url&gt; &lt;/repository&gt; &lt;/repositories&gt;&lt;/project&gt; Maven 依赖搜索顺序当我们执行 Maven 构建命令时，Maven 开始按照以下顺序查找依赖的库： 步骤 1 － 在本地仓库中搜索，如果找不到，执行步骤 2，如果找到了则执行其他操作。 步骤 2 － 在中央仓库中搜索，如果找不到，并且有一个或多个远程仓库已经设置，则执行步骤 4，如果找到了则下载到本地仓库中以备将来引用。 步骤 3 － 如果远程仓库没有被设置，Maven 将简单的停滞处理并抛出错误（无法找到依赖的文件）。 步骤 4 － 在一个或多个远程仓库中搜索依赖的文件，如果找到则下载到本地仓库以备将来引用，否则 Maven 将停止处理并抛出错误（无法找到依赖的文件）。 Maven 阿里云(Aliyun)仓库Maven 仓库默认在国外， 国内使用难免很慢，我们可以更换为阿里云的仓库。 第一步:修改 maven 根目录下的 conf 文件夹中的 setting.xml 文件，在 mirrors 节点上，添加内容如下： 12345678&lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;/mirror&gt;&lt;/mirrors&gt; 第二步: pom.xml文件里添加： 12345678910111213&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; Maven 插件Maven 有以下三个标准的生命周期： clean：项目清理的处理 default(或 build)：项目部署的处理 site：项目站点文档创建的处理 每个生命周期中都包含着一系列的阶段(phase)。这些 phase 就相当于 Maven 提供的统一的接口，然后这些 phase 的实现由 Maven 的插件来完成。 我们在输入 mvn 命令的时候 比如 mvn clean，clean 对应的就是 Clean 生命周期中的 clean 阶段。但是 clean 的具体操作是由 maven-clean-plugin 来实现的。 所以说 Maven 生命周期的每一个阶段的具体实现都是由 Maven 插件实现的。 Maven 实际上是一个依赖插件执行的框架，每个任务实际上是由插件完成。Maven 插件通常被用来： 创建 jar 文件 创建 war 文件 编译代码文件 代码单元测试 创建工程文档 创建工程报告 插件通常提供了一个目标的集合，并且可以使用下面的语法执行： 1&lt;code&gt;mvn [plugin-name]:[goal-name]&lt;/code&gt; 例如，一个 Java 工程可以使用 maven-compiler-plugin 的 compile-goal 编译，使用以下命令： 1&lt;code&gt;mvn compiler:compile&lt;/code&gt; 插件类型Maven 提供了下面两种类型的插件： 类型 描述 Build plugins 在构建时执行，并在 pom.xml 的 元素中配置。 Reporting plugins 在网站生成过程中执行，并在 pom.xml 的 元素中配置。 下面是一些常用插件的列表： 插件 描述 clean 构建之后清理目标文件。删除目标目录。 compiler 编译 Java 源文件。 surefile 运行 JUnit 单元测试。创建测试报告。 jar 从当前工程中构建 JAR 文件。 war 从当前工程中构建 WAR 文件。 javadoc 为工程生成 Javadoc。 antrun 从构建过程的任意一个阶段中运行一个 ant 任务的集合。 实例我们已经在我们的例子中大量使用了 maven-antrun-plugin 来输出数据到控制台上。请查看 Maven - 构建配置文件 章节。让我们用一种更好的方式理解这部分内容，在 C:\MVN\project 目录下创建一个 pom.xml 文件。 1234567891011121314151617181920212223242526272829303132&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt;&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;&lt;groupId&gt;com.companyname.projectgroup&lt;/groupId&gt;&lt;artifactId&gt;project&lt;/artifactId&gt;&lt;version&gt;1.0&lt;/version&gt;&lt;build&gt;&lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-antrun-plugin&lt;/artifactId&gt; &lt;version&gt;1.1&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;id.clean&lt;/id&gt; &lt;phase&gt;clean&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;run&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;tasks&gt; &lt;echo&gt;clean phase&lt;/echo&gt; &lt;/tasks&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt;&lt;/plugins&gt;&lt;/build&gt;&lt;/project&gt; 接下来，打开命令终端跳转到 pom.xml 所在的目录，并执行下面的 mvn 命令。 1mvn clean Maven 将开始处理并显示 clean 生命周期的 clean 阶段。 1234567891011121314151617[INFO] Scanning for projects...[INFO] ------------------------------------------------------------------[INFO] Building Unnamed - com.companyname.projectgroup:project:jar:1.0[INFO] task-segment: [post-clean][INFO] ------------------------------------------------------------------[INFO] [clean:clean &#123;execution: default-clean&#125;][INFO] [antrun:run &#123;execution: id.clean&#125;][INFO] Executing tasks [echo] clean phase[INFO] Executed tasks[INFO] ------------------------------------------------------------------[INFO] BUILD SUCCESSFUL[INFO] ------------------------------------------------------------------[INFO] Total time: &lt; 1 second[INFO] Finished at: Sat Jul 07 13:38:59 IST 2012[INFO] Final Memory: 4M/44M[INFO] ------------------------------------------------------------------ 上面的例子展示了以下关键概念： 插件是在 pom.xml 中使用 plugins 元素定义的。 每个插件可以有多个目标。 你可以定义阶段，插件会使用它的 phase 元素开始处理。我们已经使用了 clean 阶段。 你可以通过绑定到插件的目标的方式来配置要执行的任务。我们已经绑定了 echo 任务到 maven-antrun-plugin 的 run 目标。 就是这样，Maven 将处理剩下的事情。它将下载本地仓库中获取不到的插件，并开始处理。]]></content>
      <categories>
        <category>maven</category>
      </categories>
      <tags>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[svn搭建教程]]></title>
    <url>%2F2013%2F02%2F15%2Ftest%2F</url>
    <content type="text"></content>
      <categories>
        <category>-svn</category>
      </categories>
      <tags>
        <tag>-svn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[svn搭建教程]]></title>
    <url>%2F2013%2F02%2F15%2Fsvn%2F</url>
    <content type="text"><![CDATA[关于SVNSVN 教程Apache Subversion 通常被缩写成 SVN，是一个开放源代码的版本控制系统，Subversion 在 2000 年由 CollabNet Inc 开发，现在发展成为 Apache 软件基金会的一个项目，同样是一个丰富的开发者和用户社区的一部分。这个教程给你提供一个理解 SVN 系统，操作当前和历史版本的文件，比如代码、网页、文档。 适用人群这个教程设计为了让对 SVN 感兴趣的软件专业人士简单方便地开始。完成这个教程，你将充分了解 SVN 让自己获得更高的水平的专业知识。 学习前提在你继续本教程之前，你必须对简单的术语有一定的了解，比如源码，文档等等。因为在你的组织下处理各级软件项目，如果你有软件工作的知识在软件开发和软件测试流程那将是好的。 版本信息书中演示代码基于以下版本： 语言/框架 版本信息 SVN SVN 基本概念什么是版本控制系统(VCS)版本控制系统 (VCS) 是一个软件，帮助软件开发人员团队工作并维持他们完整的工作历史。 下面是版本控制系统(VCS) 的目标 允许开发者们同时工作 不会重写每个人的改变 维持每个版本的全部的历史 VCS 被分成两种 集中版本控制系统 (CVCS) 和 分散或不集中的版本控制系统 (DVCS) 在这个教程里，我们只专注于集中的版本控制系统特别是 Subversion，Subversion 基于集中的版本控制系统，意味着使用统一的服务器让团队协作。 版本控制的术语 让我们先懂得一些在这个教程将用到的术语 仓库: 仓库是任何一个版本系统的核心，它是开发者们保存工作的总部，仓库不止处理文件还有历史记录，它需要访问网络，扮演服务器的角色，版本控制工具扮演客户端的角色，客户端可以连接仓库，那么他们就可以从仓库中存储或者提取。通过保存这些更改，一个客户端的更改可以被其他人检索到，一个客户端可以让其他人的更改作为一个工作副本。 主干：trunk 是主要开发所在的目录，经常被项目开发者们查看。 标签：tags 目录用于储存项目中被命名的快照，标签操作允许给予对仓库中特定版本一个描述和一个难忘的名字。比如，LAST_STABLE_CODE_BEFORE_EMAIL_SUPPORT 比 Repository UUID: 7ceef8cb-3799-40dd-a067-c216ec2e5247 和Revision: 13 更令人难忘。 分支：分支操作用于创建开发的另一条线，当你想把开发进程复制进两个不同的方向是很有用的。比如，当你发布 5.0 版本时，你可能想从 5.0 的 bug 修复中分离出来创建一个开发 6.0 功能的分支。 工作副本：工作副本是仓库的一个快照。这个仓库被所有的成员共享，但人们不直接修改它，相反每个开发者检查这个工作副本，工作副本是一个私人的工作空间，这里开发者可以独立于其他成员做自己的工作。 提交更改：提交是一个保存更改的过程，从私人工作空间到中央服务器。提交后，更改对全部成员可用，通过更新工作副本其他开发者提取这些更改。提交是一个原子操作，要么全部提交成功要么回滚，用户绝不会看到一半完成提交。 SVN 环境搭建SVN 环境搭建Subversion 是一个受欢迎的开源的版本控制工具。他在互联网免费提供并且开源。大多数 GNU/Linux 发行版系统自带，所以它很有可能已经安装在你的系统上了。可以使用下面命令检查是否安装了。 1[jerry@CentOS ~]$ svn --version 如果 Subversion 客户端没有安装，命令将报告错误，否则它将出现安装的软件版本 12[jerry@CentOS ~]$ svn --version -bash: svn: command not found 如果你使用基于 RPM 的 GNU/Linux，可以使用 yum 命令进行安装，安装成功之后，执行 svn --version 命令。 1234567[jerry@CentOS ~]$ su -Password: [root@CentOS ~]# yum install subversion[jerry@CentOS ~]$ svn --versionsvn, version 1.6.11 (r934486)compiled Jun 23 2012, 00:44:03 如果你使用基于 Debian 的 GNU/Linux，使用 apt 命令进行安装。 12345678[jerry@Ubuntu]$ sudo apt-get update[sudo] password for jerry:[jerry@Ubuntu]$ sudo apt-get install subversion[jerry@Ubuntu]$ svn --versionsvn, version 1.7.5 (r1336830)compiled Jun 21 2013, 22:11:49 Apache 安装我们已经看到如何将 SVN 客户端安装到 GNU/Linux 上，让我们看看如何创建一个新的版本库让使用者们访问。 我们必须必须在服务器上安装 Apache httpd 模块和 svnadmin 工具。subversion 从 /etc/httpd/conf.d/subversion.conf 读取配置文件， subversion.conf 看起来像这个样子 1234567891011LoadModule dav_svn_module modules/mod_dav_svn.soLoadModule authz_svn_module modules/mod_authz_svn.so&lt;Location /svn&gt; DAV svn SVNParentPath /var/www/svn AuthType Basic AuthName &quot;Authorization Realm&quot; AuthUserFile /etc/svn-users Require valid-user&lt;/Location&gt; 让我们创建 Subversion 用户，授权他们访问版本库，htpasswd 命令用于创建和更新用来保存用户名和密码的纯文本文件给 HTTP 用户提供基本身份认证。-c 选项创建一个密码文件，如果密码文件已经存在了，它将会被覆盖。这就是为什么 -c 只在第一次使用。-m 选项用于设置是否启用 MD5 加密密码。 用户安装让我们创建 tom 1234[root@CentOS ~]# htpasswd -cm /etc/svn-users tomNew password: Re-type new password: Adding password for user tom 让我们创建 jerry 12345[root@CentOS ~]# htpasswd -m /etc/svn-users jerryNew password: Re-type new password: Adding password for user jerry[root@CentOS ~]# 创建一个 Subversion 父目录保存所有的工作，(/etc/httpd/conf.d/subversion.conf)。 12[root@CentOS ~]# mkdir /var/www/svn[root@CentOS ~]# cd /var/www/svn/ 版本库安装创建一个名为 project_repo 的版本库。svnadmin 命令用于创建一个新的版本库和一些其他目录保存数据。 12345678910[root@CentOS svn]# svnadmin create project_repo[root@CentOS svn]# ls -l project_repototal 24drwxr-xr-x. 2 root root 4096 Aug 4 22:30 confdrwxr-sr-x. 6 root root 4096 Aug 4 22:30 db-r--r--r--. 1 root root 2 Aug 4 22:30 formatdrwxr-xr-x. 2 root root 4096 Aug 4 22:30 hooksdrwxr-xr-x. 2 root root 4096 Aug 4 22:30 locks-rw-r--r--. 1 root root 229 Aug 4 22:30 README.txt 让我们更改版本库的用户和组所有权。 1[root@CentOS svn]# chown -R apache.apache project_repo/ 检查是否启用SELinux或没有使用SELinux状态工具 1234567[root@CentOS svn]# sestatusSELinux status: enabledSELinuxfs mount: /selinuxCurrent mode: enforcingMode from config file: enforcingPolicy version: 24Policy from config file: targeted 如果SELinux启用了，我们必须更改安全的上下文。 1[root@CentOS svn]# chcon -R -t httpd_sys_content_t /var/www/svn/project_repo/ 如果允许通过 HTTP 进行提交，执行下面命令。 1[root@CentOS svn]# chcon -R -t httpd_sys_rw_content_t /var/www/svn/project_repo/ 更改这些配置后，我们重启 Apache 服务器。 12345678[root@CentOS svn]# service httpd restartStopping httpd: [FAILED]Starting httpd: httpd: apr_sockaddr_info_get() failed for CentOShttpd: Could not reliably determine the server&apos;s fully qualified domain name, using 127.0.0.1 for ServerName [ OK ][root@CentOS svn]# service httpd statushttpd (pid 1372) is running...[root@CentOS svn]# 我们已经成功配置好了 Apache 服务器，现在我们将配置版本库，使用默认的授权文件给可信的用户访问，添加下列几行到 roject_repo/conf/svnserve.conf 文件。 12anon-access = noneauthz-db = authz 照惯例，每个 SVN 项目都有主干，标签，分支在项目的 root 目录。 主干是主要开发和经常被开发者们查看的目录。 分支目录用于追求不同的开发方向。 让我们在项目版本库底下创建主干，标签，分支结构。 1234[root@CentOS svn]# mkdir /tmp/svn-template[root@CentOS svn]# mkdir /tmp/svn-template/trunk[root@CentOS svn]# mkdir /tmp/svn-template/branches[root@CentOS svn]# mkdir /tmp/svn-template/tags 现在从 /tmp/svn-template 导入这些文件目录。 123456[root@CentOS svn]# svn import -m &apos;Create trunk, branches, tags directory structure&apos; /tmp/svn-template/ Adding /tmp/svn-template/trunkAdding /tmp/svn-template/branchesAdding /tmp/svn-template/tagsCommitted revision 1.[root@CentOS svn]# 完成了！我们已经成功创建版本库并允许 Tom 和 Jerry 访问，从现在开始他们可以所有版本库支持的操作了。 SVN 生命周期SVN 生命周期本章讨论了版本控制系统的生命周期。在后面的章节中，我们将会介绍每个操作对应的 SVN 命令。 创建版本库版本库相当于一个集中的空间，用于存放开发者所有的工作成果。版本库不仅能存放文件，还包括了每次修改的历史，即每个文件的变动历史。 Create 操作是用来创建一个新的版本库。大多数情况下这个操作只会执行一次。当你创建一个新的版本库的时候，你的版本控制系统会让你提供一些信息来标识版本库，例如创建的位置和版本库的名字。 检出Checkout 操作是用来从版本库创建一个工作副本。工作副本是开发者私人的工作空间，可以进行内容的修改，然后提交到版本库中。 更新顾名思义，update 操作是用来更新版本库的。这个操作将工作副本与版本库进行同步。由于版本库是由整个团队共用的，当其他人提交了他们的改动之后，你的工作副本就会过期。 让我们假设 Tom 和 Jerry 是一个项目的两个开发者。他们同时从版本库中检出了最新的版本并开始工作。此时，工作副本是与版本库完全同步的。然后，Jerry很高效的完成了他的工作并提交了更改到版本库中。 此时 Tom 的工作副本就过期了。更新操作将会从版本库中拉取 Jerry 的最新改动并将 Tom 的工作副本进行更新。 执行变更当检出之后，你就可以做很多操作来执行变更。编辑是最常用的操作。你可以编辑已存在的文件来，例如进行文件的添加/删除操作。 你可以添加文件/目录。但是这些添加的文件目录不会立刻成为版本库的一部分，而是被添加进待变更列表中，直到执行了 commit 操作后才会成为版本库的一部分。 同样地你可以删除文件/目录。删除操作立刻将文件从工作副本中删除掉，但该文件的实际删除只是被添加到了待变更列表中，直到执行了 commit 操作后才会真正删除。 Rename 操作可以更改文件/目录的名字。“移动”操作用来将文件/目录从一处移动到版本库中的另一处。 复查变化当你检出工作副本或者更新工作副本后，你的工作副本就跟版本库完全同步了。但是当你对工作副本进行一些修改之后，你的工作副本会比版本库要新。在 commit 操作之前复查下你的修改是一个很好的习惯。 Status 操作列出了工作副本中所进行的变动。正如我们之前提到的，你对工作副本的任何改动都会成为待变更列表的一部分。Status 操作就是用来查看这个待变更列表。 Status 操作只是提供了一个变动列表，但并不提供变动的详细信息。你可以用 diff 操作来查看这些变动的详细信息。 修复错误我们来假设你对工作副本做了许多修改，但是现在你不想要这些修改了，这时候 revert 操作将会帮助你。 Revert 操作重置了对工作副本的修改。它可以重置一个或多个文件/目录。当然它也可以重置整个工作副本。在这种情况下，revert 操作将会销毁待变更列表并将工作副本恢复到原始状态。 解决冲突合并的时候可能会发生冲突。Merge 操作会自动处理可以安全合并的东西。其它的会被当做冲突。例如，“hello.c” 文件在一个分支上被修改，在另一个分支上被删除了。这种情况就需要人为处理。Resolve 操作就是用来帮助用户找出冲突并告诉版本库如何处理这些冲突。 提交更改Commit 操作是用来将更改从工作副本到版本库。这个操作会修改版本库的内容，其它开发者可以通过更新他们的工作副本来查看这些修改。 在提交之前，你必须将文件/目录添加到待变更列表中。列表中记录了将会被提交的改动。当提交的时候，我们通常会提供一个注释来说明为什么会进行这些改动。这个注释也会成为版本库历史记录的一部分。Commit 是一个原子操作，也就是说要么完全提交成功，要么失败回滚。用户不会看到成功提交一半的情况。 SVN 检出过程SVN 检出过程SVN提供了 checkout 命令来从版本库检出一个工作副本。下面的命令将会在当前工作副本中新建一个名为 project_repo 的文件夹。不用担心版本库的 URL 地址是什么，大部分时间里，SVN 管理员会提供给你地址和访问权限的。 1[tom@CentOS ~]$ svn checkout http://svn.server.com/svn/project_repo --username=tom 以上命令将产生如下结果： 1234A project_repo/trunkA project_repo/branchesA project_repo/tagsChecked out revision 1. 每一次成功提交之后，修订版本号都会显示出来。如果你想查看更多关于版本库的信息，执行 info 命令。 1234[tom@CentOS trunk]$ pwd/home/tom/project_repo/trunk[tom@CentOS trunk]$ svn info 以上命令将产生如下结果： 123456789101112Path: .URL: http://svn.server.com/svn/project_repo/trunkRepository Root: http://svn.server.com/svn/project_repoRepository UUID: 7ceef8cb-3799-40dd-a067-c216ec2e5247Revision: 1Node Kind: directorySchedule: normalLast Changed Author: jerryLast Changed Rev: 0Last Changed Date: 2013-08-24 18:15:52 +0530 (Sat, 24 Aug 2013)[tom@CentOS trunk]$ SVN 执行修改SVN 执行修改Jerry 从版本库检出了最新的版本并开始在项目上工作。他在 trunk 目录下创建了一个 array.c 文件。 123[jerry@CentOS ~]$ cd project_repo/trunk/[jerry@CentOS trunk]$ cat array.c 以上命令将产生如下结果： 1234567891011121314151617#include &lt;stdio.h&gt;#define MAX 16int main(void) &#123; int i, n, arr[MAX]; printf(&quot;Enter the total number of elements: &quot;); scanf(&quot;%d&quot;, &amp;n); printf(&quot;Enter the elements\n&quot;); for (i = 0; i &lt; n; ++i) scanf(&quot;%d&quot;, &amp;arr[i]); printf(&quot;Array has following elements\n&quot;); for (i = 0; i &lt; n; ++i) printf(&quot;|%d| &quot;, arr[i]); printf(&quot;\n&quot;); return 0;&#125; 他想在提交之前测试他的代码。 12345678910111213[jerry@CentOS trunk]$ make arraycc array.c -o array[jerry@CentOS trunk]$ ./array Enter the total number of elements: 5Enter the elements12345Array has following elements|1| |2| |3| |4| |5| 他编译并测试了代码，一切正常，现在是时候提交更改了。 123[jerry@CentOS trunk]$ svn status? array.c? array SVN显示在文件名前显示“?”，因为它不知道如何处理这些文件。 在提交之前，Jerry 需要将文件添加到待变更列表中。 12[jerry@CentOS trunk]$ svn add array.c A array.c 现在让我们来用 status 命令来检查它。SVN在 array.c 文件前面显示了一个 A，它意味着这个文件已经被成功地添加到了待变更列表中。 123[jerry@CentOS trunk]$ svn status? arrayA array.c 为了把 array.c 存储到版本库中，使用 commit -m 加上注释信息来提交。如果你忽略了 -m 选项， SVN会打开一个可以输入多行的文本编辑器来让你输入提交信息。 1234[jerry@CentOS trunk]$ svn commit -m &quot;Initial commit&quot;Adding trunk/array.cTransmitting file data .Committed revision 2. 现在 array.c 被成功地添加到了版本库中，并且修订版本号增加了1。 SVN 检查更改SVN 检查更改Jerry 往仓库里添加了一个叫做 array.c 的文件。 Tom 签出最后一个版本后开始工作。 1[tom@CentOS ~]$ svn co http://svn.server.com/svn/project_repo --username=tom 上面的命令将会产生下面的效果 12345A project_repo/trunkA project_repo/trunk/array.cA project_repo/branchesA project_repo/tagsChecked out revision 2. 但是，他发现有人已经添加了代码，他很好奇是谁添加的，于是他用下面的命令检查 log 信息： 1[tom@CentOS trunk]$ svn log 上面的命令将会产生下面的效果 123456789------------------------------------------------------------------------r2 | jerry | 2013-08-17 20:40:43 +0530 (Sat, 17 Aug 2013) | 1 lineInitial commit------------------------------------------------------------------------r1 | jerry | 2013-08-04 23:43:08 +0530 (Sun, 04 Aug 2013) | 1 lineCreate trunk, branches, tags directory structure------------------------------------------------------------------------ 当 Tom 查看 Jerry 的代码时，他注意到了里面的一个 bug 。 Jerry 没有检查数组溢出，这会导致很严重的问题。所以 Tom 决定修复这个问题。在修改之后， array.c 将会是这个样子。 1234567891011121314151617181920212223242526272829#include &lt;stdio.h&gt;#define MAX 16int main(void)&#123; int i, n, arr[MAX]; printf(&quot;Enter the total number of elements: &quot;); scanf(&quot;%d&quot;, &amp;n); /* handle array overflow condition */ if (n &gt; MAX) &#123; fprintf(stderr, &quot;Number of elements must be less than %d\n&quot;, MAX); return 1; &#125; printf(&quot;Enter the elements\n&quot;); for (i = 0; i &lt; n; ++i) scanf(&quot;%d&quot;, &amp;arr[i]); printf(&quot;Array has following elements\n&quot;); for (i = 0; i &lt; n; ++i) printf(&quot;|%d| &quot;, arr[i]); printf(&quot;\n&quot;); return 0;&#125; Tom 想使用 status 操作来看看将要生效的更改列表 12[tom@CentOS trunk]$ svn statusM array.c array.c 文件已经被修改，Subversion 会在修改过的文件前面加一个字母 M 。接下来 Tom 编译测试了他的代码，并且工作良好。在提交更改前，他想要再次检查他的更改。 1234567891011121314151617[tom@CentOS trunk]$ svn diffIndex: array.c===================================================================--- array.c (revision 2)+++ array.c (working copy)@@ -9,6 +9,11 @@ printf(&quot;Enter the total number of elements: &quot;); scanf(&quot;%d&quot;, &amp;n);+ if (n &gt; MAX) &#123;+ fprintf(stderr, &quot;Number of elements must be less than %d\n&quot;, MAX);+ return 1;+ &#125;+ printf(&quot;Enter the elements\n&quot;); for (i = 0; i &lt; n; ++i) Tom 在 array.c 文件中添加了几行代码,Subversion 会在新添加的这几行代码前面添加 ＋ 号标记，现在，他已经准备好提交他的代码。 1[tom@CentOS trunk]$ svn commit -m &quot;Fix array overflow problem&quot; 上面的命令将会产生下面的效果 123Sending trunk/array.cTransmitting file data .Committed revision 3. Tom 的更改被成功得提交到了仓库中。 SVN 更新过程SVN 更新过程Jerry 提交了他第一个版本的代码. 但是他想他应该写两个函数用来接收输入和显示数组，在修改之后， array.c 看起来像是下面这样。 12345678910111213141516171819202122232425262728293031#include &lt;stdio.h&gt;#define MAX 16void accept_input(int *arr, int n) &#123; int i; for (i = 0; i &lt; n; ++i) scanf(&quot;%d&quot;, &amp;arr[i]);&#125;void display(int *arr, int n) &#123; int i; for (i = 0; i &lt; n; ++i) printf(&quot;|%d| &quot;, arr[i]); printf(&quot;\n&quot;);&#125;int main(void) &#123; int i, n, arr[MAX]; printf(&quot;Enter the total number of elements: &quot;); scanf(&quot;%d&quot;, &amp;n); printf(&quot;Enter the elements\n&quot;); accept_input(arr, n); printf(&quot;Array has following elements\n&quot;); display(arr, n); return 0;&#125; Jerry 编译和测试了他的代码，现在他准备提交他的更改。在此之前，他想要用下面的命令查看更改。 1[jerry@CentOS trunk]$ svn diff 上面的命令将会产生下面的效果 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647Index: array.c===================================================================--- array.c (revision 2)+++ array.c (working copy)@@ -2,6 +2,24 @@ #define MAX 16+void accept_input(int *arr, int n)+&#123;+ int i;++ for (i = 0; i &amp; n; ++i)+ scanf(&quot;%d&quot;, &amp;arr[i]);+&#125;++void display(int *arr, int n)+&#123;+ int i;++ for (i = 0; i &lt; n; ++i)+ printf(&quot;|%d| &quot;, arr[i]);+ + printf(&quot;\n&quot;);+&#125;+ int main(void) &#123; int i, n, arr[MAX];@@ -10,15 +28,10 @@ scanf(&quot;%d&quot;, &amp;n); printf(&quot;Enter the elements\n&quot;);+ accept_input(arr, n);- for (i = 0; i &lt; n; ++i)- scanf(&quot;%d&quot;, &amp;arr[i]);- printf(&quot;Array has following elements\n&quot;);- for (i = 0; i &lt; n; ++i)- printf(&quot;|%d| &quot;, arr[i]);- - printf(&quot;\n&quot;);+ display(arr, n); return 0; &#125; 对于新增加的行， Subversion 在前面加上了 ＋ 号，并且用 - 号标记了删除掉的行。现在， Jerry 尝试使用下面的命令来提交他的更改： 1[jerry@CentOS trunk]$ svn commit -m &quot;Add function to accept input and to display array contents&quot; 上面的命令将会产生下面的效果 1234Sending trunk/array.csvn: Commit failed (details follow):svn: File or directory &apos;array.c&apos; is out of date; try updatingsvn: resource out of date; try updating Subversion 不会允许 Jerry 提交他的更改，因为 Tom 已经修改了仓库，所以 Jerry 的工作副本已经失效。为了避免两人的代码被互相覆盖，Subversion 不允许他进行这样的操作。Jerry 在提交他的更改之前必须先更新工作副本。所以他使用了 update 命令，如下： 123[jerry@CentOS trunk]$ svn updateG array.cUpdated to revision 3. Subversion 在这个文件前面加上了字母 G 标记, 这意味着这个文件是被合并过的。 1[jerry@CentOS trunk]$ svn diff 上面的命令将会产生下面的效果 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748Index: array.c===================================================================--- array.c (revision 3)+++ array.c (working copy)@@ -2,6 +2,24 @@ #define MAX 16+void accept_input(int *arr, int n)+&#123;+ int i;++ for (i = 0; i &lt; n; ++i)+ scanf(&quot;%d&quot;, &amp;arr[i]);+&#125;++void display(int *arr, int n)+&#123;+ int i;++ for (i = 0; i &lt; n; ++i)+ printf(&quot;|%d| &quot;, arr[i]);+ + printf(&quot;\n&quot;);+&#125;+ int main(void) &#123; int i, n, arr[MAX];@@ -15,15 +33,10 @@ &#125; printf(&quot;Enter the elements\n&quot;);+ accept_input(arr, n);- for (i = 0; i &lt; n; ++i)- scanf(&quot;%d&quot;, &amp;arr[i]);- printf(&quot;Array has following elements\n&quot;);- for (i = 0; i &lt; n; ++i)- printf(&quot;|%d| &quot;, arr[i]);- - printf(&quot;\n&quot;);+ display(arr, n); return 0; &#125; Subversion 只展示出了 Jerry 的更改，但是 array.c 文件被合并了。如果你仔细观察，Subversion 现在展示的版本号是3。在之前的输出中，它展示的版本号是2。只是展示出了谁对其进行了更改和更改的目的。 1234567891011121314jerry@CentOS trunk]$ svn log------------------------------------------------------------------------r3 | tom | 2013-08-18 20:21:50 +0530 (Sun, 18 Aug 2013) | 1 lineFix array overflow problem------------------------------------------------------------------------r2 | jerry | 2013-08-17 20:40:43 +0530 (Sat, 17 Aug 2013) | 1 lineInitial commit------------------------------------------------------------------------r1 | jerry | 2013-08-04 23:43:08 +0530 (Sun, 04 Aug 2013) | 1 lineCreate trunk, branches, tags directory structure------------------------------------------------------------------------ 现在 Jerry 的工作目录是和仓库同步的，他现在可以安全地提交更改了。 1234[jerry@CentOS trunk]$ svn commit -m &quot;Add function to accept input and to display array contents&quot;Sending trunk/array.cTransmitting file data .Committed revision 4. SVN 修复错误SVN 修复错误假设 Jerry 意外地更改了 array.c 文件而导致编译错误，他想放弃修改。在这种状况下，‘revert’ 操作将派上用场。revert 操作将撤销任何文件或目录里的局部更改。 1[jerry@CentOS trunk]$ svn status 上面的命令将会产生下面的效果 1M array.c 让我们尝试创建一个数组，如下： 1[jerry@CentOS trunk]$ make array 上面的命令将会产生下面的效果 1234567cc array.c -o arrayarray.c: In function ‘main’:array.c:26: error: ‘n’ undeclared (first use in this function)array.c:26: error: (Each undeclared identifier is reported only oncearray.c:26: error: for each function it appears in.)array.c:34: error: ‘arr’ undeclared (first use in this function)make: *** [array] Error 1 Jerry 在 array.c 文件里执行了‘revert’操作。 12345[jerry@CentOS trunk]$ svn revert array.c Reverted &apos;array.c&apos;[jerry@CentOS trunk]$ svn status[jerry@CentOS trunk]$ 现在开始编译代码。 12[jerry@CentOS trunk]$ make arraycc array.c -o array 进行 revert 操作之后，他的文件恢复了原始的状态。 revert 操作不单单可以使单个文件恢复原状，而且可以使整个目录恢复原状。恢复目录用 －R 命令，如下。 1234[jerry@CentOS project_repo]$ pwd/home/jerry/project_repo[jerry@CentOS project_repo]$ svn revert -R trunk 现在，我们已经知道如何撤销更改。但是，假使你想恢复一个已经提交的版本怎么办！Version Control System 工具不允许删除仓库的历史纪录。为了消除一个旧版本，我们必须撤销旧版本里的所有更改然后提交一个新版本。这种操作叫做 reverse merge。 假设 Jerry 添加了一段线性搜索操作的代码，核查之后，他提交了更改。 12345678910111213141516171819202122232425262728293031[jerry@CentOS trunk]$ svn diffIndex: array.c===================================================================--- array.c (revision 21)+++ array.c (working copy)@@ -2,6 +2,16 @@ #define MAX 16+int linear_search(int *arr, int n, int key)+&#123;+ int i;++ for (i = 0; i &lt; n; ++i)+ if (arr[i] == key)+ return i;+ return -1;+&#125;+ void bubble_sort(int *arr, int n) &#123; int i, j, temp, flag = 1;[jerry@CentOS trunk]$ svn status? arrayM array.c[jerry@CentOS trunk]$ svn commit -m &quot;Added code for linear search&quot;Sending trunk/array.cTransmitting file data .Committed revision 22. Jerry 很好奇 Tom 以前写的代码。所以他检查了 Subversion 的 log 信息。 1[jerry@CentOS trunk]$ svn log 上面的命令将会产生下面的效果 12345678------------------------------------------------------------------------r5 | tom | 2013-08-24 17:15:28 +0530 (Sat, 24 Aug 2013) | 1 lineAdd binary search operation------------------------------------------------------------------------r4 | jerry | 2013-08-18 20:43:25 +0530 (Sun, 18 Aug 2013) | 1 lineAdd function to accept input and to display array contents 查看 log 信息之后，Jerry 意识到他犯了个严重的错误。因为 Tom 已经写了比线性搜索更好的二分法搜索，Jerry 发现自己的代码很冗余，他决定撤销之前对版本的修改。首先，找到仓库的当前版本，现在是版本 22，我们要撤销回之前的版本，比如版本 21。 1234567891011[jerry@CentOS trunk]$ svn up At revision 22.[jerry@CentOS trunk]$ svn merge -r 22:21 array.c --- Reverse-merging r22 into &apos;array.c&apos;:U array.c[jerry@CentOS trunk]$ svn commit -m &quot;Reverted to revision 21&quot;Sending trunk/array.cTransmitting file data .Committed revision 23. SVN 解决冲突SVN 解决冲突Tom决定给他的工程添加一个 README 文件，于是他创建了这个文件并在其中添加了 TODO 列表。添加完成之后，该文件的存放处位于 revision 6. 12345678910111213[tom@CentOS trunk]$ cat README /* TODO: Add contents in README file */[tom@CentOS trunk]$ svn status? README[tom@CentOS trunk]$ svn add README A README[tom@CentOS trunk]$ svn commit -m &quot;Added README file. Will update it&apos;s content in future.&quot;Adding trunk/READMETransmitting file data .Committed revision 6. Jerry 检出了位于 revision 6 最后的代码，并且他直接立刻开始了工作。几个小时以后，Tom 更新了 README 文件并且提交了他所修改的地方。修改的 README 将会看上去像这个样子。 12345678910111213[tom@CentOS trunk]$ cat README * Supported operations:1) Accept input2) Display array elements[tom@CentOS trunk]$ svn statusM README[tom@CentOS trunk]$ svn commit -m &quot;Added supported operation in README&quot;Sending trunk/READMETransmitting file data .Committed revision 7. 现在，仓库位于修改版本 7，并且 Jerry 的工作副本已经过期。Jerry 也更新 README 文件并且试图提交他的更改。 Jerry 的 README 文件将会看上去像这个样子： 1234567891011121314[jerry@CentOS trunk]$ cat README * File list1) array.c Implementation of array operation.2) README Instructions for user.[jerry@CentOS trunk]$ svn statusM README[jerry@CentOS trunk]$ svn commit -m &quot;Updated README&quot;Sending trunk/READMEsvn: Commit failed (details follow):svn: File or directory &apos;README&apos; is out of date; try updatingsvn: resource out of date; try updating 第一步：视图冲突Subversion 已经检测出 README 自上次更新后文件已经更改。所以，Jerry 必须更新他的工作副本。 12345[jerry@CentOS trunk]$ svn upConflict discovered in &apos;README&apos;.Select: (p) postpone, (df) diff-full, (e) edit, (mc) mine-conflict, (tc) theirs-conflict, (s) show all options: Subversion 提示说有一个冲突在 README 文件，并且 Subversion 并不知道如何解决这个问题。于是 Jerry 选择 df 选项来检查冲突。 1234567891011121314151617181920212223[jerry@CentOS trunk]$ svn upConflict discovered in &apos;README&apos;.Select: (p) postpone, (df) diff-full, (e) edit, (mc) mine-conflict, (tc) theirs-conflict, (s) show all options: df--- .svn/text-base/README.svn-base Sat Aug 24 18:07:13 2013+++ .svn/tmp/README.tmp Sat Aug 24 18:13:03 2013@@ -1 +1,11 @@-/* TODO: Add contents in README file */+&lt;&lt;&lt;&lt;&lt;&lt;&lt; .mine+* File list++1) array.c Implementation of array operation.+2) README Instructions for user.+=======+* Supported operations:++1) Accept input+2) Display array elements+&gt;&gt;&gt;&gt;&gt;&gt;&gt; .r7Select: (p) postpone, (df) diff-full, (e) edit, (r) resolved, (mc) mine-conflict, (tc) theirs-conflict, (s) show all options: 第二步：推迟冲突接下来 Jerry 用 postpone（p） 来解决冲突。 1234567Select: (p) postpone, (df) diff-full, (e) edit, (r) resolved, (mc) mine-conflict, (tc) theirs-conflict, (s) show all options: pC READMEUpdated to revision 7.Summary of conflicts: Text conflicts: 1 在用文档编辑器打开 README 文件后，Jerry 意识到 Subversion 已经包含了 Tom 的代码和他的代码，并被冲突标示包裹了起来。 1234567891011121314[jerry@CentOS trunk]$ cat README&lt;&lt;&lt;&lt;&lt;&lt;&lt; .min* File list1) array.c Implementation of array operation.2) README Instructions for user.=======* Supported operations:1) Accept input2) Display array elements&gt;&gt;&gt;&gt;&gt;&gt;&gt; .r7 Jerry 想让 Tom 的更改跟他的保持一致，所以他决定移除包含冲突标识的行。 所以，更改后的 README 将会是这个样子。 12345678910[jerry@CentOS trunk]$ cat README* File list1) array.c Implementation of array operation.2) README Instructions for user.* Supported operations:1) Accept input2) Display array elements Jerry 解决了冲突并试图再次提交。 123456789[jerry@CentOS trunk]$ svn commit -m &quot;Updated README&quot;svn: Commit failed (details follow):svn: Aborting commit: &apos;/home/jerry/project_repo/trunk/README&apos; remains in conflict[jerry@CentOS trunk]$ svn status? README.r6? README.r7? README.mineC README 第三步：解决冲突在上面的提交中，字母 C 指示说有一个冲突在 README 文件。Jerry 解决了冲突但并没有告诉 Subversion 已经解决了冲突。 他使用了 resolve 命令通知 Subversion 冲突的解决。 12345678910[jerry@CentOS trunk]$ svn resolve --accept=working READMEResolved conflicted state of &apos;README&apos;[jerry@CentOS trunk]$ svn statusM README[jerry@CentOS trunk]$ svn commit -m &quot;Updated README&quot;Sending trunk/READMETransmitting file data .Committed revision 8. SVN 标签SVN 标签版本管理系统支持tag选项，通过使用tag的概念，我们可以给某一个具体版本的代码一个更加有意义的名字。标签允许给某一个具体版本的代码一个描述性强，难忘的名字。举个例子：BASIC_ARRAY_OPERATIONS 就比修改版本 7更有意义。 让我们来看一个 tag 标签的例子。Tom为了能更好的审查代码，决定创建一个tag。 1[tom@CentOS project_repo]$ svn copy --revision=4 trunk/ tags/basic_array_operations 上面的命令将会产生出下面的结果。 123A tags/basic_array_operations/array.cUpdated to revision 4.A tags/basic_array_operations 上面的代码成功完成，新的目录将会被创建在 tags 目录下。 123[tom@CentOS project_repo]$ ls -l tags/total 4drwxrwxr-x. 3 tom tom 4096 Aug 24 18:18 basic_array_operations Tom想要在提交前双击。状态选项显示 tag 选项成功，所以他可以安全的提交他的更改。 1234567[tom@CentOS project_repo]$ svn statusA + tags/basic_array_operations[tom@CentOS project_repo]$ svn commit -m &quot;Created tag for basic array operations&quot;Adding tags/basic_array_operationsCommitted revision 5. SVN 分支SVN 分支Branch 选项会给开发者创建出另外一条线路。当有人希望开发进程分开成两条不同的线路时，这个选项会非常有用。我们先假设你已经发布了一个产品的 1.0 版本，你可能想创建一个新的分支，这样就可以不干扰到 1.0 版本的bug修复的同时，又可以开发2.0版本。 在这一节，我们将看到如何创建，穿过和合并分支。Jerry 因为代码冲突的事情不开心，所以他决定创建一个新的私有分支。 123456789101112131415[jerry@CentOS project_repo]$ lsbranches tags trunk[jerry@CentOS project_repo]$ svn copy trunk branches/jerry_branchA branches/jerry_branch[jerry@CentOS project_repo]$ svn statusA + branches/jerry_branch[jerry@CentOS project_repo]$ svn commit -m &quot;Jerry&apos;s private branch&quot;Adding branches/jerry_branchAdding branches/jerry_branch/READMECommitted revision 9.[jerry@CentOS project_repo]$ 现在 Jerry 在自己的分支下开始工作。他给序列添加了 sort 选项。Jerry 修改后的代码如下： 123[jerry@CentOS project_repo]$ cd branches/jerry_branch/[jerry@CentOS jerry_branch]$ cat array.c 上面的代码将会产生下面的结果： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162#include &lt;stdio.h&gt;#define MAX 16void bubble_sort(int *arr, int n)&#123; int i, j, temp, flag = 1; for (i = 1; i &lt; n &amp;&amp; flag == 1; ++i) &#123; flag = 0; for (j = 0; j &lt; n - i; ++j) &#123; if (arr[j] &gt; arr[j + 1]) &#123; flag = 1; temp = arr[j]; arr[j] = arr[j + 1]; arr[j + 1] = temp; &#125; &#125; &#125;&#125;void accept_input(int *arr, int n)&#123; int i; for (i = 0; i &lt; n; ++i) scanf(&quot;%d&quot;, &amp;arr[i]);&#125;void display(int *arr, int n)&#123; int i; for (i = 0; i &lt; n; ++i) printf(&quot;|%d| &quot;, arr[i]); printf(&quot;\n&quot;);&#125;int main(void)&#123; int i, n, key, ret, arr[MAX]; printf(&quot;Enter the total number of elements: &quot;); scanf(&quot;%d&quot;, &amp;n); /* Error handling for array overflow */ if (n &gt;MAX) &#123; fprintf(stderr, &quot;Number of elements must be less than %d\n&quot;, MAX); return 1; &#125; printf(&quot;Enter the elements\n&quot;); accept_input(arr, n); printf(&quot;Array has following elements\n&quot;); display(arr, n); printf(&quot;Sorted data is\n&quot;); bubble_sort(arr, n); display(arr, n); return 0;&#125; Jerry 编译并且测试了他的代码，准备提交他的更改。 1234[jerry@CentOS jerry_branch]$ make arraycc array.c -o array[jerry@CentOS jerry_branch]$ ./array 上面的命令将会产生如下的结果： 1234567891011121314151617181920Enter the total number of elements: 5Enter the elements10-427 9Array has following elements|10| |-4| |2| |7| |9| Sorted data is|-4| |2| |7| |9| |10| [jerry@CentOS jerry_branch]$ svn status? arrayM array.c[jerry@CentOS jerry_branch]$ svn commit -m &quot;Added sort operation&quot;Sending jerry_branch/array.cTransmitting file data .Committed revision 10. 同时，越过主干，Tom 决定实现 search 选项。Tom 添加了 search 选项而添加代码，他的代码如下： 1[tom@CentOS trunk]$ svn diff 上面的命令将会产生下面的结果： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768Index: array.c===================================================================--- array.c (revision 10)+++ array.c (working copy)@@ -2,6 +2,27 @@ #define MAX 16+int bin_search(int *arr, int n, int key)+&#123;+ int low, high, mid;++ low = 0;+ high = n - 1;+ mid = low + (high - low) / 2;++ while (low &lt;= high) &#123;+ if (arr[mid] == key)+ return mid;+ if (arr[mid] &gt; key)+ high = mid - 1;+ else+ low = mid + 1;+ mid = low + (high - low) / 2;+ &#125;++ return -1;+&#125;+ void accept_input(int *arr, int n) &#123; int i;@@ -22,7 +43,7 @@ int main(void) &#123;- int i, n, arr[MAX];+ int i, n, ret, key, arr[MAX]; printf(&quot;Enter the total number of elements: &quot;); scanf(&quot;%d&quot;, &amp;n);@@ -39,5 +60,16 @@ printf(&quot;Array has following elements\n&quot;); display(arr, n);+ printf(&quot;Enter the element to be searched: &quot;);+ scanf(&quot;%d&quot;, &amp;key);++ ret = bin_search(arr, n, key);+ if (ret &lt; 0) &#123;+ fprintf(stderr, &quot;%d element not present in array\n&quot;, key);+ return 1;+ &#125;++ printf(&quot;%d element found at location %d\n&quot;, key, ret + 1);+ return 0; &#125;After reviewing, he commits his changes.[tom@CentOS trunk]$ svn status? arrayM array.c[tom@CentOS trunk]$ svn commit -m &quot;Added search operation&quot;Sending trunk/array.cTransmitting file data .Committed revision 11. 但是 Tom 好奇 Jerry 在他自己的私有分支中干了什么： 123456789101112[tom@CentOS trunk]$ cd ../branches/[tom@CentOS branches]$ svn upA jerry_branchA jerry_branch/array.cA jerry_branch/README[tom@CentOS branches]$ svn log------------------------------------------------------------------------r9 | jerry | 2013-08-27 21:56:51 +0530 (Tue, 27 Aug 2013) | 1 lineAdded sort operation------------------------------------------------------------------------ 通过查看 Subversion 的 log 信息，Tom 发现 Jerry 依赖 ‘sort’ 选项。Tom 决定增添用折半查找，期望数据总是根据种类进行分类。但是如果用户提供的数据是没有进行分类呢？在那种情况下，折半查找将会失效。所以他决定接着 Jerry 的代码，在搜索选项前先进性分类。所以他告诉 Subversion 合并 Jerry 的分支到主干中去。 123456[tom@CentOS trunk]$ pwd/home/tom/project_repo/trunk[tom@CentOS trunk]$ svn merge ../branches/jerry_branch/--- Merging r9 through r11 into &apos;.&apos;:U array.c 在融合后，array.c 会看上去是这个样子： 1[tom@CentOS trunk]$ cat array.c 上面的代码将会产生下面的结果： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192#include &lt;stdio.h&gt;#define MAX 16void bubble_sort(int *arr, int n)&#123; int i, j, temp, flag = 1; for (i = 1; i &lt; n &amp;&amp; flag == 1; ++i) &#123; flag = 0; for (j = 0; j &lt; n - i; ++j) &#123; if (arr[j] &gt; arr[j + 1]) &#123; flag = 1; temp = arr[j]; arr[j] = arr[j + 1]; arr[j + 1] = temp; &#125; &#125; &#125;&#125;int bin_search(int *arr, int n, int key)&#123; int low, high, mid; low = 0; high = n - 1; mid = low + (high - low) / 2; while (low &lt;= high) &#123; if (arr[mid] == key) return mid; if (arr[mid] &gt; key) high = mid - 1; else low = mid + 1; mid = low + (high - low) / 2; &#125; return -1;&#125;void accept_input(int *arr, int n)&#123; int i; for (i = 0; i &lt; n; ++i) scanf(&quot;%d&quot;, &amp;arr[i]);&#125;void display(int *arr, int n)&#123; int i; for (i = 0; i &lt; n; ++i) printf(&quot;|%d| &quot;, arr[i]); printf(&quot;\n&quot;);&#125;int main(void)&#123; int i, n, ret, key, arr[MAX]; printf(&quot;Enter the total number of elements: &quot;); scanf(&quot;%d&quot;, &amp;n); /* Error handling for array overflow */ if (n &gt; MAX) &#123; fprintf(stderr, &quot;Number of elements must be less than %d\n&quot;, MAX); return 1; &#125; printf(&quot;Enter the elements\n&quot;); accept_input(arr, n); printf(&quot;Array has following elements\n&quot;); display(arr, n); printf(&quot;Sorted data is\n&quot;); bubble_sort(arr, n); display(arr, n); printf(&quot;Enter the element to be searched: &quot;); scanf(&quot;%d&quot;, &amp;key); ret = bin_search(arr, n, key); if (ret &lt; 0) &#123; fprintf(stderr, &quot;%d element not present in array\n&quot;, key); return 1; &#125; printf(&quot;%d element found at location %d\n&quot;, key, ret + 1); return 0;&#125; 经过编译和测试后，Tom 提交了他的更改到仓库。 12345678910111213141516171819202122232425[tom@CentOS trunk]$ make arraycc array.c -o array[tom@CentOS trunk]$ ./array Enter the total number of elements: 5Enter the elements10-28153Array has following elements|10| |-2| |8| |15| |3| Sorted data is|-2| |3| |8| |10| |15| Enter the element to be searched: -2-2 element found at location 1[tom@CentOS trunk]$ svn commit -m &quot;Merge changes from Jerry&apos;s code&quot;Sending trunkSending trunk/array.cTransmitting file data .Committed revision 12.[tom@CentOS trunk]$]]></content>
      <categories>
        <category>-svn</category>
      </categories>
      <tags>
        <tag>-svn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java设计模式单例模式]]></title>
    <url>%2F2013%2F02%2F15%2Fjava%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[例子下面的代码将创建一个MainWindow类。 MainWindow类的构造函数是私有的，并且有一个自身的静态实例。 MainWindow类提供了一个静态方法来获取它的静态实例到外部世界。 Main，我们的演示类将使用MainWindow类来获取一个MainWindow对象。 123456789101112131415161718192021222324252627class MainWindow &#123; //create an object of MainWindow private static MainWindow instance = new MainWindow(); //make the constructor private so that this class cannot be //instantiated by other class private MainWindow()&#123;&#125; //Get the only object available public static MainWindow getInstance()&#123; return instance; &#125; public void showMessage()&#123; System.out.println("Hello World!"); &#125;&#125;public class Main &#123; public static void main(String[] args) &#123; //Get the only object available MainWindow object = MainWindow.getInstance(); //show the message object.showMessage(); &#125;&#125; 上面的代码生成以下结果。 1Hello World!]]></content>
      <categories>
        <category>java设计模式</category>
      </categories>
      <tags>
        <tag>java设计模式</tag>
      </tags>
  </entry>
</search>
